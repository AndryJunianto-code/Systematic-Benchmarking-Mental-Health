{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndryJunianto-code/Systematic-Benchmarking-Mental-Health/blob/main/Final_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiThj0tIi3i"
      },
      "source": [
        "# Import & Hierarchy Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UrT3-gAfXzi"
      },
      "outputs": [],
      "source": [
        "!pip -q install gensim==4.3.3 xgboost==2.1.4 lightgbm==4.5.0 emoji==2.12.1 unidecode==1.3.8 imblearn scipy imbalanced-learn tensorflow scikit-posthocs openpyxl matplotlib numpy seaborn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6FDn1MJKc7S",
        "outputId": "2ff054fe-17fa-40ea-fdd7-1d760a4221e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import json\n",
        "import zipfile\n",
        "import warnings\n",
        "import pickle\n",
        "import joblib\n",
        "import random\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from unidecode import unidecode\n",
        "import emoji\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, LSTM, InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import scipy.sparse\n",
        "from scipy.stats import friedmanchisquare, stats\n",
        "import scikit_posthocs as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6iI3oI14-U"
      },
      "outputs": [],
      "source": [
        "# Define the complete hierarchy tree with all ancestors\n",
        "hierarchy_tree = {\n",
        "    'Root': [],\n",
        "\n",
        "    'Normal': ['Root'],\n",
        "    'Internal': ['Root'],\n",
        "    'External': ['Root'],\n",
        "\n",
        "    'Anxiety': ['Internal', 'Root'],\n",
        "    'Depression': ['Internal', 'Root'],\n",
        "    'Stress': ['Internal', 'Root'],\n",
        "    'Suicidal': ['Internal', 'Root'],\n",
        "\n",
        "    'Bipolar': ['External', 'Root'],\n",
        "    'Personality disorder': ['External', 'Root']\n",
        "}\n",
        "\n",
        "le = pickle.load(open('/content/drive/MyDrive/Skripsi Dataset/File/status_label_encoder.pkl','rb'))\n",
        "labels = le['classes']\n",
        "\n",
        "# For each leaf class, define its complete ancestor set (including itself)\n",
        "def get_complete_ancestors(class_name):\n",
        "    \"\"\"Get all ancestors including the class itself\"\"\"\n",
        "    return [class_name] + hierarchy_tree[class_name]\n",
        "\n",
        "# Store complete ancestor sets for each class\n",
        "ancestor_sets = {}\n",
        "for class_name in labels:\n",
        "    ancestor_sets[class_name] = get_complete_ancestors(class_name)\n",
        "\n",
        "# Hierarchical Formula\n",
        "def hierarchical_metrics_journal(y_true, y_pred, labels, ancestor_sets):\n",
        "    \"\"\"\n",
        "    Implement the exact formula from the journal:\n",
        "    hP = ∑|Anc_i ∩ Anĉ_i| / ∑|Anĉ_i|\n",
        "    hR = ∑|Anc_i ∩ Anĉ_i| / ∑|Anc_i|\n",
        "    hF1 = 2 * hP * hR / (hP + hR)\n",
        "    \"\"\"\n",
        "\n",
        "    total_intersection = 0\n",
        "    total_predicted_ancestors = 0\n",
        "    total_true_ancestors = 0\n",
        "\n",
        "    for true_idx, pred_idx in zip(y_true, y_pred):\n",
        "        true_label = labels[true_idx]\n",
        "        pred_label = labels[pred_idx]\n",
        "\n",
        "        # Get ancestor sets\n",
        "        true_ancestors = set(ancestor_sets[true_label])\n",
        "        pred_ancestors = set(ancestor_sets[pred_label])\n",
        "\n",
        "        intersection = true_ancestors.intersection(pred_ancestors)\n",
        "\n",
        "        total_intersection += len(intersection)\n",
        "        total_predicted_ancestors += len(pred_ancestors)\n",
        "        total_true_ancestors += len(true_ancestors)\n",
        "\n",
        "    # Calculate hierarchical precision and recall\n",
        "    if total_predicted_ancestors > 0:\n",
        "        hP = total_intersection / total_predicted_ancestors\n",
        "    else:\n",
        "        hP = 0.0\n",
        "\n",
        "    if total_true_ancestors > 0:\n",
        "        hR = total_intersection / total_true_ancestors\n",
        "    else:\n",
        "        hR = 0.0\n",
        "\n",
        "    # Calculate hierarchical F1\n",
        "    if hP + hR > 0:\n",
        "        hF1 = 2 * hP * hR / (hP + hR)\n",
        "    else:\n",
        "        hF1 = 0.0\n",
        "\n",
        "    return hF1, hP, hR, total_intersection, total_predicted_ancestors, total_true_ancestors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I1TQptObY_V",
        "outputId": "21d8b091-703b-46c1-f2e2-aafcc006c446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All random seeds set to: 42\n"
          ]
        }
      ],
      "source": [
        "def set_global_random_seed(seed=42):\n",
        "    \"\"\"Set all random seeds for complete reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    set_config(assume_finite=True, working_memory=256)\n",
        "    print(f\"All random seeds set to: {seed}\")\n",
        "\n",
        "set_global_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ufSf2ecynh"
      },
      "source": [
        "# Data Cleaning, Split & Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "601c9ca18c344b92828e8d50a6ee9fe3"
          ]
        },
        "id": "RdMGRUmWdBqK",
        "outputId": "4325e624-aaeb-409e-f36d-c5a9b37cd42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "status\n",
            "Normal                  16351\n",
            "Depression              15404\n",
            "Suicidal                10652\n",
            "Anxiety                  3888\n",
            "Bipolar                  2877\n",
            "Stress                   2669\n",
            "Personality disorder     1201\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "601c9ca18c344b92828e8d50a6ee9fe3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/52680 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text cleaning completed!\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Skripsi Dataset/Combined Data.csv\")\n",
        "print(df['status'].value_counts())\n",
        "\n",
        "df.dropna(inplace = True)\n",
        "df['statement'] = df['statement'].astype(str)\n",
        "\n",
        "def clean_text(text, lemmatizer):\n",
        "    # Step 1: Basic text cleaning\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www.\\S+', ' ', text)\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = re.sub(r'#\\w+', ' ', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lemmatization\n",
        "    filtered_tokens = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if len(word) > 2\n",
        "    ]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply cleaning\n",
        "tqdm.pandas()\n",
        "df['statement_cleaned'] = df['statement'].progress_apply(\n",
        "    lambda x: clean_text(x, lemmatizer)\n",
        ")\n",
        "\n",
        "print(\"Text cleaning completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee2lUvy1d5Ad",
        "outputId": "1705090b-dc78-49a3-fca8-0156ecddf2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
          ]
        }
      ],
      "source": [
        "# label encoding\n",
        "le = LabelEncoder()\n",
        "df['status_label'] = le.fit_transform(df['status'])\n",
        "label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_map)\n",
        "with open('/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl','wb') as f:\n",
        "    pickle.dump({'classes':list(le.classes_)}, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4-e6nABeDpJ",
        "outputId": "c18ab7b5-dbb4-4954-8585-2904bad0c37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Super class mapping: {'External': 0, 'Internal': 1, 'Normal': 2}\n",
            "Train size: 42144 Test size: 10536\n"
          ]
        }
      ],
      "source": [
        "# Super-class 1: Internal (Anxiety, Depression, Stress, Suicidal)\n",
        "# Super-class 2: External (Bipolar, Personality Disorder)\n",
        "# Super-class 3: Normal\n",
        "df['super_class'] = df['status'].apply(lambda x: 'Internal' if x in ['Anxiety','Depression','Stress','Suicidal']\n",
        "                                                     else ('External' if x in ['Bipolar','Personality disorder'] else 'Normal'))\n",
        "# Encode super_class\n",
        "le_super = LabelEncoder()\n",
        "df['super_label'] = le_super.fit_transform(df['super_class'])\n",
        "with open('/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl','wb') as f:\n",
        "    pickle.dump({'classes':list(le_super.classes_)}, f)\n",
        "print(\"Super class mapping:\", dict(zip(le_super.classes_, le_super.transform(le_super.classes_))))\n",
        "\n",
        "# train/test split stratified by original label\n",
        "X = df['statement_cleaned'].values\n",
        "y = df['status_label'].values\n",
        "y_super = df['super_label'].values\n",
        "\n",
        "X_train_statement, X_test_statement, y_train, y_test, y_super_train, y_super_test = train_test_split(\n",
        "    X, y, y_super, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "joblib.dump(X_train_statement, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_statement.pkl')\n",
        "joblib.dump(X_test_statement, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_statement.pkl')\n",
        "joblib.dump(y_train, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "joblib.dump(y_test, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_test.pkl')\n",
        "joblib.dump(y_super_train, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "joblib.dump(y_super_test, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_test.pkl')\n",
        "\n",
        "print(\"Train size:\", len(X_train_statement), \"Test size:\", len(X_test_statement))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYomvghNeNXV",
        "outputId": "f5767f8f-abb9-4b6d-e2c0-41a20e1bd546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples : 42144\n",
            "Test samples     : 10536\n",
            "\n",
            "Training FastText on TRAIN split only ...\n",
            "\n",
            "Creating sentence vectors ...\n",
            "FastText sentence vectors shapes: (42144, 100) (10536, 100)\n",
            "\n",
            "Training Tokenizer on TRAIN split only ...\n",
            "Max sequence length (95th percentile): 282\n",
            "Creating embedding matrix with vocabulary size: 43899\n",
            "Vocabulary size: 43899\n",
            "Padded sequences shape - Train: (42144, 282), Test: (10536, 282)\n",
            "Embedding matrix shape: (43899, 100)\n",
            "\n",
            "Testing OOV handling:\n",
            "✓ 'unknownword123' has vector (OOV handling works!)\n",
            "✓ 'runningly' has vector (OOV handling works!)\n",
            "✓ 'unhappiness' has vector (OOV handling works!)\n",
            "\n",
            "Processing completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# FastText\n",
        "X_train_statement = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_statement.pkl')\n",
        "X_test_statement = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_statement.pkl')\n",
        "\n",
        "# Convert to lists if they are numpy arrays\n",
        "if hasattr(X_train_statement, 'shape'):\n",
        "    X_train_statement = X_train_statement.tolist()\n",
        "if hasattr(X_test_statement, 'shape'):\n",
        "    X_test_statement = X_test_statement.tolist()\n",
        "\n",
        "# Tokenise training set only\n",
        "print(f\"Training samples : {len(X_train_statement)}\")\n",
        "print(f\"Test samples     : {len(X_test_statement)}\")\n",
        "\n",
        "sentences_train = [simple_preprocess(str(s)) for s in X_train_statement]\n",
        "sentences_test  = [simple_preprocess(str(s)) for s in X_test_statement]\n",
        "\n",
        "\n",
        "# Train FastText on training set only\n",
        "print(\"\\nTraining FastText on TRAIN split only ...\")\n",
        "fasttext_model = gensim.models.FastText(\n",
        "    sentences=sentences_train,      # <-- TRAIN ONLY\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    epochs=10,\n",
        "    sg=1,\n",
        "    seed=42,\n",
        "    workers=1\n",
        ")\n",
        "\n",
        "# Sentence-vector helper\n",
        "def sent_vector(model, tokens):\n",
        "    vecs = [model.wv[w] for w in tokens]      # sub-word fallback → no OOV error\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "# Create sentence vectors\n",
        "print(\"\\nCreating sentence vectors ...\")\n",
        "X_train_fast = np.vstack([sent_vector(fasttext_model, s) for s in sentences_train])\n",
        "X_test_fast  = np.vstack([sent_vector(fasttext_model, s) for s in sentences_test])\n",
        "\n",
        "# Save the sentence-level vectors\n",
        "np.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy', X_train_fast)\n",
        "np.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_fasttext.npy', X_test_fast)\n",
        "fasttext_model.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_model.model')\n",
        "\n",
        "print(\"FastText sentence vectors shapes:\", X_train_fast.shape, X_test_fast.shape)\n",
        "\n",
        "# Tokenizer\n",
        "print(\"\\nTraining Tokenizer on TRAIN split only ...\")\n",
        "tokenizer = Tokenizer()                     # no oov_token needed – FastText handles OOV\n",
        "tokenizer.fit_on_texts([' '.join(tok) for tok in sentences_train])   # <-- TRAIN ONLY\n",
        "\n",
        "# Convert to sequences of word indices\n",
        "X_train_sequences = tokenizer.texts_to_sequences([' '.join(tokens) for tokens in sentences_train])\n",
        "X_test_sequences = tokenizer.texts_to_sequences([' '.join(tokens) for tokens in sentences_test])\n",
        "\n",
        "# Calculate max sequence length based on train\n",
        "train_lengths = [len(seq) for seq in X_train_sequences]\n",
        "max_sequence_length = int(np.percentile(train_lengths, 95))\n",
        "print(f\"Max sequence length (95th percentile): {max_sequence_length}\")\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "print(f\"Creating embedding matrix with vocabulary size: {vocab_size}\")\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_matrix[i] = fasttext_model.wv[word]\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Padded sequences shape - Train: {X_train_padded.shape}, Test: {X_test_padded.shape}\")\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Verify OOV handling works\n",
        "print(\"\\nTesting OOV handling:\")\n",
        "test_words = [\"unknownword123\", \"runningly\", \"unhappiness\"]  # Words that probably don't exist\n",
        "for word in test_words:\n",
        "    try:\n",
        "        vector = fasttext_model.wv[word]\n",
        "        print(f\"✓ '{word}' has vector (OOV handling works!)\")\n",
        "    except:\n",
        "        print(f\"✗ '{word}' not in vocabulary\")\n",
        "\n",
        "# Save the word-level data for GRU\n",
        "np.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy', X_train_padded)\n",
        "np.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_fasttext_sequences.npy', X_test_padded)\n",
        "np.save('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy', embedding_matrix)\n",
        "joblib.dump(tokenizer, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_tokenizer.pkl')\n",
        "\n",
        "print(\"\\nProcessing completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G59BbouCXwb",
        "outputId": "5e1d67c8-4fb6-4838-ad90-786e533064ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 52680\n",
            "Tokenizing data...\n",
            "Training Word2Vec on entire dataset...\n",
            "Creating sentence vectors...\n",
            "Word2Vec sentence vectors shapes: (42144, 100) (10536, 100)\n",
            "Training tokenizer with UNK token on entire dataset...\n",
            "Max sequence length (95th percentile): 281\n",
            "Creating embedding matrix with vocabulary size: 49137\n",
            "UNK token ID: 1\n",
            "Using random initialization for OOV words (scale=0.01)\n",
            "UNK vector example (first 5 dims): [ 0.00496714 -0.00138264  0.00647689  0.0152303  -0.00234153]\n",
            "\n",
            "=== OOV Analysis ===\n",
            "Total vocabulary: 49136 words\n",
            "Known by Word2Vec: 24452 words (49.76%)\n",
            "OOV words: 24684 words (50.24%)\n",
            "OOV strategy: Each of 24684 OOV words gets unique random vector\n",
            "\n",
            "Processing completed successfully!\n",
            "\n",
            "=== Environment Info (Include in Thesis Appendix) ===\n",
            "OOV initialization: random normal (mean=0, std=0.01)\n",
            "Word2Vec min_count: 2\n",
            "Embedding dimensions: 100\n"
          ]
        }
      ],
      "source": [
        "# word2vec\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Skripsi Dataset/FinalFile/'\n",
        "EMBEDDING_DIM = 100\n",
        "W2V_PARAMS = {\n",
        "    'vector_size': EMBEDDING_DIM,\n",
        "    'window': 5,\n",
        "    'min_count': 2,\n",
        "    'epochs': 10,\n",
        "    'sg': 1,\n",
        "    'seed': 42,\n",
        "    'workers': 1\n",
        "}\n",
        "OOV_INIT_SCALE = 0.01\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Load Data ---\n",
        "X_train_statement = joblib.load(os.path.join(DRIVE_PATH, 'X_train_statement.pkl'))\n",
        "X_test_statement = joblib.load(os.path.join(DRIVE_PATH, 'X_test_statement.pkl'))\n",
        "\n",
        "# Convert to lists\n",
        "X_train_statement = X_train_statement.tolist()\n",
        "X_test_statement = X_test_statement.tolist()\n",
        "\n",
        "# Tokenize Data\n",
        "print(\"Tokenizing data...\")\n",
        "sentences_train = [simple_preprocess(str(s)) for s in X_train_statement]\n",
        "\n",
        "# Train Word2Vec\n",
        "print(\"Training Word2Vec on training set...\")\n",
        "word2vec_model = gensim.models.Word2Vec(\n",
        "    sentences=sentences_train,\n",
        "    **W2V_PARAMS\n",
        ")\n",
        "word2vec_model.save(os.path.join(DRIVE_PATH, 'w2v_model.model'))\n",
        "\n",
        "def sent_vector_w2v(model, tokens):\n",
        "    \"\"\"\n",
        "    Averages word vectors, skipping OOV words.\n",
        "    Word2Vec throws an error for OOV, unlike FastText.\n",
        "    \"\"\"\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "# Create sentence vectors\n",
        "print(\"\\nCreating sentence vectors...\")\n",
        "X_train_w2v = np.vstack([sent_vector_w2v(word2vec_model, s) for s in sentences_train])\n",
        "X_test_w2v = np.vstack([sent_vector_w2v(word2vec_model, s) for s in sentences_test])\n",
        "\n",
        "# Save the sentence level vectors\n",
        "np.save(os.path.join(DRIVE_PATH, 'X_train_w2v.npy'), X_train_w2v)\n",
        "np.save(os.path.join(DRIVE_PATH, 'X_test_w2v.npy'), X_test_w2v)\n",
        "print(f\"Word2Vec sentence vectors shapes: {X_train_w2v.shape} {X_test_w2v.shape}\")\n",
        "\n",
        "print(\"\\nTraining tokenizer with UNK token on training set...\")\n",
        "tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts([' '.join(tok) for tok in sentences_train]) # <-- ENTIRE DATASET\n",
        "\n",
        "# Convert train/test splits to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences([' '.join(tokens) for tokens in sentences_train])\n",
        "X_test_sequences = tokenizer.texts_to_sequences([' '.join(tokens) for tokens in sentences_test])\n",
        "\n",
        "# Calculate max sequence length\n",
        "all_sequences = tokenizer.texts_to_sequences([' '.join(tokens) for tokens in sentences_train])\n",
        "all_lengths = [len(seq) for seq in all_sequences]\n",
        "max_sequence_length = int(np.percentile(all_lengths, 95))\n",
        "print(f\"Max sequence length (95th percentile): {max_sequence_length}\")\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Create Embedding Matrix\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Creating embedding matrix with vocabulary size: {vocab_size}\")\n",
        "print(f\"UNK token ID: {tokenizer.word_index['<UNK>']}\")\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "oov_count = 0\n",
        "known_count = 0\n",
        "oov_vectors = {}\n",
        "\n",
        "print(f\"Using random initialization for OOV words (scale={OOV_INIT_SCALE})\")\n",
        "\n",
        "# Handle the <UNK> token\n",
        "unk_token_index = tokenizer.word_index['<UNK>']\n",
        "unk_vector = np.random.normal(0, OOV_INIT_SCALE, size=EMBEDDING_DIM)\n",
        "embedding_matrix[unk_token_index] = unk_vector\n",
        "print(f\"UNK vector example (first 5 dims): {unk_vector[:5]}\")\n",
        "\n",
        "# Handle all other words\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i == unk_token_index:\n",
        "        continue\n",
        "\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "        known_count += 1\n",
        "    else:\n",
        "        # Assign a unique, consistent random vector for reproducibility\n",
        "        if word not in oov_vectors:\n",
        "            oov_vectors[word] = np.random.normal(0, OOV_INIT_SCALE, size=EMBEDDING_DIM)\n",
        "        embedding_matrix[i] = oov_vectors[word]\n",
        "        oov_count += 1\n",
        "\n",
        "# OOV Analysis\n",
        "print(\"\\n=== OOV Analysis ===\")\n",
        "total_vocab_words = len(tokenizer.word_index) - 1 # Exclude <UNK>\n",
        "print(f\"Total vocabulary: {total_vocab_words} words\")\n",
        "print(f\"Known by Word2Vec: {known_count} words ({known_count/total_vocab_words*100:.2f}%)\")\n",
        "print(f\"OOV words: {oov_count} words ({oov_count/total_vocab_words*100:.2f}%)\")\n",
        "print(f\"OOV strategy: Each of {oov_count} OOV words gets unique random vector\")\n",
        "\n",
        "# Save sequence data\n",
        "np.save(os.path.join(DRIVE_PATH, 'w2v_embedding_matrix.npy'), embedding_matrix)\n",
        "np.save(os.path.join(DRIVE_PATH, 'X_train_w2v_sequences.npy'), X_train_padded)\n",
        "np.save(os.path.join(DRIVE_PATH, 'X_test_w2v_sequences.npy'), X_test_padded)\n",
        "joblib.dump(tokenizer, os.path.join(DRIVE_PATH, 'w2v_tokenizer.pkl'))\n",
        "\n",
        "print(\"\\nProcessing completed successfully!\")\n",
        "\n",
        "print(\"\\n=== Environment Info (Include in Thesis Appendix) ===\")\n",
        "print(f\"OOV initialization: random normal (mean=0, std={OOV_INIT_SCALE})\")\n",
        "print(f\"Word2Vec min_count: {W2V_PARAMS['min_count']}\")\n",
        "print(f\"Embedding dimensions: {EMBEDDING_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsFwzUq4t09e",
        "outputId": "1757af4b-d6e1-4c9d-c0ac-6daf5c1827f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF shapes: (42144, 1000) (10536, 1000)\n",
            "OOV rate in test set: 95.91%\n"
          ]
        }
      ],
      "source": [
        "#TF-IDF\n",
        "\n",
        "X_train_statement = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_statement.pkl')\n",
        "X_test_statement = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_statement.pkl')\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2), stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_statement)\n",
        "X_test_tfidf = tfidf.transform(X_test_statement)\n",
        "\n",
        "scipy.sparse.save_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz', X_train_tfidf)\n",
        "scipy.sparse.save_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_tfidf.npz', X_test_tfidf)\n",
        "joblib.dump(tfidf, '/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_test_tfidf.shape)\n",
        "\n",
        "train_vocab = set(tfidf.get_feature_names_out())\n",
        "test_words = set()\n",
        "for doc in X_test_statement:\n",
        "    test_words.update(doc.split())\n",
        "oov_rate = len(test_words - train_vocab) / len(test_words) if test_words else 0\n",
        "print(f\"OOV rate in test set: {oov_rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj0B3o0w0mSb"
      },
      "source": [
        "# FastText Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "m9ZwDWNA0puk",
        "outputId": "d6673457-b175-4864-93cc-644e921ab58a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LogisticRegression\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.5510, WeightedF1=0.6229, HierF1=0.8012\n",
            "Fold 2 done. MacroF1=0.5433, WeightedF1=0.6195, HierF1=0.7986\n",
            "Fold 3 done. MacroF1=0.5312, WeightedF1=0.6035, HierF1=0.7908\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1312099468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         ])\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 )\n\u001b[0;32m--> 526\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             ]\n\u001b[0;32m--> 451\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  **options)\n\u001b[1;32m    712\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    714\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    715\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0m\u001b[1;32m     48\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# FLAT FASTTEXT ML\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define machine learning models to test\n",
        "ml_models = {\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# MACHINE LEARNING MODELS LOOP\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "        X_tr = X_train_fasttext[train_idx]\n",
        "        X_val = X_train_fasttext[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        model = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42)),\n",
        "            ('clf', clone(model_class))\n",
        "        ])\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-FastText-{model_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{model_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{model_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{model_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2LgJPsaM1M3",
        "outputId": "05a6f8a8-9d5e-4809-cbaa-4cda3752ffb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training GRU - FAIR COMPARISON\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.7296, WeightedF1=0.7508, HierF1=0.8898\n",
            "Fold 2 done. MacroF1=0.6995, WeightedF1=0.7477, HierF1=0.8810\n",
            "Fold 3 done. MacroF1=0.6887, WeightedF1=0.7234, HierF1=0.8700\n",
            "Fold 4 done. MacroF1=0.6890, WeightedF1=0.7332, HierF1=0.8753\n",
            "Fold 5 done. MacroF1=0.7035, WeightedF1=0.7451, HierF1=0.8808\n",
            "\n",
            "GRU Final Results:\n",
            "Macro F1: 0.7021 ± 0.0149\n",
            "Weighted F1: 0.7400 ± 0.0102\n",
            "Hierarchical F1: 0.8794 ± 0.0066\n",
            "Total Training Time: 16220.30 seconds\n"
          ]
        }
      ],
      "source": [
        "# FLAT GRU\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# GRU IMPLEMENTATION\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training GRU - FAIR COMPARISON\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Start timing for GRU\n",
        "model_start_time = time.time()\n",
        "\n",
        "# Metrics containers for GRU\n",
        "p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "hF1s=[]; hPs=[]; hRs = []\n",
        "all_y_true = []; all_y_pred = []\n",
        "\n",
        "fold_idx = 0\n",
        "for train_idx, val_idx in skf.split(X_train_sequences, y_train):\n",
        "    fold_idx += 1\n",
        "\n",
        "    # Get the data splits\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "\n",
        "    # RESAMPLE PIPE\n",
        "    resample_pipe = ImbPipeline([\n",
        "        ('ros', RandomOverSampler(random_state=42)),\n",
        "        ('rus', RandomUnderSampler(random_state=42))\n",
        "    ])\n",
        "    X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "    y_tr_cat = tf.keras.utils.to_categorical(y_tr_hybrid, num_classes)\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=max_sequence_length,\n",
        "            trainable=False\n",
        "        ),\n",
        "        GRU(64),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr_hybrid, y_tr_cat,\n",
        "        validation_data=(X_val_seq, y_val_cat),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_val_seq, verbose=0), axis=1)\n",
        "\n",
        "    p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "    # Store fold results\n",
        "    p_macros.append(p_macro)\n",
        "    r_macros.append(r_macro)\n",
        "    f1_macros.append(f1_macro)\n",
        "    p_weights.append(p_weighted)\n",
        "    r_weights.append(r_weighted)\n",
        "    f1_weights.append(f1_weighted)\n",
        "    hF1s.append(hF1)\n",
        "    hPs.append(hP)\n",
        "    hRs.append(hR)\n",
        "\n",
        "    all_y_true.extend(list(y_val))\n",
        "    all_y_pred.extend(list(y_pred))\n",
        "\n",
        "    # Save individual fold result\n",
        "    all_results.append({\n",
        "            'Model': f'Flat-FastText-GRU-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "    print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "model_total_time = time.time() - model_start_time\n",
        "# Calculate and save final averages for this model\n",
        "all_results.append({\n",
        "        'Model': f'Flat-FastText-GRU-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "all_results.append({\n",
        "        'Model': f'Flat-FastText-GRU-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "print(f\"\\nGRU Final Results:\")\n",
        "print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOaxmQHPkjo7",
        "outputId": "6c2d47cb-130d-4eba-ad17-37efeb43e877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training HardVoting-XGBoost-GRU-LightGBM\n",
            "============================================================\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097878 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 1 done. MacroF1=0.6971, WeightedF1=0.7386, HierF1=0.8787\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096236 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 2 done. MacroF1=0.6872, WeightedF1=0.7357, HierF1=0.8776\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187566 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73220, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 3 done. MacroF1=0.6859, WeightedF1=0.7264, HierF1=0.8727\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088678 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 4 done. MacroF1=0.6984, WeightedF1=0.7352, HierF1=0.8765\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091639 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 5 done. MacroF1=0.6937, WeightedF1=0.7398, HierF1=0.8791\n",
            "\n",
            "HardVoting-XGBoost-GRU-LightGBM Final Results:\n",
            "Macro F1: 0.6925 ± 0.0051\n",
            "Weighted F1: 0.7352 ± 0.0047\n",
            "Hierarchical F1: 0.8769 ± 0.0023\n",
            "Total Training Time: 18311.78 seconds\n"
          ]
        }
      ],
      "source": [
        "# HARD VOTING ENSEMBLES\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# voting ensembles\n",
        "voting_ensembles = {\n",
        "    'HardVoting-XGBoost-GRU-LightGBM': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "    'HardVoting-XGBoost-GRU-RandomForest': ['XGBoost', 'GRU', 'RandomForest']\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# HARD VOTING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, model_names in voting_ensembles.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_fasttext[train_idx]\n",
        "        X_val_ft = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Apply resampling to FastText\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        # Apply resampling to sequences\n",
        "        X_tr_seq_hybrid, _ = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Store predictions from each model\n",
        "        fold_predictions = []\n",
        "\n",
        "        # Train each model in the ensemble\n",
        "        for model_name in model_names:\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                y_pred_gru = np.argmax(gru_model.predict(X_val_seq, verbose=0), axis=1)\n",
        "                fold_predictions.append(y_pred_gru)\n",
        "\n",
        "            else:\n",
        "                # ML Models\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "                y_pred_ml = model.predict(X_val_ft)\n",
        "                fold_predictions.append(y_pred_ml)\n",
        "\n",
        "        # Hard Voting: majority vote\n",
        "        fold_predictions = np.array(fold_predictions)\n",
        "        y_pred_vote = []\n",
        "\n",
        "        for i in range(len(y_val)):\n",
        "            votes = fold_predictions[:, i]\n",
        "            unique, counts = np.unique(votes, return_counts=True)\n",
        "            max_count = np.max(counts)\n",
        "            if np.sum(counts == max_count) > 1:\n",
        "                y_pred_vote.append(votes[0])  # Tie → use first model (XGBoost)\n",
        "            else:\n",
        "                y_pred_vote.append(unique[np.argmax(counts)])\n",
        "\n",
        "        y_pred = np.array(y_pred_vote)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUKE_JUZoU27",
        "outputId": "7a0b3e53-dabf-4cdf-aaf8-8b23491bc747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training WeightedSoft-XGBoost-GRU-RandomForest\n",
            "Model Weights: {'XGBoost': 0.33625349487418454, 'GRU': 0.3448275862068966, 'RandomForest': 0.31891891891891894}\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.7469, WeightedF1=0.7739, HierF1=0.8971\n",
            "Fold 2 done. MacroF1=0.7312, WeightedF1=0.7644, HierF1=0.8930\n",
            "Fold 3 done. MacroF1=0.7344, WeightedF1=0.7573, HierF1=0.8892\n",
            "Fold 4 done. MacroF1=0.7265, WeightedF1=0.7588, HierF1=0.8892\n",
            "Fold 5 done. MacroF1=0.7399, WeightedF1=0.7635, HierF1=0.8924\n",
            "\n",
            "WeightedSoft-XGBoost-GRU-RandomForest Final Results:\n",
            "Macro F1: 0.7358 ± 0.0070\n",
            "Weighted F1: 0.7636 ± 0.0058\n",
            "Hierarchical F1: 0.8922 ± 0.0029\n",
            "Total Training Time: 18810.27 seconds\n",
            "Used Weights: {'XGBoost': 0.33625349487418454, 'GRU': 0.3448275862068966, 'RandomForest': 0.31891891891891894}\n"
          ]
        }
      ],
      "source": [
        "# WEIGHTED SOFT VOTING ENSEMBLES\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define weighted soft voting ensembles with performance-based weights\n",
        "weighted_ensembles = {\n",
        "    'WeightedSoft-XGBoost-GRU-LightGBM': {\n",
        "        'models': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "        'weights': [0.7195, 0.7292, 0.7119]\n",
        "    },\n",
        "    'WeightedSoft-XGBoost-GRU-RandomForest': {\n",
        "        'models': ['XGBoost', 'GRU', 'RandomForest'],\n",
        "        'weights': [0.7195, 0.7292, 0.6844]\n",
        "    }\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# WEIGHTED SOFT VOTING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in weighted_ensembles.items():\n",
        "    model_names = ensemble_config['models']\n",
        "    performance_weights = ensemble_config['weights']\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    normalized_weights = np.array(performance_weights) / np.sum(performance_weights)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Model Weights: {dict(zip(model_names, normalized_weights))}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_fasttext[train_idx]\n",
        "        X_val_ft = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Apply resampling to FastText\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        # Apply resampling to sequences\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Store probability predictions from each model\n",
        "        fold_probabilities = []\n",
        "\n",
        "        # Train each model in the ensemble and get probabilities\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model - get probabilities\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                # Get probability predictions from GRU\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                fold_probabilities.append(y_proba_gru)\n",
        "\n",
        "            else:\n",
        "                # ML Models\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "                y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                fold_probabilities.append(y_proba_ml)\n",
        "\n",
        "        # Weighted Soft Voting: weighted average of probabilities\n",
        "        weighted_probabilities = np.zeros_like(fold_probabilities[0])\n",
        "\n",
        "        for i, proba in enumerate(fold_probabilities):\n",
        "            # Apply normalized performance weight to each model's probabilities\n",
        "            weighted_probabilities += normalized_weights[i] * proba\n",
        "\n",
        "        # Get final predictions (class with highest weighted probability)\n",
        "        y_pred = np.argmax(weighted_probabilities, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Used Weights: {dict(zip(model_names, normalized_weights))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50EFQBnYdYtj",
        "outputId": "0ebe18cb-61eb-4124-b85c-e68d16d7a291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training Stacking-XGB-GRU-RF-LR-Andry\n",
            "Base Models: ['XGBoost', 'GRU', 'RandomForest']\n",
            "Meta Model: LogisticRegression\n",
            "============================================================\n",
            "Stage 1: Generating Out-of-Fold predictions...\n",
            "Stage 1 completed: OOF predictions generated for all base models\n",
            "Stage 2: Training and evaluating meta-classifier...\n",
            "Fold 1 done. MacroF1=0.6902, WeightedF1=0.7319, HierF1=0.8740\n",
            "Fold 2 done. MacroF1=0.6816, WeightedF1=0.7337, HierF1=0.8743\n",
            "Fold 3 done. MacroF1=0.6498, WeightedF1=0.7020, HierF1=0.8546\n",
            "Fold 4 done. MacroF1=0.6574, WeightedF1=0.7204, HierF1=0.8596\n",
            "Fold 5 done. MacroF1=0.6654, WeightedF1=0.7211, HierF1=0.8642\n",
            "\n",
            "Stacking-XGB-GRU-RF-LR-Andry Final Results:\n",
            "Macro F1: 0.6689 ± 0.0150\n",
            "Weighted F1: 0.7218 ± 0.0113\n",
            "Hierarchical F1: 0.8653 ± 0.0078\n",
            "Total Training Time: 4427.52 seconds\n",
            "Base Models: ['XGBoost', 'GRU', 'RandomForest']\n",
            "Meta Model: LogisticRegression\n"
          ]
        }
      ],
      "source": [
        "# STACKING ENSEMBLES\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define stacking ensembles\n",
        "stacking_ensembles = {\n",
        "    'Stacking-XGB-GRU-LGBM-LR': {\n",
        "        'base_models': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    },\n",
        "    'Stacking-XGB-GRU-RF-LR': {\n",
        "        'base_models': ['XGBoost', 'GRU', 'RandomForest'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    }\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# STACKING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in stacking_ensembles.items():\n",
        "    base_model_names = ensemble_config['base_models']\n",
        "    meta_model = ensemble_config['meta_model']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Generate OOF Predictions\n",
        "    print(\"Stage 1: Generating Out-of-Fold predictions...\")\n",
        "\n",
        "    # Initialize OOF arrays for base model predictions\n",
        "    oof_predictions = {}\n",
        "    for model_name in base_model_names:\n",
        "        oof_predictions[model_name] = np.zeros((len(X_train_fasttext), num_classes))\n",
        "\n",
        "    # Generate OOF predictions for each base model\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_fasttext[train_idx]\n",
        "        X_val_ft = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "         # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Apply resampling to FastText\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        # Apply resampling to sequences\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Train each base model and get OOF predictions\n",
        "        for model_name in base_model_names:\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                # Get probability predictions for validation set\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                oof_predictions['GRU'][val_idx] = y_proba_gru\n",
        "\n",
        "            else:\n",
        "                # ML Models\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'SVM-rbf':\n",
        "                    model = SVC(kernel='rbf', random_state=42, probability=True)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "\n",
        "                # Get probability predictions for validation set\n",
        "                if model_name == 'SVM-rbf':\n",
        "                    y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                else:\n",
        "                    y_proba_ml = model.predict_proba(X_val_ft)\n",
        "\n",
        "                oof_predictions[model_name][val_idx] = y_proba_ml\n",
        "\n",
        "    print(\"Stage 1 completed: OOF predictions generated for all base models\")\n",
        "\n",
        "    # Train and Evaluate Meta-Classifier\n",
        "    print(\"Stage 2: Training and evaluating meta-classifier...\")\n",
        "\n",
        "    # Create meta-features dataset\n",
        "    X_meta = np.hstack([oof_predictions[model_name] for model_name in base_model_names])\n",
        "\n",
        "    # Metrics containers for stacking ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    # Cross-validation on meta-features\n",
        "    for train_idx, val_idx in skf.split(X_meta, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get meta-features splits\n",
        "        X_tr_meta = X_meta[train_idx]\n",
        "        X_val_meta = X_meta[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # === GOLD STANDARD: RESAMPLING FOR META-CLASSIFIER ===\n",
        "        resample_pipe_meta = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        # Apply resampling to meta-features\n",
        "        X_tr_meta_hybrid, y_tr_hybrid = resample_pipe_meta.fit_resample(X_tr_meta, y_tr)\n",
        "\n",
        "        # Train meta-classifier\n",
        "        meta_model_clone = clone(meta_model)\n",
        "        meta_model_clone.fit(X_tr_meta_hybrid, y_tr_hybrid)\n",
        "\n",
        "        # Predict with meta-classifier\n",
        "        y_pred = meta_model_clone.predict(X_val_meta)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddhrkX5atLHu"
      },
      "outputs": [],
      "source": [
        "# recalibrate finding best model for each level lcpn\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "super_ancestor_sets = {\n",
        "    'Internal': {'Internal', 'Root'},\n",
        "    'External': {'External', 'Root'},\n",
        "    'Normal': {'Normal', 'Root'}\n",
        "}\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get dimensions for GRU\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define all models\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'GRU': 'gru_model'\n",
        "}\n",
        "\n",
        "# Performance tracking for all algorithms and levels\n",
        "algorithm_performance = {}\n",
        "all_results = []\n",
        "\n",
        "# Initialize performance tracking for all algorithms\n",
        "for algo_name in list(ml_models.keys()):\n",
        "    algorithm_performance[algo_name] = {\n",
        "        'root': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'internal': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'external': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'overall': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []}\n",
        "    }\n",
        "\n",
        "fold_no = 0\n",
        "for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "    fold_no += 1\n",
        "    print(f\"\\n---- Fold {fold_no} ----\")\n",
        "\n",
        "    # Split data\n",
        "    X_tr = X_train_fasttext[train_idx]\n",
        "    X_val = X_train_fasttext[val_idx]\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    # Test each algorithm\n",
        "    for algo_name in list(ml_models.keys()):\n",
        "        print(f\"\\nTesting {algo_name}...\")\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE ===\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        if algo_name == 'GRU':\n",
        "            # For GRU, use sequence data\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "        else:\n",
        "            # For ML models, use word2vec features\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Get corresponding super labels for the hybrid-sampled data\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        if algo_name == 'GRU':\n",
        "            # GRU Root Model\n",
        "            y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "            y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "            root_model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(len(super_labels), activation='softmax')\n",
        "            ])\n",
        "\n",
        "            root_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            root_model.fit(\n",
        "                X_tr_hybrid, y_super_tr_hybrid_cat,\n",
        "                validation_data=(X_val_seq, y_super_val_cat),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "        else:\n",
        "            # ML Root Model\n",
        "            root_model = clone(ml_models[algo_name])\n",
        "            root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            # Apply resampling to internal data\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            if algo_name == 'GRU':\n",
        "                # GRU Internal Model\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "                y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "                # Get validation data for internal level\n",
        "                internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "                if internal_val_mask.sum() > 0:\n",
        "                    X_val_internal = X_val_seq[internal_val_mask]\n",
        "                    y_val_internal = y_val[internal_val_mask]\n",
        "                    y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "                    y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "                    validation_data = (X_val_internal, y_val_internal_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                internal_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_internal_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                internal_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                internal_model.fit(\n",
        "                    X_tr_internal_hybrid, y_tr_internal_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                # ML Internal Model\n",
        "                if algo_name in ['XGBoost', 'LightGBM']:\n",
        "                    internal_label_encoder = LabelEncoder()\n",
        "                    y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                else:\n",
        "                    internal_label_encoder = None\n",
        "                    y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "                internal_model = clone(ml_models[algo_name])\n",
        "                internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            # Apply resampling to external data\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            if algo_name == 'GRU':\n",
        "                # GRU External Model\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                num_external_classes = len(np.unique(y_tr_external_hybrid_encoded))\n",
        "                y_tr_external_cat = tf.keras.utils.to_categorical(y_tr_external_hybrid_encoded, num_classes=num_external_classes)\n",
        "\n",
        "                # Get validation data for external level\n",
        "                external_val_mask = (y_super_val == list(super_labels).index('External'))\n",
        "                if external_val_mask.sum() > 0:\n",
        "                    X_val_external = X_val_seq[external_val_mask]\n",
        "                    y_val_external = y_val[external_val_mask]\n",
        "                    y_val_external_encoded = external_label_encoder.transform(y_val_external)\n",
        "                    y_val_external_cat = tf.keras.utils.to_categorical(y_val_external_encoded, num_classes=num_external_classes)\n",
        "                    validation_data = (X_val_external, y_val_external_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                external_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_external_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                external_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                external_model.fit(\n",
        "                    X_tr_external_hybrid, y_tr_external_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                # ML External Model\n",
        "                if algo_name in ['XGBoost', 'LightGBM']:\n",
        "                    external_label_encoder = LabelEncoder()\n",
        "                    y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                else:\n",
        "                    external_label_encoder = None\n",
        "                    y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "                external_model = clone(ml_models[algo_name])\n",
        "                external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        if algo_name == 'GRU':\n",
        "            # GRU Prediction\n",
        "            y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "            y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "        else:\n",
        "            # ML Prediction\n",
        "            y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                if algo_name == 'GRU':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if algo_name in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                        pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                if algo_name == 'GRU':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if algo_name in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                        pred_encoded = external_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:  # Normal\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics for ALL LEVELS\n",
        "        # =====================================================================\n",
        "        fold_time = time.time() - fold_start_time\n",
        "\n",
        "        # ROOT LEVEL METRICS\n",
        "        root_macro_p = precision_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_macro_r = recall_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_macro_f1 = f1_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_weighted_p = precision_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_weighted_r = recall_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_weighted_f1 = f1_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_hF1, root_hP, root_hR, _, _, _ = hierarchical_metrics_journal(y_super_val, y_super_val_pred, list(super_labels), super_ancestor_sets)\n",
        "\n",
        "        algorithm_performance[algo_name]['root']['macro_p'].append(root_macro_p)\n",
        "        algorithm_performance[algo_name]['root']['macro_r'].append(root_macro_r)\n",
        "        algorithm_performance[algo_name]['root']['macro_f1'].append(root_macro_f1)\n",
        "        algorithm_performance[algo_name]['root']['weighted_p'].append(root_weighted_p)\n",
        "        algorithm_performance[algo_name]['root']['weighted_r'].append(root_weighted_r)\n",
        "        algorithm_performance[algo_name]['root']['weighted_f1'].append(root_weighted_f1)\n",
        "        algorithm_performance[algo_name]['root']['hF1'].append(root_hF1)\n",
        "        algorithm_performance[algo_name]['root']['hP'].append(root_hP)\n",
        "        algorithm_performance[algo_name]['root']['hR'].append(root_hR)\n",
        "\n",
        "        # INTERNAL LEVEL METRICS\n",
        "        internal_true_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "        if internal_true_mask.sum() > 0:\n",
        "            idxs_internal = np.where(internal_true_mask)[0]\n",
        "            y_internal_true = y_val[idxs_internal]\n",
        "            y_internal_pred = y_val_final_pred[idxs_internal]\n",
        "\n",
        "            internal_macro_p = precision_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_macro_r = recall_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_macro_f1 = f1_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_weighted_p = precision_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_weighted_r = recall_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_weighted_f1 = f1_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_hF1, internal_hP, internal_hR, _, _, _ = hierarchical_metrics_journal(y_internal_true, y_internal_pred, labels, ancestor_sets)\n",
        "        else:\n",
        "            internal_macro_p = internal_macro_r = internal_macro_f1 = 0.0\n",
        "            internal_weighted_p = internal_weighted_r = internal_weighted_f1 = 0.0\n",
        "            internal_hF1 = internal_hP = internal_hR = 0.0\n",
        "\n",
        "        algorithm_performance[algo_name]['internal']['macro_p'].append(internal_macro_p)\n",
        "        algorithm_performance[algo_name]['internal']['macro_r'].append(internal_macro_r)\n",
        "        algorithm_performance[algo_name]['internal']['macro_f1'].append(internal_macro_f1)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_p'].append(internal_weighted_p)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_r'].append(internal_weighted_r)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_f1'].append(internal_weighted_f1)\n",
        "        algorithm_performance[algo_name]['internal']['hF1'].append(internal_hF1)\n",
        "        algorithm_performance[algo_name]['internal']['hP'].append(internal_hP)\n",
        "        algorithm_performance[algo_name]['internal']['hR'].append(internal_hR)\n",
        "\n",
        "        # EXTERNAL LEVEL METRICS\n",
        "        external_true_mask = (y_super_val == list(super_labels).index('External'))\n",
        "        if external_true_mask.sum() > 0:\n",
        "            idxs_external = np.where(external_true_mask)[0]\n",
        "            y_external_true = y_val[idxs_external]\n",
        "            y_external_pred = y_val_final_pred[idxs_external]\n",
        "\n",
        "            external_macro_p = precision_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_macro_r = recall_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_macro_f1 = f1_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_weighted_p = precision_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_weighted_r = recall_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_weighted_f1 = f1_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_hF1, external_hP, external_hR, _, _, _ = hierarchical_metrics_journal(y_external_true, y_external_pred, labels, ancestor_sets)\n",
        "        else:\n",
        "            external_macro_p = external_macro_r = external_macro_f1 = 0.0\n",
        "            external_weighted_p = external_weighted_r = external_weighted_f1 = 0.0\n",
        "            external_hF1 = external_hP = external_hR = 0.0\n",
        "\n",
        "        algorithm_performance[algo_name]['external']['macro_p'].append(external_macro_p)\n",
        "        algorithm_performance[algo_name]['external']['macro_r'].append(external_macro_r)\n",
        "        algorithm_performance[algo_name]['external']['macro_f1'].append(external_macro_f1)\n",
        "        algorithm_performance[algo_name]['external']['weighted_p'].append(external_weighted_p)\n",
        "        algorithm_performance[algo_name]['external']['weighted_r'].append(external_weighted_r)\n",
        "        algorithm_performance[algo_name]['external']['weighted_f1'].append(external_weighted_f1)\n",
        "        algorithm_performance[algo_name]['external']['hF1'].append(external_hF1)\n",
        "        algorithm_performance[algo_name]['external']['hP'].append(external_hP)\n",
        "        algorithm_performance[algo_name]['external']['hR'].append(external_hR)\n",
        "\n",
        "        # OVERALL METRICS\n",
        "        overall_macro_p = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_macro_r = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_macro_f1 = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_weighted_p = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_weighted_r = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_weighted_f1 = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_hF1, overall_hP, overall_hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        algorithm_performance[algo_name]['overall']['macro_p'].append(overall_macro_p)\n",
        "        algorithm_performance[algo_name]['overall']['macro_r'].append(overall_macro_r)\n",
        "        algorithm_performance[algo_name]['overall']['macro_f1'].append(overall_macro_f1)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_p'].append(overall_weighted_p)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_r'].append(overall_weighted_r)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_f1'].append(overall_weighted_f1)\n",
        "        algorithm_performance[algo_name]['overall']['hF1'].append(overall_hF1)\n",
        "        algorithm_performance[algo_name]['overall']['hP'].append(overall_hP)\n",
        "        algorithm_performance[algo_name]['overall']['hR'].append(overall_hR)\n",
        "\n",
        "        # Store fold results for Excel output\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-{algo_name}',\n",
        "            'Level': 'Overall',\n",
        "            'Fold': f'Fold_{fold_no}',\n",
        "            'Macro_Precision': overall_macro_p,\n",
        "            'Macro_Recall': overall_macro_r,\n",
        "            'Macro_F1': overall_macro_f1,\n",
        "            'Weighted_Precision': overall_weighted_p,\n",
        "            'Weighted_Recall': overall_weighted_r,\n",
        "            'Weighted_F1': overall_weighted_f1,\n",
        "            'Hierarchical_Precision': overall_hP,\n",
        "            'Hierarchical_Recall': overall_hR,\n",
        "            'Hierarchical_F1': overall_hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': fold_time\n",
        "        })\n",
        "\n",
        "        print(f\"{algo_name} - Root: {root_weighted_f1:.4f}, Internal: {internal_weighted_f1:.4f}, External: {external_weighted_f1:.4f}, Overall: {overall_weighted_f1:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CALCULATE FINAL MEANS AND STDS\n",
        "# =============================================================================\n",
        "for algo_name in algorithm_performance.keys():\n",
        "    for level in ['root', 'internal', 'external', 'overall']:\n",
        "        metrics = algorithm_performance[algo_name][level]\n",
        "\n",
        "        # Add Mean rows\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-{algo_name}',\n",
        "            'Level': level.capitalize(),\n",
        "            'Fold': 'Mean',\n",
        "            'Macro_Precision': np.mean(metrics['macro_p']),\n",
        "            'Macro_Recall': np.mean(metrics['macro_r']),\n",
        "            'Macro_F1': np.mean(metrics['macro_f1']),\n",
        "            'Weighted_Precision': np.mean(metrics['weighted_p']),\n",
        "            'Weighted_Recall': np.mean(metrics['weighted_r']),\n",
        "            'Weighted_F1': np.mean(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision': np.mean(metrics['hP']),\n",
        "            'Hierarchical_Recall': np.mean(metrics['hR']),\n",
        "            'Hierarchical_F1': np.mean(metrics['hF1']),\n",
        "            'Type': 'Mean',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        # Add Std rows\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-FastText-{algo_name}',\n",
        "            'Level': level.capitalize(),\n",
        "            'Fold': 'Std',\n",
        "            'Macro_Precision': np.std(metrics['macro_p']),\n",
        "            'Macro_Recall': np.std(metrics['macro_r']),\n",
        "            'Macro_F1': np.std(metrics['macro_f1']),\n",
        "            'Weighted_Precision': np.std(metrics['weighted_p']),\n",
        "            'Weighted_Recall': np.std(metrics['weighted_r']),\n",
        "            'Weighted_F1': np.std(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision': np.std(metrics['hP']),\n",
        "            'Hierarchical_Recall': np.std(metrics['hR']),\n",
        "            'Hierarchical_F1': np.std(metrics['hF1']),\n",
        "            'Type': 'Std',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "# =============================================================================\n",
        "# DISPLAY FINAL RESULTS\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL PERFORMANCE RESULTS FOR ALL ALGORITHMS AT EACH LEVEL\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "results_data = []\n",
        "for algo_name in algorithm_performance.keys():\n",
        "    for level in ['root', 'internal', 'external', 'overall']:\n",
        "        metrics = algorithm_performance[algo_name][level]\n",
        "\n",
        "        result_row = {\n",
        "            'Algorithm': algo_name,\n",
        "            'Level': level.capitalize(),\n",
        "            'Macro_Precision_Mean': np.mean(metrics['macro_p']),\n",
        "            'Macro_Precision_Std': np.std(metrics['macro_p']),\n",
        "            'Macro_Recall_Mean': np.mean(metrics['macro_r']),\n",
        "            'Macro_Recall_Std': np.std(metrics['macro_r']),\n",
        "            'Macro_F1_Mean': np.mean(metrics['macro_f1']),\n",
        "            'Macro_F1_Std': np.std(metrics['macro_f1']),\n",
        "            'Weighted_Precision_Mean': np.mean(metrics['weighted_p']),\n",
        "            'Weighted_Precision_Std': np.std(metrics['weighted_p']),\n",
        "            'Weighted_Recall_Mean': np.mean(metrics['weighted_r']),\n",
        "            'Weighted_Recall_Std': np.std(metrics['weighted_r']),\n",
        "            'Weighted_F1_Mean': np.mean(metrics['weighted_f1']),\n",
        "            'Weighted_F1_Std': np.std(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision_Mean': np.mean(metrics['hP']),\n",
        "            'Hierarchical_Precision_Std': np.std(metrics['hP']),\n",
        "            'Hierarchical_Recall_Mean': np.mean(metrics['hR']),\n",
        "            'Hierarchical_Recall_Std': np.std(metrics['hR']),\n",
        "            'Hierarchical_F1_Mean': np.mean(metrics['hF1']),\n",
        "            'Hierarchical_F1_Std': np.std(metrics['hF1'])\n",
        "        }\n",
        "        results_data.append(result_row)\n",
        "\n",
        "summary_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Display results by level\n",
        "for level in ['Root', 'Internal', 'External', 'Overall']:\n",
        "    print(f\"\\n{level} LEVEL PERFORMANCE:\")\n",
        "    print(\"-\" * 100)\n",
        "    level_results = summary_df[summary_df['Level'] == level]\n",
        "\n",
        "    for _, row in level_results.iterrows():\n",
        "        print(f\"{row['Algorithm']:20} | \"\n",
        "              f\"Macro F1: {row['Macro_F1_Mean']:.4f}±{row['Macro_F1_Std']:.4f} | \"\n",
        "              f\"Weighted F1: {row['Weighted_F1_Mean']:.4f}±{row['Weighted_F1_Std']:.4f} | \"\n",
        "              f\"Hierarchical F1: {row['Hierarchical_F1_Mean']:.4f}±{row['Hierarchical_F1_Std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m-W8YX6nXxH",
        "outputId": "e8ba4f54-3f85-43f7-ac42-b0de06e64cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LCPN-Hierarchical-SVM-linear\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.5363, WeightedF1=0.6184, HierF1=0.8000\n",
            "Fold 2 done. MacroF1=0.5314, WeightedF1=0.6147, HierF1=0.7996\n",
            "Fold 3 done. MacroF1=0.5278, WeightedF1=0.6065, HierF1=0.7962\n",
            "Fold 4 done. MacroF1=0.5316, WeightedF1=0.6093, HierF1=0.7961\n",
            "Fold 5 done. MacroF1=0.5278, WeightedF1=0.6068, HierF1=0.7938\n",
            "\n",
            "LCPN-SVM-linear Final Results:\n",
            "Macro F1: 0.5310 ± 0.0031\n",
            "Weighted F1: 0.6111 ± 0.0047\n",
            "Hierarchical F1: 0.7972 ± 0.0023\n",
            "Total Training Time: 3241.86 seconds\n",
            "\n",
            "============================================================\n",
            "Training LCPN-Hierarchical-SVM-rbf\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6149, WeightedF1=0.6942, HierF1=0.8455\n",
            "Fold 2 done. MacroF1=0.6017, WeightedF1=0.6840, HierF1=0.8433\n",
            "Fold 3 done. MacroF1=0.5981, WeightedF1=0.6771, HierF1=0.8388\n",
            "Fold 4 done. MacroF1=0.6043, WeightedF1=0.6807, HierF1=0.8400\n",
            "Fold 5 done. MacroF1=0.6054, WeightedF1=0.6817, HierF1=0.8409\n",
            "\n",
            "LCPN-SVM-rbf Final Results:\n",
            "Macro F1: 0.6049 ± 0.0056\n",
            "Weighted F1: 0.6836 ± 0.0058\n",
            "Hierarchical F1: 0.8417 ± 0.0024\n",
            "Total Training Time: 2993.17 seconds\n",
            "\n",
            "============================================================\n",
            "Training LCPN-Hierarchical-RandomForest\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.5997, WeightedF1=0.6812, HierF1=0.8517\n",
            "Fold 2 done. MacroF1=0.5992, WeightedF1=0.6805, HierF1=0.8510\n",
            "Fold 3 done. MacroF1=0.5888, WeightedF1=0.6672, HierF1=0.8448\n",
            "Fold 4 done. MacroF1=0.5921, WeightedF1=0.6732, HierF1=0.8465\n",
            "Fold 5 done. MacroF1=0.6081, WeightedF1=0.6843, HierF1=0.8524\n",
            "\n",
            "LCPN-RandomForest Final Results:\n",
            "Macro F1: 0.5976 ± 0.0067\n",
            "Weighted F1: 0.6773 ± 0.0062\n",
            "Hierarchical F1: 0.8493 ± 0.0031\n",
            "Total Training Time: 1321.19 seconds\n"
          ]
        }
      ],
      "source": [
        "# Uniform LCPN ML\n",
        "# Load common data\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define models\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# LCPN HIERARCHICAL MODELS LOOP\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training LCPN-Hierarchical-{model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Start timing for this model\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "        X_tr = X_train_fasttext[train_idx]\n",
        "        X_val = X_train_fasttext[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Get corresponding super labels\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        root_model = clone(model_class)\n",
        "        root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "        joblib.dump(root_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{model_name}_root_fold{fold_idx}.joblib')\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            # Apply resampling to internal data\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            # For tree-based models (XGBoost, LightGBM), re-encode labels to be consecutive\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                joblib.dump(internal_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{model_name}_internal_label_encoder_fold{fold_idx}.joblib')\n",
        "            else:\n",
        "                internal_label_encoder = None\n",
        "                y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "            internal_model = clone(model_class)\n",
        "            internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "            joblib.dump(internal_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{model_name}_internal_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            # Apply resampling to external data\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            # For tree-based models (XGBoost, LightGBM), we need to re-encode labels to be consecutive\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                joblib.dump(external_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{model_name}_external_label_encoder_fold{fold_idx}.joblib')\n",
        "            else:\n",
        "                external_label_encoder = None\n",
        "                y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "            external_model = clone(model_class)\n",
        "            external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "            joblib.dump(external_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{model_name}_external_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        # Get root predictions\n",
        "        y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = external_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:  # Normal\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    # Fallback to majority class\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-FastText-{model_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-FastText-{model_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-FastText-{model_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\nLCPN-{model_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZWaJT87WmmD",
        "outputId": "9096e997-b649-44af-d831-ff311eb68f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LCPN-Hierarchical-GRU - TRUE GOLD STANDARD\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "Training ROOT level...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level...\n",
            "  Internal validation samples: 5198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level...\n",
            "  External validation samples: 616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making hierarchical predictions...\n",
            "Fold 1 - MacroF1: 0.5653, WeightedF1: 0.7062, HierF1: 0.8689\n",
            "\n",
            "--- Fold 2 ---\n",
            "Training ROOT level...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level...\n",
            "  Internal validation samples: 5198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level...\n",
            "  External validation samples: 616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making hierarchical predictions...\n",
            "Fold 2 - MacroF1: 0.6609, WeightedF1: 0.7135, HierF1: 0.8647\n",
            "\n",
            "--- Fold 3 ---\n",
            "Training ROOT level...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level...\n",
            "  Internal validation samples: 5197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level...\n",
            "  External validation samples: 618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making hierarchical predictions...\n",
            "Fold 3 - MacroF1: 0.6937, WeightedF1: 0.7370, HierF1: 0.8752\n",
            "\n",
            "--- Fold 4 ---\n",
            "Training ROOT level...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level...\n",
            "  Internal validation samples: 5196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level...\n",
            "  External validation samples: 618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making hierarchical predictions...\n",
            "Fold 4 - MacroF1: 0.6898, WeightedF1: 0.7393, HierF1: 0.8769\n",
            "\n",
            "--- Fold 5 ---\n",
            "Training ROOT level...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level...\n",
            "  Internal validation samples: 5197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level...\n",
            "  External validation samples: 616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making hierarchical predictions...\n",
            "Fold 5 - MacroF1: 0.5476, WeightedF1: 0.6844, HierF1: 0.8576\n",
            "\n",
            "LCPN-GRU Final Results:\n",
            "Macro F1: 0.6315 ± 0.0626\n",
            "Weighted F1: 0.7161 ± 0.0204\n",
            "Hierarchical F1: 0.8687 ± 0.0070\n",
            "Total Training Time: 32643.91 seconds\n"
          ]
        }
      ],
      "source": [
        "# LCPN GRU\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# LCPN HIERARCHICAL GRU\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training LCPN-Hierarchical-GRU\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Start timing for GRU\n",
        "model_start_time = time.time()\n",
        "\n",
        "# Metrics containers for GRU\n",
        "p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "hF1s=[]; hPs=[]; hRs=[]\n",
        "all_y_true = []; all_y_pred = []\n",
        "\n",
        "fold_idx = 0\n",
        "for train_idx, val_idx in skf.split(X_train_sequences, y_train):\n",
        "    fold_idx += 1\n",
        "    print(f\"\\n--- Fold {fold_idx} ---\")\n",
        "\n",
        "    # Get sequence data splits\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    # SINGLE RESAMPLE PIPE\n",
        "    resample_pipe = ImbPipeline([\n",
        "        ('ros', RandomOverSampler(random_state=42)),\n",
        "        ('rus', RandomUnderSampler(random_state=42))\n",
        "    ])\n",
        "    X_tr_seq_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "    # Get corresponding super labels for the hybrid-sampled data\n",
        "    y_super_tr_hybrid = []\n",
        "    for leaf_label in y_tr_hybrid:\n",
        "        super_label = leaf_to_super_name[leaf_label]\n",
        "        super_idx = list(super_labels).index(super_label)\n",
        "        y_super_tr_hybrid.append(super_idx)\n",
        "    y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "    # =====================================================================\n",
        "    # 1) Train ROOT level classifier\n",
        "    # =====================================================================\n",
        "    print(\"Training ROOT level...\")\n",
        "    # Convert to categorical for GRU\n",
        "    y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "    y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "    root_model = Sequential([\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=max_sequence_length,\n",
        "            trainable=False\n",
        "        ),\n",
        "        GRU(64),\n",
        "        Dense(len(super_labels), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    root_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    root_model.fit(\n",
        "        X_tr_seq_hybrid, y_super_tr_hybrid_cat,\n",
        "        validation_data=(X_val_seq, y_super_val_cat),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "        verbose=0,\n",
        "    )\n",
        "    root_model.save(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_GRU_root_fold{fold_idx}.h5')\n",
        "\n",
        "    # =====================================================================\n",
        "    # 2) Train INTERNAL level classifier\n",
        "    # =====================================================================\n",
        "    internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "    if internal_mask_tr.sum() > 0:\n",
        "        print(\"Training INTERNAL level...\")\n",
        "        X_tr_internal_seq = X_tr_seq_hybrid[internal_mask_tr]\n",
        "        y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "        # Apply resampling to internal data\n",
        "        resample_internal = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_internal_seq_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal_seq, y_tr_internal)\n",
        "\n",
        "        # Re-encode internal labels to be consecutive\n",
        "        internal_label_encoder = LabelEncoder()\n",
        "        y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "\n",
        "        # Convert to categorical for GRU\n",
        "        num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "        y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "        # Get validation data for internal level\n",
        "        internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "        if internal_val_mask.sum() > 0:\n",
        "            X_val_internal_seq = X_val_seq[internal_val_mask]\n",
        "            y_val_internal = y_val[internal_val_mask]\n",
        "\n",
        "            # ✅ Re-encode validation labels using the same encoder\n",
        "            y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "            y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "            validation_data = (X_val_internal_seq, y_val_internal_cat)\n",
        "            monitor_metric = 'val_loss'\n",
        "            print(f\"  Internal validation samples: {internal_val_mask.sum()}\")\n",
        "        else:\n",
        "            validation_data = None\n",
        "            monitor_metric = 'loss'\n",
        "            print(\"  No internal validation samples, monitoring loss\")\n",
        "\n",
        "        internal_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(num_internal_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        internal_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        internal_model.fit(\n",
        "            X_tr_internal_seq_hybrid, y_tr_internal_cat,\n",
        "            validation_data=validation_data,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "        internal_model.save(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_GRU_internal_fold{fold_idx}.h5')\n",
        "        joblib.dump(internal_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_GRU_internal_label_encoder_fold{fold_idx}.joblib')\n",
        "    else:\n",
        "        print(f\"  No internal samples for training\")\n",
        "        internal_model = None\n",
        "        internal_label_encoder = None\n",
        "\n",
        "    # =====================================================================\n",
        "    # 3) Train EXTERNAL level classifier\n",
        "    # =====================================================================\n",
        "    external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "    if external_mask_tr.sum() > 0:\n",
        "        print(\"Training EXTERNAL level...\")\n",
        "        X_tr_external_seq = X_tr_seq_hybrid[external_mask_tr]\n",
        "        y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "        # Apply resampling to external data\n",
        "        resample_external = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_external_seq_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external_seq, y_tr_external)\n",
        "\n",
        "        # Re-encode external labels to be consecutive\n",
        "        external_label_encoder = LabelEncoder()\n",
        "        y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "\n",
        "        # Convert to categorical for GRU\n",
        "        num_external_classes = len(np.unique(y_tr_external_hybrid_encoded))\n",
        "        y_tr_external_cat = tf.keras.utils.to_categorical(y_tr_external_hybrid_encoded, num_classes=num_external_classes)\n",
        "\n",
        "        # Get validation data for external level\n",
        "        external_val_mask = (y_super_val == list(super_labels).index('External'))\n",
        "        if external_val_mask.sum() > 0:\n",
        "            X_val_external_seq = X_val_seq[external_val_mask]\n",
        "            y_val_external = y_val[external_val_mask]\n",
        "\n",
        "            # ✅ Re-encode validation labels using the same encoder\n",
        "            y_val_external_encoded = external_label_encoder.transform(y_val_external)\n",
        "            y_val_external_cat = tf.keras.utils.to_categorical(y_val_external_encoded, num_classes=num_external_classes)\n",
        "\n",
        "            validation_data = (X_val_external_seq, y_val_external_cat)\n",
        "            monitor_metric = 'val_loss'\n",
        "            print(f\"  External validation samples: {external_val_mask.sum()}\")\n",
        "        else:\n",
        "            validation_data = None\n",
        "            monitor_metric = 'loss'\n",
        "            print(\"  No external validation samples, monitoring loss\")\n",
        "\n",
        "        external_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(num_external_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        external_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        external_model.fit(\n",
        "            X_tr_external_seq_hybrid, y_tr_external_cat,\n",
        "            validation_data=validation_data,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "        external_model.save(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_GRU_external_fold{fold_idx}.h5')\n",
        "        joblib.dump(external_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_GRU_external_label_encoder_fold{fold_idx}.joblib')\n",
        "    else:\n",
        "        print(f\"  No external samples for training\")\n",
        "        external_model = None\n",
        "        external_label_encoder = None\n",
        "\n",
        "    # =====================================================================\n",
        "    # 4) Hierarchical Prediction\n",
        "    # =====================================================================\n",
        "    print(\"Making hierarchical predictions...\")\n",
        "    # Get root predictions\n",
        "    y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "    y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "\n",
        "    # Build final leaf predictions\n",
        "    y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "    # Find normal leaf index\n",
        "    normal_leaf_idx = None\n",
        "    for idx, name in leaf_index_to_name.items():\n",
        "        if name.lower() == 'normal' or name == 'Normal':\n",
        "            normal_leaf_idx = idx\n",
        "            break\n",
        "    if normal_leaf_idx is None:\n",
        "        normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "        normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "    # Apply hierarchical prediction logic\n",
        "    for i in range(len(y_super_val_pred)):\n",
        "        pred_sup = y_super_val_pred[i]\n",
        "\n",
        "        if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "            single_sample = X_val_seq[i:i+1]\n",
        "            pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "            # Decode back to original label using the encoder\n",
        "            y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "        elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "            single_sample = X_val_seq[i:i+1]\n",
        "            pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "            # Decode back to original label using the encoder\n",
        "            y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "        else:  # Normal\n",
        "            if normal_leaf_idx is not None:\n",
        "                y_val_final_pred[i] = normal_leaf_idx\n",
        "            else:\n",
        "                # Fallback to majority class\n",
        "                vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "    # =====================================================================\n",
        "    # 5) Calculate Metrics\n",
        "    # =====================================================================\n",
        "    p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "    # Store fold results\n",
        "    p_macros.append(p_macro)\n",
        "    r_macros.append(r_macro)\n",
        "    f1_macros.append(f1_macro)\n",
        "    p_weights.append(p_weighted)\n",
        "    r_weights.append(r_weighted)\n",
        "    f1_weights.append(f1_weighted)\n",
        "    hF1s.append(hF1)\n",
        "    hPs.append(hP)\n",
        "    hRs.append(hR)\n",
        "\n",
        "    all_y_true.extend(list(y_val))\n",
        "    all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "    # Save individual fold result\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-FastText-GRU-skipgram',\n",
        "        'Fold': f'Fold_{fold_idx}',\n",
        "        'Macro_Precision': p_macro,\n",
        "        'Macro_Recall': r_macro,\n",
        "        'Macro_F1': f1_macro,\n",
        "        'Weighted_Precision': p_weighted,\n",
        "        'Weighted_Recall': r_weighted,\n",
        "        'Weighted_F1': f1_weighted,\n",
        "        'Hierarchical_Precision': hP,\n",
        "        'Hierarchical_Recall': hR,\n",
        "        'Hierarchical_F1': hF1,\n",
        "        'Type': 'Fold',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {fold_idx} - MacroF1: {f1_macro:.4f}, WeightedF1: {f1_weighted:.4f}, HierF1: {hF1:.4f}\")\n",
        "\n",
        "# Calculate total time for this model\n",
        "model_total_time = time.time() - model_start_time\n",
        "\n",
        "# Calculate and save final averages for this model\n",
        "all_results.append({\n",
        "    'Model': f'LCPN-FastText-GRU-skipgram',\n",
        "    'Fold': 'Mean',\n",
        "    'Macro_Precision': np.mean(p_macros),\n",
        "    'Macro_Recall': np.mean(r_macros),\n",
        "    'Macro_F1': np.mean(f1_macros),\n",
        "    'Weighted_Precision': np.mean(p_weights),\n",
        "    'Weighted_Recall': np.mean(r_weights),\n",
        "    'Weighted_F1': np.mean(f1_weights),\n",
        "    'Hierarchical_Precision': np.mean(hPs),\n",
        "    'Hierarchical_Recall': np.mean(hRs),\n",
        "    'Hierarchical_F1': np.mean(hF1s),\n",
        "    'Type': 'Mean',\n",
        "    'Time': model_total_time\n",
        "})\n",
        "\n",
        "all_results.append({\n",
        "    'Model': f'LCPN-FastText-GRU-skipgram',\n",
        "    'Fold': 'Std',\n",
        "    'Macro_Precision': np.std(p_macros),\n",
        "    'Macro_Recall': np.std(r_macros),\n",
        "    'Macro_F1': np.std(f1_macros),\n",
        "    'Weighted_Precision': np.std(p_weights),\n",
        "    'Weighted_Recall': np.std(r_weights),\n",
        "    'Weighted_F1': np.std(f1_weights),\n",
        "    'Hierarchical_Precision': np.std(hPs),\n",
        "    'Hierarchical_Recall': np.std(hRs),\n",
        "    'Hierarchical_F1': np.std(hF1s),\n",
        "    'Type': 'Std',\n",
        "    'Time': ''\n",
        "})\n",
        "\n",
        "print(f\"\\nLCPN-GRU Final Results:\")\n",
        "print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Hybrid LCPN Combo SEPERATED\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "# Load common data\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "\n",
        "# Define all algorithms\n",
        "algorithms = ['GRU', 'XGBoost', 'LightGBM', 'SVM-rbf']\n",
        "\n",
        "# COLAB INSTANCE A\n",
        "allowed_roots = ['GRU']\n",
        "\n",
        "# COLAB INSTANCE B\n",
        "# allowed_roots = ['LightGBM', 'XGBoost']\n",
        "\n",
        "# COLAB INSTANCE C\n",
        "# allowed_roots = ['SVM-rbf']\n",
        "\n",
        "print(f\"THIS INSTANCE WILL TEST: Root algorithms = {allowed_roots}\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN COMBINATION SEARCH\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"HYBRID LCPN COMBINATION SEARCH - WITH LABEL ENCODERS (FastText)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# For each fold, test all combinations\n",
        "fold_no = 0\n",
        "for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "    fold_no += 1\n",
        "    print(f\"\\n---- Fold {fold_no} ----\")\n",
        "\n",
        "    # Get both FastText embeddings and sequences for this fold\n",
        "    X_tr_emb = X_train_fasttext[train_idx]\n",
        "    X_val_emb = X_train_fasttext[val_idx]\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    # Load all models AND label encoders for this fold\n",
        "    print(\"Loading models and label encoders for this fold...\")\n",
        "    models = {}\n",
        "    label_encoders = {}\n",
        "\n",
        "    for algo in algorithms:\n",
        "        # Load root model\n",
        "        try:\n",
        "            if algo == 'GRU':\n",
        "                model = tf.keras.models.load_model(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_root_fold{fold_no}.h5')\n",
        "                model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                models[f'{algo}_root'] = model\n",
        "            else:\n",
        "                models[f'{algo}_root'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_root_fold{fold_no}.joblib')\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not load root model for {algo}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Load internal model and label encoder\n",
        "        try:\n",
        "            if algo == 'GRU':\n",
        "                model = tf.keras.models.load_model(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_internal_fold{fold_no}.h5')\n",
        "                # Recompile to fix the metrics warning\n",
        "                model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                models[f'{algo}_internal'] = model\n",
        "            else:\n",
        "                models[f'{algo}_internal'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_internal_fold{fold_no}.joblib')\n",
        "\n",
        "            # load label encoder for tree-based models and GRU\n",
        "            if algo in ['XGBoost', 'LightGBM', 'GRU']:\n",
        "                try:\n",
        "                    label_encoders[f'{algo}_internal'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_internal_label_encoder_fold{fold_no}.joblib')\n",
        "                    print(f\"  Loaded internal label encoder for {algo}\")\n",
        "                except:\n",
        "                    print(f\"  Warning: Could not load internal label encoder for {algo}\")\n",
        "                    label_encoders[f'{algo}_internal'] = None\n",
        "        except Exception as e:\n",
        "            models[f'{algo}_internal'] = None\n",
        "\n",
        "        # Load external model and label encoder\n",
        "        try:\n",
        "            if algo == 'GRU':\n",
        "                model = tf.keras.models.load_model(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_external_fold{fold_no}.h5')\n",
        "                model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                models[f'{algo}_external'] = model\n",
        "            else:\n",
        "                models[f'{algo}_external'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_external_fold{fold_no}.joblib')\n",
        "\n",
        "            if algo in ['XGBoost', 'LightGBM', 'GRU']:\n",
        "                try:\n",
        "                    label_encoders[f'{algo}_external'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_{algo}_external_label_encoder_fold{fold_no}.joblib')\n",
        "                    print(f\"  Loaded external label encoder for {algo}\")\n",
        "                except:\n",
        "                    print(f\"  Warning: Could not load external label encoder for {algo}\")\n",
        "                    label_encoders[f'{algo}_external'] = None\n",
        "        except Exception as e:\n",
        "            models[f'{algo}_external'] = None\n",
        "\n",
        "    print(\"Models and encoders loaded. Testing hybrid combinations...\")\n",
        "\n",
        "    combination_count = 0\n",
        "\n",
        "    for root_algo in allowed_roots:\n",
        "        for internal_algo in algorithms:\n",
        "            for external_algo in algorithms:\n",
        "                combination_count += 1\n",
        "\n",
        "                if (f'{root_algo}_root' not in models or models[f'{root_algo}_root'] is None):\n",
        "                    continue\n",
        "\n",
        "                # Get the models for this combination\n",
        "                root_model = models[f'{root_algo}_root']\n",
        "                internal_model = models[f'{internal_algo}_internal'] if f'{internal_algo}_internal' in models else None\n",
        "                external_model = models[f'{external_algo}_external'] if f'{external_algo}_external' in models else None\n",
        "\n",
        "                # Get label encoders for tree-based models and GRU\n",
        "                internal_encoder = label_encoders.get(f'{internal_algo}_internal', None)\n",
        "                external_encoder = label_encoders.get(f'{external_algo}_external', None)\n",
        "\n",
        "                # =====================================================================\n",
        "                # Hierarchical Prediction\n",
        "                # =====================================================================\n",
        "                # Get root predictions\n",
        "                if root_algo == 'GRU':\n",
        "                    # GRU uses sequence data\n",
        "                    y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "                    y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "                else:\n",
        "                    # ML models use embeddings\n",
        "                    y_super_val_pred = root_model.predict(X_val_emb)\n",
        "\n",
        "                # Build final leaf predictions\n",
        "                y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "                # Find normal leaf index\n",
        "                normal_leaf_idx = None\n",
        "                for idx, name in leaf_index_to_name.items():\n",
        "                    if name.lower() == 'normal' or name == 'Normal':\n",
        "                        normal_leaf_idx = idx\n",
        "                        break\n",
        "                if normal_leaf_idx is None:\n",
        "                    normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "                    normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "                # Apply hierarchical prediction logic\n",
        "                for i in range(len(y_super_val_pred)):\n",
        "                    pred_sup = y_super_val_pred[i]\n",
        "\n",
        "                    if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                        if internal_algo == 'GRU':\n",
        "                            single_sample = X_val_seq[i:i+1]\n",
        "                            pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "                            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                            if internal_encoder is not None:\n",
        "                                y_val_final_pred[i] = internal_encoder.inverse_transform([pred_encoded])[0]\n",
        "                            else:\n",
        "                                y_val_final_pred[i] = pred_encoded\n",
        "                        else:\n",
        "                            single_sample = X_val_emb[i].reshape(1, -1)\n",
        "                            if internal_algo in ['XGBoost', 'LightGBM'] and internal_encoder is not None:\n",
        "                                pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                                y_val_final_pred[i] = internal_encoder.inverse_transform([pred_encoded])[0]\n",
        "                            else:\n",
        "                                y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "                    elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                        if external_algo == 'GRU':\n",
        "                            single_sample = X_val_seq[i:i+1]\n",
        "                            pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "                            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                            if external_encoder is not None:\n",
        "                                y_val_final_pred[i] = external_encoder.inverse_transform([pred_encoded])[0]\n",
        "                            else:\n",
        "                                y_val_final_pred[i] = pred_encoded\n",
        "                        else:\n",
        "                            single_sample = X_val_emb[i].reshape(1, -1)\n",
        "                            if external_algo in ['XGBoost', 'LightGBM'] and external_encoder is not None:\n",
        "                                pred_encoded = external_model.predict(single_sample)[0]\n",
        "                                y_val_final_pred[i] = external_encoder.inverse_transform([pred_encoded])[0]\n",
        "                            else:\n",
        "                                y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "                    else:\n",
        "                        if normal_leaf_idx is not None:\n",
        "                            y_val_final_pred[i] = normal_leaf_idx\n",
        "                        else:\n",
        "                            vals, counts = np.unique(y_tr, return_counts=True)\n",
        "                            y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "                # =====================================================================\n",
        "                # Calculate Metrics\n",
        "                # =====================================================================\n",
        "                p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "                # Store results\n",
        "                hybrid_name = f'Hybrid-FastText-{root_algo}-{internal_algo}-{external_algo}'\n",
        "\n",
        "                all_results.append({\n",
        "                    'Model': hybrid_name,\n",
        "                    'Root': root_algo,\n",
        "                    'Internal': internal_algo,\n",
        "                    'External': external_algo,\n",
        "                    'Fold': f'Fold_{fold_no}',\n",
        "                    'Macro_Precision': p_macro,\n",
        "                    'Macro_Recall': r_macro,\n",
        "                    'Macro_F1': f1_macro,\n",
        "                    'Weighted_Precision': p_weighted,\n",
        "                    'Weighted_Recall': r_weighted,\n",
        "                    'Weighted_F1': f1_weighted,\n",
        "                    'Hierarchical_Precision': hP,\n",
        "                    'Hierarchical_Recall': hR,\n",
        "                    'Hierarchical_F1': hF1,\n",
        "                    'Type': 'Fold',\n",
        "                    'Instance': 'A'\n",
        "                })\n",
        "\n",
        "                print(f\"  {hybrid_name}: Weighted F1 = {f1_weighted:.4f}\")\n",
        "\n",
        "    print(f\"Fold {fold_no} completed: {combination_count} combinations tested\")\n",
        "\n",
        "final_results_df = pd.DataFrame(all_results)\n",
        "final_results_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_hybrid_lcpn_instance_A.csv', index=False)\n",
        "print(f\"\\nInstance completed! Results saved for: {allowed_roots}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4YCZPGmasYg",
        "outputId": "cee9b241-c4e8-41be-e377-d260a61d6101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THIS INSTANCE WILL TEST: Root algorithms = ['GRU']\n",
            "\n",
            "================================================================================\n",
            "HYBRID LCPN COMBINATION SEARCH - WITH LABEL ENCODERS (FastText)\n",
            "================================================================================\n",
            "\n",
            "---- Fold 1 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for GRU\n",
            "  Loaded external label encoder for GRU\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-FastText-GRU-GRU-GRU: Weighted F1 = 0.7062\n",
            "  Hybrid-FastText-GRU-GRU-XGBoost: Weighted F1 = 0.7473\n",
            "  Hybrid-FastText-GRU-GRU-LightGBM: Weighted F1 = 0.7473\n",
            "  Hybrid-FastText-GRU-GRU-SVM-rbf: Weighted F1 = 0.7468\n",
            "  Hybrid-FastText-GRU-XGBoost-GRU: Weighted F1 = 0.6992\n",
            "  Hybrid-FastText-GRU-XGBoost-XGBoost: Weighted F1 = 0.7403\n",
            "  Hybrid-FastText-GRU-XGBoost-LightGBM: Weighted F1 = 0.7403\n",
            "  Hybrid-FastText-GRU-XGBoost-SVM-rbf: Weighted F1 = 0.7398\n",
            "  Hybrid-FastText-GRU-LightGBM-GRU: Weighted F1 = 0.6949\n",
            "  Hybrid-FastText-GRU-LightGBM-XGBoost: Weighted F1 = 0.7360\n",
            "  Hybrid-FastText-GRU-LightGBM-LightGBM: Weighted F1 = 0.7360\n",
            "  Hybrid-FastText-GRU-LightGBM-SVM-rbf: Weighted F1 = 0.7355\n",
            "  Hybrid-FastText-GRU-SVM-rbf-GRU: Weighted F1 = 0.6914\n",
            "  Hybrid-FastText-GRU-SVM-rbf-XGBoost: Weighted F1 = 0.7325\n",
            "  Hybrid-FastText-GRU-SVM-rbf-LightGBM: Weighted F1 = 0.7325\n",
            "  Hybrid-FastText-GRU-SVM-rbf-SVM-rbf: Weighted F1 = 0.7320\n",
            "Fold 1 completed: 16 combinations tested\n",
            "\n",
            "---- Fold 2 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for GRU\n",
            "  Loaded external label encoder for GRU\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-FastText-GRU-GRU-GRU: Weighted F1 = 0.7135\n",
            "  Hybrid-FastText-GRU-GRU-XGBoost: Weighted F1 = 0.7140\n",
            "  Hybrid-FastText-GRU-GRU-LightGBM: Weighted F1 = 0.7142\n",
            "  Hybrid-FastText-GRU-GRU-SVM-rbf: Weighted F1 = 0.7132\n",
            "  Hybrid-FastText-GRU-XGBoost-GRU: Weighted F1 = 0.7159\n",
            "  Hybrid-FastText-GRU-XGBoost-XGBoost: Weighted F1 = 0.7163\n",
            "  Hybrid-FastText-GRU-XGBoost-LightGBM: Weighted F1 = 0.7165\n",
            "  Hybrid-FastText-GRU-XGBoost-SVM-rbf: Weighted F1 = 0.7155\n",
            "  Hybrid-FastText-GRU-LightGBM-GRU: Weighted F1 = 0.7125\n",
            "  Hybrid-FastText-GRU-LightGBM-XGBoost: Weighted F1 = 0.7130\n",
            "  Hybrid-FastText-GRU-LightGBM-LightGBM: Weighted F1 = 0.7132\n",
            "  Hybrid-FastText-GRU-LightGBM-SVM-rbf: Weighted F1 = 0.7122\n",
            "  Hybrid-FastText-GRU-SVM-rbf-GRU: Weighted F1 = 0.7020\n",
            "  Hybrid-FastText-GRU-SVM-rbf-XGBoost: Weighted F1 = 0.7025\n",
            "  Hybrid-FastText-GRU-SVM-rbf-LightGBM: Weighted F1 = 0.7027\n",
            "  Hybrid-FastText-GRU-SVM-rbf-SVM-rbf: Weighted F1 = 0.7017\n",
            "Fold 2 completed: 16 combinations tested\n",
            "\n",
            "---- Fold 3 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for GRU\n",
            "  Loaded external label encoder for GRU\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-FastText-GRU-GRU-GRU: Weighted F1 = 0.7370\n",
            "  Hybrid-FastText-GRU-GRU-XGBoost: Weighted F1 = 0.7358\n",
            "  Hybrid-FastText-GRU-GRU-LightGBM: Weighted F1 = 0.7351\n",
            "  Hybrid-FastText-GRU-GRU-SVM-rbf: Weighted F1 = 0.7364\n",
            "  Hybrid-FastText-GRU-XGBoost-GRU: Weighted F1 = 0.7157\n",
            "  Hybrid-FastText-GRU-XGBoost-XGBoost: Weighted F1 = 0.7146\n",
            "  Hybrid-FastText-GRU-XGBoost-LightGBM: Weighted F1 = 0.7139\n",
            "  Hybrid-FastText-GRU-XGBoost-SVM-rbf: Weighted F1 = 0.7152\n",
            "  Hybrid-FastText-GRU-LightGBM-GRU: Weighted F1 = 0.7156\n",
            "  Hybrid-FastText-GRU-LightGBM-XGBoost: Weighted F1 = 0.7144\n",
            "  Hybrid-FastText-GRU-LightGBM-LightGBM: Weighted F1 = 0.7137\n",
            "  Hybrid-FastText-GRU-LightGBM-SVM-rbf: Weighted F1 = 0.7150\n",
            "  Hybrid-FastText-GRU-SVM-rbf-GRU: Weighted F1 = 0.7053\n",
            "  Hybrid-FastText-GRU-SVM-rbf-XGBoost: Weighted F1 = 0.7042\n",
            "  Hybrid-FastText-GRU-SVM-rbf-LightGBM: Weighted F1 = 0.7035\n",
            "  Hybrid-FastText-GRU-SVM-rbf-SVM-rbf: Weighted F1 = 0.7048\n",
            "Fold 3 completed: 16 combinations tested\n",
            "\n",
            "---- Fold 4 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for GRU\n",
            "  Loaded external label encoder for GRU\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-FastText-GRU-GRU-GRU: Weighted F1 = 0.7393\n",
            "  Hybrid-FastText-GRU-GRU-XGBoost: Weighted F1 = 0.7385\n",
            "  Hybrid-FastText-GRU-GRU-LightGBM: Weighted F1 = 0.7382\n",
            "  Hybrid-FastText-GRU-GRU-SVM-rbf: Weighted F1 = 0.7384\n",
            "  Hybrid-FastText-GRU-XGBoost-GRU: Weighted F1 = 0.7265\n",
            "  Hybrid-FastText-GRU-XGBoost-XGBoost: Weighted F1 = 0.7256\n",
            "  Hybrid-FastText-GRU-XGBoost-LightGBM: Weighted F1 = 0.7254\n",
            "  Hybrid-FastText-GRU-XGBoost-SVM-rbf: Weighted F1 = 0.7255\n",
            "  Hybrid-FastText-GRU-LightGBM-GRU: Weighted F1 = 0.7285\n",
            "  Hybrid-FastText-GRU-LightGBM-XGBoost: Weighted F1 = 0.7277\n",
            "  Hybrid-FastText-GRU-LightGBM-LightGBM: Weighted F1 = 0.7275\n",
            "  Hybrid-FastText-GRU-LightGBM-SVM-rbf: Weighted F1 = 0.7276\n",
            "  Hybrid-FastText-GRU-SVM-rbf-GRU: Weighted F1 = 0.7194\n",
            "  Hybrid-FastText-GRU-SVM-rbf-XGBoost: Weighted F1 = 0.7185\n",
            "  Hybrid-FastText-GRU-SVM-rbf-LightGBM: Weighted F1 = 0.7183\n",
            "  Hybrid-FastText-GRU-SVM-rbf-SVM-rbf: Weighted F1 = 0.7184\n",
            "Fold 4 completed: 16 combinations tested\n",
            "\n",
            "---- Fold 5 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for GRU\n",
            "  Loaded external label encoder for GRU\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-FastText-GRU-GRU-GRU: Weighted F1 = 0.6844\n",
            "  Hybrid-FastText-GRU-GRU-XGBoost: Weighted F1 = 0.7214\n",
            "  Hybrid-FastText-GRU-GRU-LightGBM: Weighted F1 = 0.7216\n",
            "  Hybrid-FastText-GRU-GRU-SVM-rbf: Weighted F1 = 0.7226\n",
            "  Hybrid-FastText-GRU-XGBoost-GRU: Weighted F1 = 0.6807\n",
            "  Hybrid-FastText-GRU-XGBoost-XGBoost: Weighted F1 = 0.7177\n",
            "  Hybrid-FastText-GRU-XGBoost-LightGBM: Weighted F1 = 0.7178\n",
            "  Hybrid-FastText-GRU-XGBoost-SVM-rbf: Weighted F1 = 0.7189\n",
            "  Hybrid-FastText-GRU-LightGBM-GRU: Weighted F1 = 0.6796\n",
            "  Hybrid-FastText-GRU-LightGBM-XGBoost: Weighted F1 = 0.7166\n",
            "  Hybrid-FastText-GRU-LightGBM-LightGBM: Weighted F1 = 0.7168\n",
            "  Hybrid-FastText-GRU-LightGBM-SVM-rbf: Weighted F1 = 0.7178\n",
            "  Hybrid-FastText-GRU-SVM-rbf-GRU: Weighted F1 = 0.6662\n",
            "  Hybrid-FastText-GRU-SVM-rbf-XGBoost: Weighted F1 = 0.7032\n",
            "  Hybrid-FastText-GRU-SVM-rbf-LightGBM: Weighted F1 = 0.7034\n",
            "  Hybrid-FastText-GRU-SVM-rbf-SVM-rbf: Weighted F1 = 0.7044\n",
            "Fold 5 completed: 16 combinations tested\n",
            "\n",
            "Instance completed! Results saved for: ['GRU']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load results from all instances\n",
        "results_a = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_hybrid_lcpn_instance_A.csv')\n",
        "results_b = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_hybrid_lcpn_instance_B.csv')\n",
        "results_c = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_hybrid_lcpn_instance_C.csv')\n",
        "\n",
        "# Combine all results\n",
        "all_results_combined = pd.concat([results_a, results_b, results_c], ignore_index=True)\n",
        "\n",
        "# Filter only Fold results\n",
        "fold_results = all_results_combined[all_results_combined['Type'] == 'Fold'].copy()\n",
        "\n",
        "print(f\"Total fold evaluations: {len(fold_results)}\")\n",
        "print(f\"Unique hybrid combinations: {fold_results['Model'].nunique()}\")\n",
        "\n",
        "# Calculate mean and std for each hybrid combination across all folds\n",
        "hybrid_summary = fold_results.groupby(['Model', 'Root', 'Internal', 'External']).agg({\n",
        "    'Macro_Precision': ['mean', 'std'],\n",
        "    'Macro_Recall': ['mean', 'std'],\n",
        "    'Macro_F1': ['mean', 'std'],\n",
        "    'Weighted_Precision': ['mean', 'std'],\n",
        "    'Weighted_Recall': ['mean', 'std'],\n",
        "    'Weighted_F1': ['mean', 'std'],\n",
        "    'Hierarchical_Precision': ['mean', 'std'],\n",
        "    'Hierarchical_Recall': ['mean', 'std'],\n",
        "    'Hierarchical_F1': ['mean', 'std']\n",
        "}).round(4)\n",
        "\n",
        "# Flatten column names\n",
        "hybrid_summary.columns = ['_'.join(col).strip() for col in hybrid_summary.columns.values]\n",
        "hybrid_summary = hybrid_summary.reset_index()\n",
        "\n",
        "print(f\"\\nCombined summary for {len(hybrid_summary)} hybrid combinations\")\n",
        "\n",
        "final_combined_results = []\n",
        "\n",
        "# Add Fold results\n",
        "final_combined_results.extend(fold_results.to_dict('records'))\n",
        "\n",
        "for _, row in hybrid_summary.iterrows():\n",
        "    final_combined_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'Root': row['Root'],\n",
        "        'Internal': row['Internal'],\n",
        "        'External': row['External'],\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': row['Macro_Precision_mean'],\n",
        "        'Macro_Recall': row['Macro_Recall_mean'],\n",
        "        'Macro_F1': row['Macro_F1_mean'],\n",
        "        'Weighted_Precision': row['Weighted_Precision_mean'],\n",
        "        'Weighted_Recall': row['Weighted_Recall_mean'],\n",
        "        'Weighted_F1': row['Weighted_F1_mean'],\n",
        "        'Hierarchical_Precision': row['Hierarchical_Precision_mean'],\n",
        "        'Hierarchical_Recall': row['Hierarchical_Recall_mean'],\n",
        "        'Hierarchical_F1': row['Hierarchical_F1_mean'],\n",
        "        'Type': 'Mean',\n",
        "        'Time': '',\n",
        "        'Instance': 'Combined'\n",
        "    })\n",
        "\n",
        "    final_combined_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'Root': row['Root'],\n",
        "        'Internal': row['Internal'],\n",
        "        'External': row['External'],\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': row['Macro_Precision_std'],\n",
        "        'Macro_Recall': row['Macro_Recall_std'],\n",
        "        'Macro_F1': row['Macro_F1_std'],\n",
        "        'Weighted_Precision': row['Weighted_Precision_std'],\n",
        "        'Weighted_Recall': row['Weighted_Recall_std'],\n",
        "        'Weighted_F1': row['Weighted_F1_std'],\n",
        "        'Hierarchical_Precision': row['Hierarchical_Precision_std'],\n",
        "        'Hierarchical_Recall': row['Hierarchical_Recall_std'],\n",
        "        'Hierarchical_F1': row['Hierarchical_F1_std'],\n",
        "        'Type': 'Std',\n",
        "        'Time': '',\n",
        "        'Instance': 'Combined'\n",
        "    })\n",
        "\n",
        "final_combined_df = pd.DataFrame(final_combined_results)\n",
        "final_combined_df.to_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_hybrid_lcpn_combined_final.csv', index=False)\n",
        "\n",
        "print(f\"Final combined results saved with {len(final_combined_df)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78XI_pnvDzJN",
        "outputId": "54b3244c-3aad-4273-b5ae-b7c9ee413b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total fold evaluations: 320\n",
            "Unique hybrid combinations: 64\n",
            "\n",
            "Combined summary for 64 hybrid combinations\n",
            "Final combined results saved with 448 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CQXgeBJc22M",
        "outputId": "fc718405-4310-4588-ef35-eeaa39f73f76"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training Hybrid-FastText-GRU-GRU-SVM-rbf\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "Training ROOT level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level (SVM-rbf)...\n",
            "Making hierarchical predictions...\n",
            "Fold 1 done. MacroF1=0.6990, WeightedF1=0.7468, HierF1=0.8828\n",
            "\n",
            "--- Fold 2 ---\n",
            "Training ROOT level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level (SVM-rbf)...\n",
            "Making hierarchical predictions...\n",
            "Fold 2 done. MacroF1=0.6828, WeightedF1=0.7315, HierF1=0.8734\n",
            "\n",
            "--- Fold 3 ---\n",
            "Training ROOT level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EXTERNAL level (SVM-rbf)...\n",
            "Making hierarchical predictions...\n",
            "Fold 3 done. MacroF1=0.7049, WeightedF1=0.7415, HierF1=0.8806\n",
            "\n",
            "--- Fold 4 ---\n",
            "Training ROOT level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training INTERNAL level (GRU)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training EXTERNAL level (SVM-rbf)...\n",
            "Making hierarchical predictions...\n",
            "Fold 4 done. MacroF1=0.6920, WeightedF1=0.7413, HierF1=0.8793\n",
            "\n",
            "--- Fold 5 ---\n",
            "Training ROOT level (GRU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training INTERNAL level (GRU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training EXTERNAL level (SVM-rbf)...\n",
            "Making hierarchical predictions...\n",
            "Fold 5 done. MacroF1=0.6874, WeightedF1=0.7351, HierF1=0.8786\n",
            "\n",
            "Hybrid-FastText-GRU-GRU-SVM-rbf Final Results:\n",
            "Macro F1: 0.6932 ± 0.0079\n",
            "Weighted F1: 0.7392 ± 0.0053\n",
            "Hierarchical F1: 0.8790 ± 0.0031\n",
            "Total Training Time: 30197.69 seconds\n"
          ]
        }
      ],
      "source": [
        "# Hybrid LCPN - Best Combination: GRU + GRU + SVM-rbf\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the best hybrid combination\n",
        "hybrid_combinations = {\n",
        "    'Hybrid-FastText-GRU-GRU-SVM-rbf': {\n",
        "        'root': 'GRU',\n",
        "        'internal': 'GRU',\n",
        "        'external': 'SVM-rbf'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN HIERARCHICAL - BEST COMBINATION\n",
        "# =============================================================================\n",
        "for hybrid_name, algo_combination in hybrid_combinations.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {hybrid_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Start timing for this model\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"\\n--- Fold {fold_idx} ---\")\n",
        "\n",
        "        # Get both FastText embeddings and sequences for this fold\n",
        "        X_tr_emb = X_train_fasttext[train_idx]\n",
        "        X_val_emb = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_emb_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_emb, y_tr)\n",
        "        X_tr_seq_hybrid, _ = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Get corresponding super labels for the hybrid-sampled data\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier (GRU)\n",
        "        # =====================================================================\n",
        "        print(\"Training ROOT level (GRU)...\")\n",
        "        y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "        y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "        root_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(len(super_labels), activation='softmax')\n",
        "        ])\n",
        "\n",
        "        root_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        root_model.fit(\n",
        "            X_tr_seq_hybrid, y_super_tr_hybrid_cat,\n",
        "            validation_data=(X_val_seq, y_super_val_cat),\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "        root_model.save(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{hybrid_name}_root_fold{fold_idx}.h5')\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier (GRU)\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            print(\"Training INTERNAL level (GRU)...\")\n",
        "            X_tr_internal_seq = X_tr_seq_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            # Apply resampling to internal data\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_seq_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal_seq, y_tr_internal)\n",
        "\n",
        "            # Re-encode internal labels to be consecutive starting from 0\n",
        "            internal_label_encoder = LabelEncoder()\n",
        "            y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "\n",
        "            # Convert to categorical for GRU\n",
        "            num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "            y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "            # Get validation data for internal level\n",
        "            internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "            if internal_val_mask.sum() > 0:\n",
        "                X_val_internal_seq = X_val_seq[internal_val_mask]\n",
        "                y_val_internal = y_val[internal_val_mask]\n",
        "                y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "                y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "                validation_data = (X_val_internal_seq, y_val_internal_cat)\n",
        "                monitor_metric = 'val_loss'\n",
        "            else:\n",
        "                validation_data = None\n",
        "                monitor_metric = 'loss'\n",
        "\n",
        "            internal_model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(num_internal_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            internal_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            internal_model.fit(\n",
        "                X_tr_internal_seq_hybrid, y_tr_internal_cat,\n",
        "                validation_data=validation_data,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "            internal_model.save(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{hybrid_name}_internal_fold{fold_idx}.h5')\n",
        "            joblib.dump(internal_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{hybrid_name}_internal_label_encoder_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier (SVM-rbf)\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            print(\"Training EXTERNAL level (SVM-rbf)...\")\n",
        "            X_tr_external_emb = X_tr_emb_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            # Apply resampling to external data\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_emb_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external_emb, y_tr_external)\n",
        "\n",
        "            external_label_encoder = None\n",
        "            y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "            external_model = SVC(kernel='rbf', random_state=42)\n",
        "            external_model.fit(X_tr_external_emb_hybrid, y_tr_external_hybrid_encoded)\n",
        "            joblib.dump(external_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{hybrid_name}_external_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        print(\"Making hierarchical predictions...\")\n",
        "        # Get root predictions\n",
        "        y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "        y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                # Internal uses GRU (sequence data)\n",
        "                single_sample = X_val_seq[i:i+1]\n",
        "                pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "                pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                # External uses SVM-rbf (embedding data)\n",
        "                single_sample = X_val_emb[i].reshape(1, -1)\n",
        "                y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': hybrid_name,\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': hybrid_name,\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': hybrid_name,\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{hybrid_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMQ8amwanOba",
        "outputId": "e55187c8-f12e-4885-9c58-1d69f6b5ed11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training SuperEnsemble-Flat0.8-Hier0.2\n",
            "Flat Weight: 0.80, Hierarchical Weight: 0.20\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "Training Best Flat Model (Weighted Soft Voting)...\n",
            "Training Best Hierarchical Model...\n",
            "  Training ROOT level...\n",
            "  Training INTERNAL level...\n",
            "  Training EXTERNAL level...\n",
            "Getting Hierarchical Probabilities...\n",
            "Combining Flat and Hierarchical Probabilities...\n",
            "Fold 1 done. MacroF1=0.7499, WeightedF1=0.7778, HierF1=0.8988\n",
            "\n",
            "--- Fold 2 ---\n",
            "Training Best Flat Model (Weighted Soft Voting)...\n",
            "Training Best Hierarchical Model...\n",
            "  Training ROOT level...\n",
            "  Training INTERNAL level...\n",
            "  Training EXTERNAL level...\n",
            "Getting Hierarchical Probabilities...\n",
            "Combining Flat and Hierarchical Probabilities...\n",
            "Fold 2 done. MacroF1=0.7443, WeightedF1=0.7713, HierF1=0.8961\n",
            "\n",
            "--- Fold 3 ---\n",
            "Training Best Flat Model (Weighted Soft Voting)...\n",
            "Training Best Hierarchical Model...\n",
            "  Training ROOT level...\n",
            "  Training INTERNAL level...\n",
            "  Training EXTERNAL level...\n",
            "Getting Hierarchical Probabilities...\n",
            "Combining Flat and Hierarchical Probabilities...\n",
            "Fold 3 done. MacroF1=0.7456, WeightedF1=0.7622, HierF1=0.8922\n",
            "\n",
            "--- Fold 4 ---\n",
            "Training Best Flat Model (Weighted Soft Voting)...\n",
            "Training Best Hierarchical Model...\n",
            "  Training ROOT level...\n",
            "  Training INTERNAL level...\n",
            "  Training EXTERNAL level...\n",
            "Getting Hierarchical Probabilities...\n",
            "Combining Flat and Hierarchical Probabilities...\n",
            "Fold 4 done. MacroF1=0.7333, WeightedF1=0.7673, HierF1=0.8929\n",
            "\n",
            "--- Fold 5 ---\n",
            "Training Best Flat Model (Weighted Soft Voting)...\n",
            "Training Best Hierarchical Model...\n",
            "  Training ROOT level...\n",
            "  Training INTERNAL level...\n",
            "  Training EXTERNAL level...\n",
            "Getting Hierarchical Probabilities...\n",
            "Combining Flat and Hierarchical Probabilities...\n",
            "Fold 5 done. MacroF1=0.7480, WeightedF1=0.7731, HierF1=0.8958\n",
            "\n",
            "SuperEnsemble-Flat0.8-Hier0.2 Final Results:\n",
            "Macro F1: 0.7442 ± 0.0058\n",
            "Weighted F1: 0.7703 ± 0.0053\n",
            "Hierarchical F1: 0.8951 ± 0.0024\n",
            "Total Training Time: 6779.74 seconds\n",
            "Weights - Flat: 0.80, Hierarchical: 0.20\n"
          ]
        }
      ],
      "source": [
        "# SUPER ENSEMBLE: Best Flat + Best Hierarchical Model Combination\n",
        "# Load common data\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Define weight combinations to try (w_flat, w_hier)\n",
        "weight_combinations = [\n",
        "    #(0.5, 0.5),    # Equal weights\n",
        "    #(0.6, 0.4),    # More weight to flat\n",
        "    #(0.4, 0.6),    # More weight to hierarchical\n",
        "    #(0.7, 0.3),    # Strong preference for flat\n",
        "    #(0.3, 0.7),    # Strong preference for hierarchical\n",
        "    (0.8, 0.2),    # Very strong preference for flat\n",
        "    #(0.2, 0.8),    # Very strong preference for hierarchical\n",
        "    #(0.9, 0.1),    # Almost all weight to flat\n",
        "    #(0.1, 0.9),    # Almost all weight to hierarchical\n",
        "]\n",
        "\n",
        "# Define the best models\n",
        "best_flat_models = ['XGBoost', 'GRU', 'RandomForest']\n",
        "best_flat_weights = [0.7195, 0.7292, 0.6844]  # Based on your benchmarking scores\n",
        "best_hierarchical_combo = 'Hybrid-FastText-GRU-GRU-SVM-rbf'\n",
        "\n",
        "# Normalize flat weights to sum to 1\n",
        "normalized_flat_weights = np.array(best_flat_weights) / np.sum(best_flat_weights)\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# SUPER ENSEMBLE: FLAT + HIERARCHICAL COMBINATION\n",
        "# =============================================================================\n",
        "for w_flat, w_hier in weight_combinations:\n",
        "    ensemble_name = f'SuperEnsemble-Flat{w_flat}-Hier{w_hier}'\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Flat Weight: {w_flat:.2f}, Hierarchical Weight: {w_hier:.2f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"\\n--- Fold {fold_idx} ---\")\n",
        "\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_fasttext[train_idx]\n",
        "        X_val_ft = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        # === GOLD STANDARD: SINGLE RESAMPLE PIPE (NO LEAKAGE) ===\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Apply SAME resampling to FastText\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        # Apply SAME resampling to sequences (same labels)\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) TRAIN BEST FLAT MODEL (Weighted Soft Voting Ensemble)\n",
        "        # =====================================================================\n",
        "        print(\"Training Best Flat Model (Weighted Soft Voting)...\")\n",
        "        flat_probabilities = []\n",
        "\n",
        "        # Train each model in the flat ensemble and get probabilities\n",
        "        for i, model_name in enumerate(best_flat_models):\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model - get probabilities\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                # Get probability predictions from GRU\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                flat_probabilities.append(y_proba_gru)\n",
        "\n",
        "            else:\n",
        "                # ML Models - get probabilities\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "                y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                flat_probabilities.append(y_proba_ml)\n",
        "\n",
        "        # Weighted Soft Voting: weighted average of probabilities\n",
        "        weighted_flat_proba = np.zeros_like(flat_probabilities[0])\n",
        "        for i, proba in enumerate(flat_probabilities):\n",
        "            weighted_flat_proba += normalized_flat_weights[i] * proba\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) TRAIN BEST HIERARCHICAL MODEL (GRU + GRU + SVM-rbf)\n",
        "        # =====================================================================\n",
        "        print(\"Training Best Hierarchical Model...\")\n",
        "\n",
        "        # Get corresponding super labels for the hybrid-sampled data\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # Train ROOT level (GRU)\n",
        "        print(\"  Training ROOT level...\")\n",
        "        y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "        y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "        root_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(len(super_labels), activation='softmax')\n",
        "        ])\n",
        "\n",
        "        root_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        root_model.fit(\n",
        "            X_tr_seq_hybrid, y_super_tr_hybrid_cat,\n",
        "            validation_data=(X_val_seq, y_super_val_cat),\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Train INTERNAL level (GRU)\n",
        "        internal_model = None\n",
        "        internal_label_encoder = None\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            print(\"  Training INTERNAL level...\")\n",
        "            X_tr_internal_seq = X_tr_seq_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_seq_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal_seq, y_tr_internal)\n",
        "\n",
        "            internal_label_encoder = LabelEncoder()\n",
        "            y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "            num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "            y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "            internal_model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(num_internal_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            internal_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "            if internal_val_mask.sum() > 0:\n",
        "                X_val_internal_seq = X_val_seq[internal_val_mask]\n",
        "                y_val_internal = y_val[internal_val_mask]\n",
        "                y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "                y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "                validation_data = (X_val_internal_seq, y_val_internal_cat)\n",
        "            else:\n",
        "                validation_data = None\n",
        "\n",
        "            internal_model.fit(\n",
        "                X_tr_internal_seq_hybrid, y_tr_internal_cat,\n",
        "                validation_data=validation_data,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        # Train EXTERNAL level (SVM-rbf)\n",
        "        external_model = None\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            print(\"  Training EXTERNAL level...\")\n",
        "            X_tr_external_emb = X_tr_ft_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_emb_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external_emb, y_tr_external)\n",
        "\n",
        "            external_model = SVC(kernel='rbf', random_state=42, probability=True)  # Enable probability\n",
        "            external_model.fit(X_tr_external_emb_hybrid, y_tr_external_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) GET HIERARCHICAL PROBABILITIES (Probabilistic LCPN)\n",
        "        # =====================================================================\n",
        "        print(\"Getting Hierarchical Probabilities...\")\n",
        "\n",
        "        # Get root probabilities\n",
        "        y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)  # Shape: (n_samples, n_super_classes)\n",
        "\n",
        "        # Initialize hierarchical probabilities matrix\n",
        "        hierarchical_proba = np.zeros((len(y_val), num_classes))\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Build hierarchical probabilities\n",
        "        for i in range(len(y_val)):\n",
        "            # Get super class probabilities for this sample\n",
        "            super_proba = y_super_val_pred_proba[i]  # Shape: (n_super_classes,)\n",
        "\n",
        "            # Internal branch\n",
        "            internal_prob = super_proba[list(super_labels).index('Internal')]\n",
        "            if internal_model is not None and internal_label_encoder is not None:\n",
        "                single_sample = X_val_seq[i:i+1]\n",
        "                internal_leaf_proba = internal_model.predict(single_sample, verbose=0)[0]  # Shape: (n_internal_classes,)\n",
        "\n",
        "                # Map internal leaf probabilities to global leaf indices\n",
        "                for leaf_idx, prob in enumerate(internal_leaf_proba):\n",
        "                    global_leaf_idx = internal_label_encoder.inverse_transform([leaf_idx])[0]\n",
        "                    hierarchical_proba[i, global_leaf_idx] += internal_prob * prob\n",
        "\n",
        "            # External branch\n",
        "            external_prob = super_proba[list(super_labels).index('External')]\n",
        "            if external_model is not None:\n",
        "                single_sample = X_val_ft[i].reshape(1, -1)\n",
        "                external_leaf_proba = external_model.predict_proba(single_sample)[0]  # Shape: (n_external_classes,)\n",
        "\n",
        "                # For SVM, we need to handle class mapping\n",
        "                external_classes = external_model.classes_\n",
        "                for class_idx, prob in enumerate(external_leaf_proba):\n",
        "                    global_leaf_idx = external_classes[class_idx]\n",
        "                    hierarchical_proba[i, global_leaf_idx] += external_prob * prob\n",
        "\n",
        "            # Normal branch\n",
        "            normal_prob = super_proba[list(super_labels).index('Normal')]\n",
        "            if normal_leaf_idx is not None:\n",
        "                hierarchical_proba[i, normal_leaf_idx] += normal_prob\n",
        "\n",
        "        # Normalize hierarchical probabilities to sum to 1 for each sample\n",
        "        hierarchical_proba = hierarchical_proba / hierarchical_proba.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) COMBINE FLAT AND HIERARCHICAL PROBABILITIES\n",
        "        # =====================================================================\n",
        "        print(\"Combining Flat and Hierarchical Probabilities...\")\n",
        "\n",
        "        # Final Score = (w_flat × P_flat) + (w_hier × P_hier)\n",
        "        final_probabilities = (w_flat * weighted_flat_proba) + (w_hier * hierarchical_proba)\n",
        "\n",
        "        # Get final predictions (class with highest combined probability)\n",
        "        y_pred = np.argmax(final_probabilities, axis=1)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': ensemble_name,\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': '',\n",
        "            'Flat_Weight': w_flat,\n",
        "            'Hier_Weight': w_hier\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': ensemble_name,\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time,\n",
        "        'Flat_Weight': w_flat,\n",
        "        'Hier_Weight': w_hier\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': ensemble_name,\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': '',\n",
        "        'Flat_Weight': w_flat,\n",
        "        'Hier_Weight': w_hier\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Weights - Flat: {w_flat:.2f}, Hierarchical: {w_hier:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoimxhJnHlQk"
      },
      "source": [
        "# Word2Vec Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74VuT_y2HnDU",
        "outputId": "156d316b-0cf2-4816-ddab-d6b999f6671f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LightGBM\n",
            "============================================================\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091758 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 1 done. MacroF1=0.6587, WeightedF1=0.7116, HierF1=0.8631\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 2 done. MacroF1=0.6535, WeightedF1=0.7086, HierF1=0.8615\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091728 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73220, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 3 done. MacroF1=0.6388, WeightedF1=0.6945, HierF1=0.8551\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090716 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 4 done. MacroF1=0.6587, WeightedF1=0.7114, HierF1=0.8613\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089568 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 5 done. MacroF1=0.6628, WeightedF1=0.7127, HierF1=0.8630\n",
            "\n",
            "LightGBM Final Results:\n",
            "Macro F1: 0.6545 ± 0.0084\n",
            "Weighted F1: 0.7078 ± 0.0068\n",
            "Hierarchical F1: 0.8608 ± 0.0029\n",
            "Total Training Time: 232.44 seconds\n",
            "\n",
            "============================================================\n",
            "Training XGBoost\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6695, WeightedF1=0.7210, HierF1=0.8707\n",
            "Fold 2 done. MacroF1=0.6666, WeightedF1=0.7193, HierF1=0.8695\n",
            "Fold 3 done. MacroF1=0.6598, WeightedF1=0.7093, HierF1=0.8647\n",
            "Fold 4 done. MacroF1=0.6683, WeightedF1=0.7183, HierF1=0.8679\n",
            "Fold 5 done. MacroF1=0.6687, WeightedF1=0.7253, HierF1=0.8711\n",
            "\n",
            "XGBoost Final Results:\n",
            "Macro F1: 0.6666 ± 0.0035\n",
            "Weighted F1: 0.7187 ± 0.0052\n",
            "Hierarchical F1: 0.8688 ± 0.0023\n",
            "Total Training Time: 302.41 seconds\n",
            "\n",
            "============================================================\n",
            "Training LogisticRegression\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.5462, WeightedF1=0.6198, HierF1=0.7990\n",
            "Fold 2 done. MacroF1=0.5381, WeightedF1=0.6149, HierF1=0.7957\n",
            "Fold 3 done. MacroF1=0.5298, WeightedF1=0.5967, HierF1=0.7877\n",
            "Fold 4 done. MacroF1=0.5386, WeightedF1=0.6091, HierF1=0.7910\n",
            "Fold 5 done. MacroF1=0.5376, WeightedF1=0.6090, HierF1=0.7900\n",
            "\n",
            "LogisticRegression Final Results:\n",
            "Macro F1: 0.5381 ± 0.0052\n",
            "Weighted F1: 0.6099 ± 0.0077\n",
            "Hierarchical F1: 0.7927 ± 0.0041\n",
            "Total Training Time: 31.43 seconds\n",
            "\n",
            "============================================================\n",
            "Training SVM-linear\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.5624, WeightedF1=0.6408, HierF1=0.8121\n",
            "Fold 2 done. MacroF1=0.5507, WeightedF1=0.6295, HierF1=0.8063\n",
            "Fold 3 done. MacroF1=0.5448, WeightedF1=0.6147, HierF1=0.7981\n",
            "Fold 4 done. MacroF1=0.5554, WeightedF1=0.6273, HierF1=0.8039\n",
            "Fold 5 done. MacroF1=0.5532, WeightedF1=0.6281, HierF1=0.8034\n",
            "\n",
            "SVM-linear Final Results:\n",
            "Macro F1: 0.5533 ± 0.0058\n",
            "Weighted F1: 0.6281 ± 0.0083\n",
            "Hierarchical F1: 0.8048 ± 0.0045\n",
            "Total Training Time: 2077.71 seconds\n",
            "\n",
            "============================================================\n",
            "Training SVM-rbf\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6165, WeightedF1=0.6944, HierF1=0.8432\n",
            "Fold 2 done. MacroF1=0.6068, WeightedF1=0.6861, HierF1=0.8422\n",
            "Fold 3 done. MacroF1=0.5972, WeightedF1=0.6726, HierF1=0.8329\n",
            "Fold 4 done. MacroF1=0.6074, WeightedF1=0.6846, HierF1=0.8372\n",
            "Fold 5 done. MacroF1=0.6124, WeightedF1=0.6850, HierF1=0.8394\n",
            "\n",
            "SVM-rbf Final Results:\n",
            "Macro F1: 0.6080 ± 0.0065\n",
            "Weighted F1: 0.6845 ± 0.0070\n",
            "Hierarchical F1: 0.8390 ± 0.0037\n",
            "Total Training Time: 2012.14 seconds\n",
            "\n",
            "============================================================\n",
            "Training RandomForest\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6056, WeightedF1=0.6844, HierF1=0.8543\n",
            "Fold 2 done. MacroF1=0.6082, WeightedF1=0.6883, HierF1=0.8555\n",
            "Fold 3 done. MacroF1=0.6002, WeightedF1=0.6766, HierF1=0.8496\n",
            "Fold 4 done. MacroF1=0.6036, WeightedF1=0.6821, HierF1=0.8523\n",
            "Fold 5 done. MacroF1=0.6145, WeightedF1=0.6904, HierF1=0.8564\n",
            "\n",
            "RandomForest Final Results:\n",
            "Macro F1: 0.6064 ± 0.0048\n",
            "Weighted F1: 0.6843 ± 0.0048\n",
            "Hierarchical F1: 0.8536 ± 0.0024\n",
            "Total Training Time: 577.98 seconds\n"
          ]
        }
      ],
      "source": [
        "# FLAT W2V ML\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "ml_models = {\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# MACHINE LEARNING MODELS LOOP\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        fold_idx += 1\n",
        "        X_tr = X_train_w2v[train_idx]\n",
        "        X_val = X_train_w2v[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        model = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42)),\n",
        "            ('clf', clone(model_class))\n",
        "        ])\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-W2V-{model_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{model_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{model_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{model_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "TGI9JfqJZjxo",
        "outputId": "aa8ece60-ed8e-470e-dd03-45eb3e4ed92c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training GRU with TF-IDF - FAIR COMPARISON\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-130543075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Train with same parameters as Word2Vec GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mX_tr_hybrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# FLAT GRU\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# GRU IMPLEMENTATION\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training GRU\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Start timing for GRU\n",
        "model_start_time = time.time()\n",
        "\n",
        "# Metrics containers for GRU\n",
        "p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "hF1s=[]; hPs=[]; hRs = []\n",
        "all_y_true = []; all_y_pred = []\n",
        "\n",
        "fold_idx = 0\n",
        "for train_idx, val_idx in skf.split(X_train_sequences, y_train):\n",
        "    fold_idx += 1\n",
        "\n",
        "    # Get data splits\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "\n",
        "    # SINGLE RESAMPLE PIPE\n",
        "    resample_pipe = ImbPipeline([\n",
        "        ('ros', RandomOverSampler(random_state=42)),\n",
        "        ('rus', RandomUnderSampler(random_state=42))\n",
        "    ])\n",
        "    X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "    y_tr_cat = tf.keras.utils.to_categorical(y_tr_hybrid, num_classes)\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=max_sequence_length,\n",
        "            trainable=False\n",
        "        ),\n",
        "        GRU(64),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr_hybrid, y_tr_cat,\n",
        "        validation_data=(X_val_seq, y_val_cat),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_val_seq, verbose=0), axis=1)\n",
        "\n",
        "    p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "    p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "    hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "    # Store fold results\n",
        "    p_macros.append(p_macro)\n",
        "    r_macros.append(r_macro)\n",
        "    f1_macros.append(f1_macro)\n",
        "    p_weights.append(p_weighted)\n",
        "    r_weights.append(r_weighted)\n",
        "    f1_weights.append(f1_weighted)\n",
        "    hF1s.append(hF1)\n",
        "    hPs.append(hP)\n",
        "    hRs.append(hR)\n",
        "\n",
        "    all_y_true.extend(list(y_val))\n",
        "    all_y_pred.extend(list(y_pred))\n",
        "\n",
        "    # Save individual fold result\n",
        "    all_results.append({\n",
        "            'Model': f'Flat-W2V-GRU-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "    print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "model_total_time = time.time() - model_start_time\n",
        "# Calculate and save final averages for this model\n",
        "all_results.append({\n",
        "        'Model': f'Flat-W2V-GRU-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "all_results.append({\n",
        "        'Model': f'Flat-W2V-GRU-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "print(f\"\\nGRU Final Results:\")\n",
        "print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feNoQAtl0Mp6",
        "outputId": "c9959b58-a9aa-4517-d5d5-a17a73a29a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training HardVoting-XGBoost-GRU-LightGBM\n",
            "============================================================\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086028 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 1 done. MacroF1=0.6966, WeightedF1=0.7381, HierF1=0.8789\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086896 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 100\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 2 done. MacroF1=0.6895, WeightedF1=0.7338, HierF1=0.8763\n"
          ]
        }
      ],
      "source": [
        "# HARD VOTING ENSEMBLES\n",
        "X_train_fasttext = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_fasttext_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/fasttext_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "# Define voting ensembles\n",
        "voting_ensembles = {\n",
        "    'HardVoting-XGBoost-GRU-LightGBM': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "    'HardVoting-XGBoost-GRU-RandomForest': ['XGBoost', 'GRU', 'RandomForest']\n",
        "}\n",
        "# =============================================================================\n",
        "# HARD VOTING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, model_names in voting_ensembles.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    model_start_time = time.time()\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_fasttext, y_train):\n",
        "        fold_idx += 1\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_fasttext[train_idx]\n",
        "        X_val_ft = X_train_fasttext[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "        # Store predictions from each model\n",
        "        fold_predictions = []\n",
        "        # Train each model in the ensemble\n",
        "        for model_name in model_names:\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "                y_pred_gru = np.argmax(gru_model.predict(X_val_seq, verbose=0), axis=1)\n",
        "                fold_predictions.append(y_pred_gru)\n",
        "            else:\n",
        "                # ML Models\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "                y_pred_ml = model.predict(X_val_ft)\n",
        "                fold_predictions.append(y_pred_ml)\n",
        "        # Hard Voting\n",
        "        fold_predictions = np.array(fold_predictions)\n",
        "        y_pred_vote = []\n",
        "        for i in range(len(y_val)):\n",
        "            votes = fold_predictions[:, i]\n",
        "            unique, counts = np.unique(votes, return_counts=True)\n",
        "            max_count = np.max(counts)\n",
        "            if np.sum(counts == max_count) > 1:\n",
        "                y_pred_vote.append(votes[0]) # Tie → use first model (XGBoost)\n",
        "            else:\n",
        "                y_pred_vote.append(unique[np.argmax(counts)])\n",
        "        y_pred = np.array(y_pred_vote)\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "    model_total_time = time.time() - model_start_time\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-FastText-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhaKN86UUJls"
      },
      "outputs": [],
      "source": [
        "# WEIGHTED SOFT VOTING ENSEMBLES\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define weighted soft voting ensembles with performance-based weights\n",
        "weighted_ensembles = {\n",
        "    'WeightedSoft-XGBoost-GRU-LightGBM': {\n",
        "        'models': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "        'weights': [0.7195, 0.7411, 0.7103]\n",
        "    },\n",
        "    'WeightedSoft-XGBoost-GRU-RandomForest': {\n",
        "        'models': ['XGBoost', 'GRU', 'RandomForest'],\n",
        "        'weights': [0.7195, 0.7411, 0.6803]\n",
        "    }\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# WEIGHTED SOFT VOTING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in weighted_ensembles.items():\n",
        "    model_names = ensemble_config['models']\n",
        "    performance_weights = ensemble_config['weights']\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    normalized_weights = np.array(performance_weights) / np.sum(performance_weights)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Model Weights: {dict(zip(model_names, normalized_weights))}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_w2v[train_idx]\n",
        "        X_val_ft = X_train_w2v[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Store probability predictions from each model\n",
        "        fold_probabilities = []\n",
        "\n",
        "        # Train each model in the ensemble and get probabilities\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                # probability predictions from GRU\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                fold_probabilities.append(y_proba_gru)\n",
        "\n",
        "            else:\n",
        "                # ML Models - get probabilities\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "                y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                fold_probabilities.append(y_proba_ml)\n",
        "\n",
        "        # Weighted Soft Voting: weighted average of probabilities\n",
        "        weighted_probabilities = np.zeros_like(fold_probabilities[0])\n",
        "\n",
        "        for i, proba in enumerate(fold_probabilities):\n",
        "            # Apply normalized performance weight to each model's probabilities\n",
        "            weighted_probabilities += normalized_weights[i] * proba\n",
        "\n",
        "        # Get final predictions (class with highest weighted probability)\n",
        "        y_pred = np.argmax(weighted_probabilities, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Used Weights: {dict(zip(model_names, normalized_weights))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5tqgpbUkYc-"
      },
      "outputs": [],
      "source": [
        "# STACKING ENSEMBLES\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define stacking ensembles\n",
        "stacking_ensembles = {\n",
        "    'Stacking-XGB-GRU-RF-LR': {\n",
        "        'base_models': ['XGBoost', 'GRU', 'RandomForest'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    },\n",
        "    'Stacking-XGB-GRU-LGBM-LR': {\n",
        "        'base_models': ['XGBoost', 'GRU', 'LightGBM'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    },\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# STACKING ENSEMBLES LOOP\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in stacking_ensembles.items():\n",
        "    base_model_names = ensemble_config['base_models']\n",
        "    meta_model = ensemble_config['meta_model']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Generate Out-of-Fold Predictions\n",
        "    print(\"Stage 1: Generating Out-of-Fold predictions...\")\n",
        "\n",
        "    # Initialize OOF arrays for base model predictions\n",
        "    oof_predictions = {}\n",
        "    for model_name in base_model_names:\n",
        "        oof_predictions[model_name] = np.zeros((len(X_train_w2v), num_classes))\n",
        "\n",
        "    # Generate OOF predictions for each base model\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        # Get data splits\n",
        "        X_tr_ft = X_train_w2v[train_idx]\n",
        "        X_val_ft = X_train_w2v[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "         # SINGLE RESAMPLE PIPE (NO LEAKAGE)\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Train each base model and get OOF predictions\n",
        "        for model_name in base_model_names:\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                oof_predictions['GRU'][val_idx] = y_proba_gru\n",
        "\n",
        "            else:\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "                elif model_name == 'LightGBM':\n",
        "                    model = LGBMClassifier(random_state=42)\n",
        "                elif model_name == 'SVM-rbf':\n",
        "                    model = SVC(kernel='rbf', random_state=42, probability=True)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "\n",
        "                # Get probability predictions for validation set\n",
        "                if model_name == 'SVM-rbf':\n",
        "                    y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                else:\n",
        "                    y_proba_ml = model.predict_proba(X_val_ft)\n",
        "\n",
        "                oof_predictions[model_name][val_idx] = y_proba_ml\n",
        "\n",
        "    print(\"Stage 1 completed: OOF predictions generated for all base models\")\n",
        "\n",
        "    # Train and Evaluate Meta-Classifier\n",
        "    print(\"Stage 2: Training and evaluating meta-classifier...\")\n",
        "\n",
        "    # Create meta-features dataset\n",
        "    X_meta = np.hstack([oof_predictions[model_name] for model_name in base_model_names])\n",
        "\n",
        "    # Metrics containers for stacking ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    # Cross-validation on meta-features\n",
        "    for train_idx, val_idx in skf.split(X_meta, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get meta-features splits\n",
        "        X_tr_meta = X_meta[train_idx]\n",
        "        X_val_meta = X_meta[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # RESAMPLING FOR META-CLASSIFIER\n",
        "        resample_pipe_meta = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Apply resampling to meta-features\n",
        "        X_tr_meta_hybrid, y_tr_hybrid = resample_pipe_meta.fit_resample(X_tr_meta, y_tr)\n",
        "\n",
        "        # Train meta-classifier\n",
        "        meta_model_clone = clone(meta_model)\n",
        "        meta_model_clone.fit(X_tr_meta_hybrid, y_tr_hybrid)\n",
        "\n",
        "        # Predict with meta-classifier\n",
        "        y_pred = meta_model_clone.predict(X_val_meta)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-W2V-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3Lh346bs5HZ",
        "outputId": "22ad39f7-415f-4e45-eb25-09e12eec8353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---- Fold 1 ----\n",
            "\n",
            "Testing GRU...\n",
            "GRU - Root: 0.9129, Internal: 0.6901, External: 0.8473, Overall: 0.7466\n",
            "\n",
            "---- Fold 2 ----\n",
            "\n",
            "Testing GRU...\n",
            "GRU - Root: 0.9086, Internal: 0.6841, External: 0.8076, Overall: 0.7364\n",
            "\n",
            "---- Fold 3 ----\n",
            "\n",
            "Testing GRU...\n",
            "GRU - Root: 0.9024, Internal: 0.6957, External: 0.5829, Overall: 0.7242\n",
            "\n",
            "---- Fold 4 ----\n",
            "\n",
            "Testing GRU...\n",
            "GRU - Root: 0.9011, Internal: 0.6711, External: 0.1933, Overall: 0.6972\n",
            "\n",
            "---- Fold 5 ----\n",
            "\n",
            "Testing GRU...\n",
            "GRU - Root: 0.8824, Internal: 0.6804, External: 0.2247, Overall: 0.6809\n",
            "\n",
            "================================================================================\n",
            "FINAL PERFORMANCE RESULTS FOR ALL ALGORITHMS AT EACH LEVEL\n",
            "================================================================================\n",
            "\n",
            "Root LEVEL PERFORMANCE:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GRU                  | Macro F1: 0.8485±0.0133 | Weighted F1: 0.9015±0.0105 | Hierarchical F1: 0.9502±0.0055\n",
            "\n",
            "Internal LEVEL PERFORMANCE:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GRU                  | Macro F1: 0.3980±0.0060 | Weighted F1: 0.6843±0.0084 | Hierarchical F1: 0.8701±0.0096\n",
            "\n",
            "External LEVEL PERFORMANCE:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GRU                  | Macro F1: 0.1503±0.0669 | Weighted F1: 0.5311±0.2782 | Hierarchical F1: 0.7710±0.0639\n",
            "\n",
            "Overall LEVEL PERFORMANCE:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GRU                  | Macro F1: 0.6207±0.0645 | Weighted F1: 0.7170±0.0245 | Hierarchical F1: 0.8679±0.0121\n"
          ]
        }
      ],
      "source": [
        "# recalibrate with GRU included\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "super_ancestor_sets = {\n",
        "    'Internal': {'Internal', 'Root'},\n",
        "    'External': {'External', 'Root'},\n",
        "    'Normal': {'Normal', 'Root'}\n",
        "}\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get dimensions for GRU\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define all ML models including GRU\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'GRU': 'gru_model'  # Special marker for GRU\n",
        "}\n",
        "\n",
        "# Performance tracking for all algorithms and levels\n",
        "algorithm_performance = {}\n",
        "all_results = []\n",
        "\n",
        "# Initialize performance tracking for all algorithms\n",
        "for algo_name in list(ml_models.keys()):\n",
        "    algorithm_performance[algo_name] = {\n",
        "        'root': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'internal': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'external': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []},\n",
        "        'overall': {'macro_p': [], 'macro_r': [], 'macro_f1': [], 'weighted_p': [], 'weighted_r': [], 'weighted_f1': [], 'hF1': [], 'hP': [], 'hR': []}\n",
        "    }\n",
        "\n",
        "fold_no = 0\n",
        "for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "    fold_no += 1\n",
        "    print(f\"\\n---- Fold {fold_no} ----\")\n",
        "\n",
        "    # Split data\n",
        "    X_tr = X_train_w2v[train_idx]\n",
        "    X_val = X_train_w2v[val_idx]\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    # Test each algorithm\n",
        "    for algo_name in list(ml_models.keys()):\n",
        "        print(f\"\\nTesting {algo_name}...\")\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE ===\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        if algo_name == 'GRU':\n",
        "            # For GRU, use sequence data\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "        else:\n",
        "            # For ML models, use word2vec features\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Get corresponding super labels for the hybrid-sampled data\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        if algo_name == 'GRU':\n",
        "            # GRU Root Model\n",
        "            y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "            y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "            root_model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(len(super_labels), activation='softmax')\n",
        "            ])\n",
        "\n",
        "            root_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            root_model.fit(\n",
        "                X_tr_hybrid, y_super_tr_hybrid_cat,\n",
        "                validation_data=(X_val_seq, y_super_val_cat),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "        else:\n",
        "            # ML Root Model\n",
        "            root_model = clone(ml_models[algo_name])\n",
        "            root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            # Apply resampling to internal data\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            if algo_name == 'GRU':\n",
        "                # GRU Internal Model\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "                y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "                internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "                if internal_val_mask.sum() > 0:\n",
        "                    X_val_internal = X_val_seq[internal_val_mask]\n",
        "                    y_val_internal = y_val[internal_val_mask]\n",
        "                    y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "                    y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "                    validation_data = (X_val_internal, y_val_internal_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                internal_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_internal_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                internal_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                internal_model.fit(\n",
        "                    X_tr_internal_hybrid, y_tr_internal_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                if algo_name in ['XGBoost', 'LightGBM']:\n",
        "                    internal_label_encoder = LabelEncoder()\n",
        "                    y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                else:\n",
        "                    internal_label_encoder = None\n",
        "                    y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "                internal_model = clone(ml_models[algo_name])\n",
        "                internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            # Apply resampling to external data\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            if algo_name == 'GRU':\n",
        "                # GRU External Model\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                num_external_classes = len(np.unique(y_tr_external_hybrid_encoded))\n",
        "                y_tr_external_cat = tf.keras.utils.to_categorical(y_tr_external_hybrid_encoded, num_classes=num_external_classes)\n",
        "\n",
        "                external_val_mask = (y_super_val == list(super_labels).index('External'))\n",
        "                if external_val_mask.sum() > 0:\n",
        "                    X_val_external = X_val_seq[external_val_mask]\n",
        "                    y_val_external = y_val[external_val_mask]\n",
        "                    y_val_external_encoded = external_label_encoder.transform(y_val_external)\n",
        "                    y_val_external_cat = tf.keras.utils.to_categorical(y_val_external_encoded, num_classes=num_external_classes)\n",
        "                    validation_data = (X_val_external, y_val_external_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                external_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_external_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                external_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                external_model.fit(\n",
        "                    X_tr_external_hybrid, y_tr_external_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                if algo_name in ['XGBoost', 'LightGBM']:\n",
        "                    external_label_encoder = LabelEncoder()\n",
        "                    y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                else:\n",
        "                    external_label_encoder = None\n",
        "                    y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "                external_model = clone(ml_models[algo_name])\n",
        "                external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        if algo_name == 'GRU':\n",
        "            # GRU Prediction\n",
        "            y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "            y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "        else:\n",
        "            # ML Prediction\n",
        "            y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                if algo_name == 'GRU':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if algo_name in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                        pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                if algo_name == 'GRU':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if algo_name in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                        pred_encoded = external_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:  # Normal\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        fold_time = time.time() - fold_start_time\n",
        "\n",
        "        # ROOT LEVEL METRICS\n",
        "        root_macro_p = precision_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_macro_r = recall_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_macro_f1 = f1_score(y_super_val, y_super_val_pred, average='macro', zero_division=0)\n",
        "        root_weighted_p = precision_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_weighted_r = recall_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_weighted_f1 = f1_score(y_super_val, y_super_val_pred, average='weighted', zero_division=0)\n",
        "        root_hF1, root_hP, root_hR, _, _, _ = hierarchical_metrics_journal(y_super_val, y_super_val_pred, list(super_labels), super_ancestor_sets)\n",
        "\n",
        "        algorithm_performance[algo_name]['root']['macro_p'].append(root_macro_p)\n",
        "        algorithm_performance[algo_name]['root']['macro_r'].append(root_macro_r)\n",
        "        algorithm_performance[algo_name]['root']['macro_f1'].append(root_macro_f1)\n",
        "        algorithm_performance[algo_name]['root']['weighted_p'].append(root_weighted_p)\n",
        "        algorithm_performance[algo_name]['root']['weighted_r'].append(root_weighted_r)\n",
        "        algorithm_performance[algo_name]['root']['weighted_f1'].append(root_weighted_f1)\n",
        "        algorithm_performance[algo_name]['root']['hF1'].append(root_hF1)\n",
        "        algorithm_performance[algo_name]['root']['hP'].append(root_hP)\n",
        "        algorithm_performance[algo_name]['root']['hR'].append(root_hR)\n",
        "\n",
        "        # INTERNAL LEVEL METRICS\n",
        "        internal_true_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "        if internal_true_mask.sum() > 0:\n",
        "            idxs_internal = np.where(internal_true_mask)[0]\n",
        "            y_internal_true = y_val[idxs_internal]\n",
        "            y_internal_pred = y_val_final_pred[idxs_internal]\n",
        "\n",
        "            internal_macro_p = precision_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_macro_r = recall_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_macro_f1 = f1_score(y_internal_true, y_internal_pred, average='macro', zero_division=0)\n",
        "            internal_weighted_p = precision_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_weighted_r = recall_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_weighted_f1 = f1_score(y_internal_true, y_internal_pred, average='weighted', zero_division=0)\n",
        "            internal_hF1, internal_hP, internal_hR, _, _, _ = hierarchical_metrics_journal(y_internal_true, y_internal_pred, labels, ancestor_sets)\n",
        "        else:\n",
        "            internal_macro_p = internal_macro_r = internal_macro_f1 = 0.0\n",
        "            internal_weighted_p = internal_weighted_r = internal_weighted_f1 = 0.0\n",
        "            internal_hF1 = internal_hP = internal_hR = 0.0\n",
        "\n",
        "        algorithm_performance[algo_name]['internal']['macro_p'].append(internal_macro_p)\n",
        "        algorithm_performance[algo_name]['internal']['macro_r'].append(internal_macro_r)\n",
        "        algorithm_performance[algo_name]['internal']['macro_f1'].append(internal_macro_f1)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_p'].append(internal_weighted_p)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_r'].append(internal_weighted_r)\n",
        "        algorithm_performance[algo_name]['internal']['weighted_f1'].append(internal_weighted_f1)\n",
        "        algorithm_performance[algo_name]['internal']['hF1'].append(internal_hF1)\n",
        "        algorithm_performance[algo_name]['internal']['hP'].append(internal_hP)\n",
        "        algorithm_performance[algo_name]['internal']['hR'].append(internal_hR)\n",
        "\n",
        "        # EXTERNAL LEVEL METRICS\n",
        "        external_true_mask = (y_super_val == list(super_labels).index('External'))\n",
        "        if external_true_mask.sum() > 0:\n",
        "            idxs_external = np.where(external_true_mask)[0]\n",
        "            y_external_true = y_val[idxs_external]\n",
        "            y_external_pred = y_val_final_pred[idxs_external]\n",
        "\n",
        "            external_macro_p = precision_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_macro_r = recall_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_macro_f1 = f1_score(y_external_true, y_external_pred, average='macro', zero_division=0)\n",
        "            external_weighted_p = precision_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_weighted_r = recall_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_weighted_f1 = f1_score(y_external_true, y_external_pred, average='weighted', zero_division=0)\n",
        "            external_hF1, external_hP, external_hR, _, _, _ = hierarchical_metrics_journal(y_external_true, y_external_pred, labels, ancestor_sets)\n",
        "        else:\n",
        "            external_macro_p = external_macro_r = external_macro_f1 = 0.0\n",
        "            external_weighted_p = external_weighted_r = external_weighted_f1 = 0.0\n",
        "            external_hF1 = external_hP = external_hR = 0.0\n",
        "\n",
        "        algorithm_performance[algo_name]['external']['macro_p'].append(external_macro_p)\n",
        "        algorithm_performance[algo_name]['external']['macro_r'].append(external_macro_r)\n",
        "        algorithm_performance[algo_name]['external']['macro_f1'].append(external_macro_f1)\n",
        "        algorithm_performance[algo_name]['external']['weighted_p'].append(external_weighted_p)\n",
        "        algorithm_performance[algo_name]['external']['weighted_r'].append(external_weighted_r)\n",
        "        algorithm_performance[algo_name]['external']['weighted_f1'].append(external_weighted_f1)\n",
        "        algorithm_performance[algo_name]['external']['hF1'].append(external_hF1)\n",
        "        algorithm_performance[algo_name]['external']['hP'].append(external_hP)\n",
        "        algorithm_performance[algo_name]['external']['hR'].append(external_hR)\n",
        "\n",
        "        # OVERALL METRICS\n",
        "        overall_macro_p = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_macro_r = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_macro_f1 = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        overall_weighted_p = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_weighted_r = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_weighted_f1 = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        overall_hF1, overall_hP, overall_hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        algorithm_performance[algo_name]['overall']['macro_p'].append(overall_macro_p)\n",
        "        algorithm_performance[algo_name]['overall']['macro_r'].append(overall_macro_r)\n",
        "        algorithm_performance[algo_name]['overall']['macro_f1'].append(overall_macro_f1)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_p'].append(overall_weighted_p)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_r'].append(overall_weighted_r)\n",
        "        algorithm_performance[algo_name]['overall']['weighted_f1'].append(overall_weighted_f1)\n",
        "        algorithm_performance[algo_name]['overall']['hF1'].append(overall_hF1)\n",
        "        algorithm_performance[algo_name]['overall']['hP'].append(overall_hP)\n",
        "        algorithm_performance[algo_name]['overall']['hR'].append(overall_hR)\n",
        "\n",
        "        # Store fold results for Excel output\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-W2V-{algo_name}',\n",
        "            'Level': 'Overall',\n",
        "            'Fold': f'Fold_{fold_no}',\n",
        "            'Macro_Precision': overall_macro_p,\n",
        "            'Macro_Recall': overall_macro_r,\n",
        "            'Macro_F1': overall_macro_f1,\n",
        "            'Weighted_Precision': overall_weighted_p,\n",
        "            'Weighted_Recall': overall_weighted_r,\n",
        "            'Weighted_F1': overall_weighted_f1,\n",
        "            'Hierarchical_Precision': overall_hP,\n",
        "            'Hierarchical_Recall': overall_hR,\n",
        "            'Hierarchical_F1': overall_hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': fold_time\n",
        "        })\n",
        "\n",
        "        print(f\"{algo_name} - Root: {root_weighted_f1:.4f}, Internal: {internal_weighted_f1:.4f}, External: {external_weighted_f1:.4f}, Overall: {overall_weighted_f1:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CALCULATE FINAL MEANS AND STDS\n",
        "# =============================================================================\n",
        "for algo_name in algorithm_performance.keys():\n",
        "    for level in ['root', 'internal', 'external', 'overall']:\n",
        "        metrics = algorithm_performance[algo_name][level]\n",
        "\n",
        "        # Add Mean rows\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-{algo_name}',\n",
        "            'Level': level.capitalize(),\n",
        "            'Fold': 'Mean',\n",
        "            'Macro_Precision': np.mean(metrics['macro_p']),\n",
        "            'Macro_Recall': np.mean(metrics['macro_r']),\n",
        "            'Macro_F1': np.mean(metrics['macro_f1']),\n",
        "            'Weighted_Precision': np.mean(metrics['weighted_p']),\n",
        "            'Weighted_Recall': np.mean(metrics['weighted_r']),\n",
        "            'Weighted_F1': np.mean(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision': np.mean(metrics['hP']),\n",
        "            'Hierarchical_Recall': np.mean(metrics['hR']),\n",
        "            'Hierarchical_F1': np.mean(metrics['hF1']),\n",
        "            'Type': 'Mean',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        # Add Std rows\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-{algo_name}',\n",
        "            'Level': level.capitalize(),\n",
        "            'Fold': 'Std',\n",
        "            'Macro_Precision': np.std(metrics['macro_p']),\n",
        "            'Macro_Recall': np.std(metrics['macro_r']),\n",
        "            'Macro_F1': np.std(metrics['macro_f1']),\n",
        "            'Weighted_Precision': np.std(metrics['weighted_p']),\n",
        "            'Weighted_Recall': np.std(metrics['weighted_r']),\n",
        "            'Weighted_F1': np.std(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision': np.std(metrics['hP']),\n",
        "            'Hierarchical_Recall': np.std(metrics['hR']),\n",
        "            'Hierarchical_F1': np.std(metrics['hF1']),\n",
        "            'Type': 'Std',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "# =============================================================================\n",
        "# DISPLAY FINAL RESULTS\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL PERFORMANCE RESULTS FOR ALL ALGORITHMS AT EACH LEVEL\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "results_data = []\n",
        "for algo_name in algorithm_performance.keys():\n",
        "    for level in ['root', 'internal', 'external', 'overall']:\n",
        "        metrics = algorithm_performance[algo_name][level]\n",
        "\n",
        "        result_row = {\n",
        "            'Algorithm': algo_name,\n",
        "            'Level': level.capitalize(),\n",
        "            'Macro_Precision_Mean': np.mean(metrics['macro_p']),\n",
        "            'Macro_Precision_Std': np.std(metrics['macro_p']),\n",
        "            'Macro_Recall_Mean': np.mean(metrics['macro_r']),\n",
        "            'Macro_Recall_Std': np.std(metrics['macro_r']),\n",
        "            'Macro_F1_Mean': np.mean(metrics['macro_f1']),\n",
        "            'Macro_F1_Std': np.std(metrics['macro_f1']),\n",
        "            'Weighted_Precision_Mean': np.mean(metrics['weighted_p']),\n",
        "            'Weighted_Precision_Std': np.std(metrics['weighted_p']),\n",
        "            'Weighted_Recall_Mean': np.mean(metrics['weighted_r']),\n",
        "            'Weighted_Recall_Std': np.std(metrics['weighted_r']),\n",
        "            'Weighted_F1_Mean': np.mean(metrics['weighted_f1']),\n",
        "            'Weighted_F1_Std': np.std(metrics['weighted_f1']),\n",
        "            'Hierarchical_Precision_Mean': np.mean(metrics['hP']),\n",
        "            'Hierarchical_Precision_Std': np.std(metrics['hP']),\n",
        "            'Hierarchical_Recall_Mean': np.mean(metrics['hR']),\n",
        "            'Hierarchical_Recall_Std': np.std(metrics['hR']),\n",
        "            'Hierarchical_F1_Mean': np.mean(metrics['hF1']),\n",
        "            'Hierarchical_F1_Std': np.std(metrics['hF1'])\n",
        "        }\n",
        "        results_data.append(result_row)\n",
        "\n",
        "summary_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Display results by level\n",
        "for level in ['Root', 'Internal', 'External', 'Overall']:\n",
        "    print(f\"\\n{level} LEVEL PERFORMANCE:\")\n",
        "    print(\"-\" * 100)\n",
        "    level_results = summary_df[summary_df['Level'] == level]\n",
        "\n",
        "    for _, row in level_results.iterrows():\n",
        "        print(f\"{row['Algorithm']:20} | \"\n",
        "              f\"Macro F1: {row['Macro_F1_Mean']:.4f}±{row['Macro_F1_Std']:.4f} | \"\n",
        "              f\"Weighted F1: {row['Weighted_F1_Mean']:.4f}±{row['Weighted_F1_Std']:.4f} | \"\n",
        "              f\"Hierarchical F1: {row['Hierarchical_F1_Mean']:.4f}±{row['Hierarchical_F1_Std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKZz6eundfKU"
      },
      "outputs": [],
      "source": [
        "# Uniform LCPN ML\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# LCPN HIERARCHICAL MODELS LOOP\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training LCPN-Hierarchical-{model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Start timing for this model\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        fold_idx += 1\n",
        "        X_tr = X_train_w2v[train_idx]\n",
        "        X_val = X_train_w2v[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Get corresponding super labels\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        root_model = clone(model_class)\n",
        "        root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            # Apply resampling to internal data\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "            else:\n",
        "                internal_label_encoder = None\n",
        "                y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "            internal_model = clone(model_class)\n",
        "            internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            # Apply resampling to external data\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "            else:\n",
        "                external_label_encoder = None\n",
        "                y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "            external_model = clone(model_class)\n",
        "            external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        # Get root predictions\n",
        "        y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = external_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-w2v-{model_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-w2v-{model_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-w2v-{model_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\nLCPN-{model_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRNf5ut93HB-"
      },
      "outputs": [],
      "source": [
        "# LCPN GRU\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# LCPN HIERARCHICAL GRU\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training LCPN-Hierarchical-GRU\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Start timing for GRU\n",
        "model_start_time = time.time()\n",
        "\n",
        "# Metrics containers for GRU\n",
        "p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "hF1s=[]; hPs=[]; hRs=[]\n",
        "all_y_true = []; all_y_pred = []\n",
        "\n",
        "fold_idx = 0\n",
        "for train_idx, val_idx in skf.split(X_train_sequences, y_train):\n",
        "    fold_idx += 1\n",
        "    print(f\"\\n--- Fold {fold_idx} ---\")\n",
        "\n",
        "    # Get sequence data splits\n",
        "    X_tr_seq = X_train_sequences[train_idx]\n",
        "    X_val_seq = X_train_sequences[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    # SINGLE RESAMPLE PIPE\n",
        "    resample_pipe = ImbPipeline([\n",
        "        ('ros', RandomOverSampler(random_state=42)),\n",
        "        ('rus', RandomUnderSampler(random_state=42))\n",
        "    ])\n",
        "    X_tr_seq_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "    # Get corresponding super labels\n",
        "    y_super_tr_hybrid = []\n",
        "    for leaf_label in y_tr_hybrid:\n",
        "        super_label = leaf_to_super_name[leaf_label]\n",
        "        super_idx = list(super_labels).index(super_label)\n",
        "        y_super_tr_hybrid.append(super_idx)\n",
        "    y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "    # =====================================================================\n",
        "    # 1) Train ROOT level classifier\n",
        "    # =====================================================================\n",
        "    print(\"Training ROOT level...\")\n",
        "    # Convert to categorical for GRU\n",
        "    y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "    y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "    root_model = Sequential([\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=max_sequence_length,\n",
        "            trainable=False\n",
        "        ),\n",
        "        GRU(64),\n",
        "        Dense(len(super_labels), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    root_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    root_model.fit(\n",
        "        X_tr_seq_hybrid, y_super_tr_hybrid_cat,\n",
        "        validation_data=(X_val_seq, y_super_val_cat),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    # =====================================================================\n",
        "    # 2) Train INTERNAL level classifier\n",
        "    # =====================================================================\n",
        "    internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "    if internal_mask_tr.sum() > 0:\n",
        "        print(\"Training INTERNAL level...\")\n",
        "        X_tr_internal_seq = X_tr_seq_hybrid[internal_mask_tr]\n",
        "        y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "        resample_internal = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_internal_seq_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal_seq, y_tr_internal)\n",
        "\n",
        "        # Re-encode internal labels to be consecutive\n",
        "        internal_label_encoder = LabelEncoder()\n",
        "        y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "\n",
        "        # Convert to categorical for GRU\n",
        "        num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "        y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "        internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "        if internal_val_mask.sum() > 0:\n",
        "            X_val_internal_seq = X_val_seq[internal_val_mask]\n",
        "            y_val_internal = y_val[internal_val_mask]\n",
        "\n",
        "            y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "            y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "            validation_data = (X_val_internal_seq, y_val_internal_cat)\n",
        "            monitor_metric = 'val_loss'\n",
        "            print(f\"  Internal validation samples: {internal_val_mask.sum()}\")\n",
        "        else:\n",
        "            validation_data = None\n",
        "            monitor_metric = 'loss'\n",
        "            print(\"  No internal validation samples, monitoring loss\")\n",
        "\n",
        "        internal_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(num_internal_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        internal_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        internal_model.fit(\n",
        "            X_tr_internal_seq_hybrid, y_tr_internal_cat,\n",
        "            validation_data=validation_data,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"  No internal samples for training\")\n",
        "        internal_model = None\n",
        "        internal_label_encoder = None\n",
        "\n",
        "    # =====================================================================\n",
        "    # 3) Train EXTERNAL level classifier\n",
        "    # =====================================================================\n",
        "    external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "    if external_mask_tr.sum() > 0:\n",
        "        print(\"Training EXTERNAL level...\")\n",
        "        X_tr_external_seq = X_tr_seq_hybrid[external_mask_tr]\n",
        "        y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "        resample_external = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_external_seq_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external_seq, y_tr_external)\n",
        "\n",
        "        # ✅ Re-encode external labels to be consecutive\n",
        "        external_label_encoder = LabelEncoder()\n",
        "        y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "\n",
        "        # Convert to categorical for GRU\n",
        "        num_external_classes = len(np.unique(y_tr_external_hybrid_encoded))\n",
        "        y_tr_external_cat = tf.keras.utils.to_categorical(y_tr_external_hybrid_encoded, num_classes=num_external_classes)\n",
        "\n",
        "        external_val_mask = (y_super_val == list(super_labels).index('External'))\n",
        "        if external_val_mask.sum() > 0:\n",
        "            X_val_external_seq = X_val_seq[external_val_mask]\n",
        "            y_val_external = y_val[external_val_mask]\n",
        "\n",
        "            y_val_external_encoded = external_label_encoder.transform(y_val_external)\n",
        "            y_val_external_cat = tf.keras.utils.to_categorical(y_val_external_encoded, num_classes=num_external_classes)\n",
        "\n",
        "            validation_data = (X_val_external_seq, y_val_external_cat)\n",
        "            monitor_metric = 'val_loss'\n",
        "            print(f\"  External validation samples: {external_val_mask.sum()}\")\n",
        "        else:\n",
        "            validation_data = None\n",
        "            monitor_metric = 'loss'\n",
        "            print(\"  No external validation samples, monitoring loss\")\n",
        "\n",
        "        external_model = Sequential([\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_sequence_length,\n",
        "                trainable=False\n",
        "            ),\n",
        "            GRU(64),\n",
        "            Dense(num_external_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        external_model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        external_model.fit(\n",
        "            X_tr_external_seq_hybrid, y_tr_external_cat,\n",
        "            validation_data=validation_data,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "            verbose=0,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"  No external samples for training\")\n",
        "        external_model = None\n",
        "        external_label_encoder = None\n",
        "\n",
        "    # =====================================================================\n",
        "    # 4) Hierarchical Prediction\n",
        "    # =====================================================================\n",
        "    print(\"Making hierarchical predictions...\")\n",
        "    # Get root predictions\n",
        "    y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "    y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "\n",
        "    # Build final leaf predictions\n",
        "    y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "    # Find normal leaf index\n",
        "    normal_leaf_idx = None\n",
        "    for idx, name in leaf_index_to_name.items():\n",
        "        if name.lower() == 'normal' or name == 'Normal':\n",
        "            normal_leaf_idx = idx\n",
        "            break\n",
        "    if normal_leaf_idx is None:\n",
        "        normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "        normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "    # Apply hierarchical prediction logic\n",
        "    for i in range(len(y_super_val_pred)):\n",
        "        pred_sup = y_super_val_pred[i]\n",
        "\n",
        "        if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "            single_sample = X_val_seq[i:i+1]\n",
        "            pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "            # Decode back to original label\n",
        "            y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "        elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "            single_sample = X_val_seq[i:i+1]\n",
        "            pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "            pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "            # Decode back to original label\n",
        "            y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "        else:\n",
        "            if normal_leaf_idx is not None:\n",
        "                y_val_final_pred[i] = normal_leaf_idx\n",
        "            else:\n",
        "                vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "    # =====================================================================\n",
        "    # 5) Calculate Metrics\n",
        "    # =====================================================================\n",
        "    p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "    p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "    hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "    # Store fold results\n",
        "    p_macros.append(p_macro)\n",
        "    r_macros.append(r_macro)\n",
        "    f1_macros.append(f1_macro)\n",
        "    p_weights.append(p_weighted)\n",
        "    r_weights.append(r_weighted)\n",
        "    f1_weights.append(f1_weighted)\n",
        "    hF1s.append(hF1)\n",
        "    hPs.append(hP)\n",
        "    hRs.append(hR)\n",
        "\n",
        "    all_y_true.extend(list(y_val))\n",
        "    all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "    # Save individual fold result\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-W2V-GRU-skipgram',\n",
        "        'Fold': f'Fold_{fold_idx}',\n",
        "        'Macro_Precision': p_macro,\n",
        "        'Macro_Recall': r_macro,\n",
        "        'Macro_F1': f1_macro,\n",
        "        'Weighted_Precision': p_weighted,\n",
        "        'Weighted_Recall': r_weighted,\n",
        "        'Weighted_F1': f1_weighted,\n",
        "        'Hierarchical_Precision': hP,\n",
        "        'Hierarchical_Recall': hR,\n",
        "        'Hierarchical_F1': hF1,\n",
        "        'Type': 'Fold',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {fold_idx} - MacroF1: {f1_macro:.4f}, WeightedF1: {f1_weighted:.4f}, HierF1: {hF1:.4f}\")\n",
        "\n",
        "# Calculate total time for this model\n",
        "model_total_time = time.time() - model_start_time\n",
        "\n",
        "# Calculate and save final averages for this model\n",
        "all_results.append({\n",
        "    'Model': f'LCPN-W2V-GRU-skipgram',\n",
        "    'Fold': 'Mean',\n",
        "    'Macro_Precision': np.mean(p_macros),\n",
        "    'Macro_Recall': np.mean(r_macros),\n",
        "    'Macro_F1': np.mean(f1_macros),\n",
        "    'Weighted_Precision': np.mean(p_weights),\n",
        "    'Weighted_Recall': np.mean(r_weights),\n",
        "    'Weighted_F1': np.mean(f1_weights),\n",
        "    'Hierarchical_Precision': np.mean(hPs),\n",
        "    'Hierarchical_Recall': np.mean(hRs),\n",
        "    'Hierarchical_F1': np.mean(hF1s),\n",
        "    'Type': 'Mean',\n",
        "    'Time': model_total_time\n",
        "})\n",
        "\n",
        "all_results.append({\n",
        "    'Model': f'LCPN-W2V-GRU-skipgram',\n",
        "    'Fold': 'Std',\n",
        "    'Macro_Precision': np.std(p_macros),\n",
        "    'Macro_Recall': np.std(r_macros),\n",
        "    'Macro_F1': np.std(f1_macros),\n",
        "    'Weighted_Precision': np.std(p_weights),\n",
        "    'Weighted_Recall': np.std(r_weights),\n",
        "    'Weighted_F1': np.std(f1_weights),\n",
        "    'Hierarchical_Precision': np.std(hPs),\n",
        "    'Hierarchical_Recall': np.std(hRs),\n",
        "    'Hierarchical_F1': np.std(hF1s),\n",
        "    'Type': 'Std',\n",
        "    'Time': ''\n",
        "})\n",
        "\n",
        "print(f\"\\nLCPN-GRU Final Results:\")\n",
        "print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcAvMHTwdCM4",
        "outputId": "9a248e38-343b-470d-ce02-103d7540931c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-316571622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hybrid LCPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load common data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Hybrid LCPN\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN CONFIGURATION\n",
        "# =============================================================================\n",
        "hybrid_configs = {\n",
        "    'Hybrid-GRU-GRU-SVM(rbf)': {\n",
        "        'root': 'gru',\n",
        "        'internal': 'gru',\n",
        "        'external': 'SVM-rbf'\n",
        "    },\n",
        "}\n",
        "\n",
        "# Define all available algorithms\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN IMPLEMENTATION\n",
        "# =============================================================================\n",
        "for hybrid_name, config in hybrid_configs.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {hybrid_name}\")\n",
        "    print(f\"Configuration: Root={config['root']}, Internal={config['internal']}, External={config['external']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"\\n--- Fold {fold_idx} ---\")\n",
        "\n",
        "        # Split data for both feature types\n",
        "        X_tr = X_train_w2v[train_idx]\n",
        "        X_val = X_train_w2v[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        original_indices = np.arange(len(X_tr))\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        # Choose resampling data based on root algorithm type\n",
        "        if config['root'] == 'gru':\n",
        "            # For GRU root, resample sequence data\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "            # Also resample the original indices to maintain mapping\n",
        "            _, original_indices_hybrid = resample_pipe.fit_resample(original_indices.reshape(-1, 1), y_tr)\n",
        "            original_indices_hybrid = original_indices_hybrid.flatten()\n",
        "        else:\n",
        "            # For ML root, resample feature data\n",
        "            X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "            original_indices_hybrid = None\n",
        "\n",
        "        # Get corresponding super labels\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        print(f\"Training ROOT level with {config['root']}...\")\n",
        "        if config['root'] == 'gru':\n",
        "            # GRU Root Model\n",
        "            y_super_tr_hybrid_cat = tf.keras.utils.to_categorical(y_super_tr_hybrid, num_classes=len(super_labels))\n",
        "            y_super_val_cat = tf.keras.utils.to_categorical(y_super_val, num_classes=len(super_labels))\n",
        "\n",
        "            root_model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(len(super_labels), activation='softmax')\n",
        "            ])\n",
        "\n",
        "            root_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            root_model.fit(\n",
        "                X_tr_hybrid, y_super_tr_hybrid_cat,\n",
        "                validation_data=(X_val_seq, y_super_val_cat),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "        else:\n",
        "            # ML Root Model\n",
        "            root_model = clone(ml_models[config['root']])\n",
        "            root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            print(f\"Training INTERNAL level with {config['internal']}...\")\n",
        "\n",
        "            # Get internal training data based on algorithm type\n",
        "            if config['internal'] == 'gru':\n",
        "                X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            else:\n",
        "                # For ML models, we need to get the corresponding feature data\n",
        "                if config['root'] == 'gru':\n",
        "                    # Map back from resampled sequence indices to original feature indices\n",
        "                    internal_hybrid_indices = np.where(internal_mask_tr)[0]\n",
        "                    internal_original_indices = original_indices_hybrid[internal_hybrid_indices]\n",
        "                    X_tr_internal = X_tr[internal_original_indices]\n",
        "                else:\n",
        "                    # Root is already ML, so we can use the resampled feature data directly\n",
        "                    X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            if config['internal'] == 'gru':\n",
        "                # GRU Internal Model\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                num_internal_classes = len(np.unique(y_tr_internal_hybrid_encoded))\n",
        "                y_tr_internal_cat = tf.keras.utils.to_categorical(y_tr_internal_hybrid_encoded, num_classes=num_internal_classes)\n",
        "\n",
        "                internal_val_mask = (y_super_val == list(super_labels).index('Internal'))\n",
        "                if internal_val_mask.sum() > 0:\n",
        "                    X_val_internal = X_val_seq[internal_val_mask]\n",
        "                    y_val_internal = y_val[internal_val_mask]\n",
        "                    y_val_internal_encoded = internal_label_encoder.transform(y_val_internal)\n",
        "                    y_val_internal_cat = tf.keras.utils.to_categorical(y_val_internal_encoded, num_classes=num_internal_classes)\n",
        "                    validation_data = (X_val_internal, y_val_internal_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                internal_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_internal_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                internal_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                internal_model.fit(\n",
        "                    X_tr_internal_hybrid, y_tr_internal_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                if config['internal'] in ['XGBoost', 'LightGBM']:\n",
        "                    internal_label_encoder = LabelEncoder()\n",
        "                    y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                else:\n",
        "                    internal_label_encoder = None\n",
        "                    y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "                internal_model = clone(ml_models[config['internal']])\n",
        "                internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            print(f\"Training EXTERNAL level with {config['external']}...\")\n",
        "\n",
        "            # Get external training data based on algorithm type\n",
        "            if config['external'] == 'gru':\n",
        "                X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            else:\n",
        "                # For ML models, we need to get the corresponding feature data\n",
        "                if config['root'] == 'gru':\n",
        "                    external_hybrid_indices = np.where(external_mask_tr)[0]\n",
        "                    external_original_indices = original_indices_hybrid[external_hybrid_indices]\n",
        "                    X_tr_external = X_tr[external_original_indices]\n",
        "                else:\n",
        "                    X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            if config['external'] == 'gru':\n",
        "                # GRU External Model\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                num_external_classes = len(np.unique(y_tr_external_hybrid_encoded))\n",
        "                y_tr_external_cat = tf.keras.utils.to_categorical(y_tr_external_hybrid_encoded, num_classes=num_external_classes)\n",
        "\n",
        "                external_val_mask = (y_super_val == list(super_labels).index('External'))\n",
        "                if external_val_mask.sum() > 0:\n",
        "                    X_val_external = X_val_seq[external_val_mask]\n",
        "                    y_val_external = y_val[external_val_mask]\n",
        "                    y_val_external_encoded = external_label_encoder.transform(y_val_external)\n",
        "                    y_val_external_cat = tf.keras.utils.to_categorical(y_val_external_encoded, num_classes=num_external_classes)\n",
        "                    validation_data = (X_val_external, y_val_external_cat)\n",
        "                    monitor_metric = 'val_loss'\n",
        "                else:\n",
        "                    validation_data = None\n",
        "                    monitor_metric = 'loss'\n",
        "\n",
        "                external_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_external_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                external_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                external_model.fit(\n",
        "                    X_tr_external_hybrid, y_tr_external_cat,\n",
        "                    validation_data=validation_data,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            else:\n",
        "                # ML External Model\n",
        "                if config['external'] in ['XGBoost', 'LightGBM']:\n",
        "                    external_label_encoder = LabelEncoder()\n",
        "                    y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                else:\n",
        "                    external_label_encoder = None\n",
        "                    y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "                external_model = clone(ml_models[config['external']])\n",
        "                external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        print(\"Making hierarchical predictions...\")\n",
        "\n",
        "        # Get root predictions\n",
        "        if config['root'] == 'gru':\n",
        "            y_super_val_pred_proba = root_model.predict(X_val_seq, verbose=0)\n",
        "            y_super_val_pred = np.argmax(y_super_val_pred_proba, axis=1)\n",
        "        else:\n",
        "            y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                if config['internal'] == 'gru':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = internal_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if config['internal'] in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                        pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                if config['external'] == 'gru':\n",
        "                    single_sample = X_val_seq[i:i+1]\n",
        "                    pred_proba = external_model.predict(single_sample, verbose=0)\n",
        "                    pred_encoded = np.argmax(pred_proba, axis=1)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    single_sample = X_val[i].reshape(1, -1)\n",
        "                    if config['external'] in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                        pred_encoded = external_model.predict(single_sample)[0]\n",
        "                        y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                    else:\n",
        "                        y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'W2V-{hybrid_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} - MacroF1: {f1_macro:.4f}, WeightedF1: {f1_weighted:.4f}, HierF1: {hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'W2V-{hybrid_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'W2V-{hybrid_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{hybrid_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th3MZM4go9jB"
      },
      "source": [
        "# TF-IDF Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBds56Sbo_nF",
        "outputId": "e42b34a5-ed65-4f55-99f9-a1a46c721665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LightGBM with TF-IDF\n",
            "============================================================\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.052226 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232451\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 1 done. MacroF1=0.6964, WeightedF1=0.7387, HierF1=0.8768\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.585321 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 233295\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 2 done. MacroF1=0.6908, WeightedF1=0.7311, HierF1=0.8750\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.584452 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232957\n",
            "[LightGBM] [Info] Number of data points in the train set: 73220, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 3 done. MacroF1=0.6906, WeightedF1=0.7239, HierF1=0.8707\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.567657 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 232669\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 4 done. MacroF1=0.6829, WeightedF1=0.7301, HierF1=0.8715\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.564039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232511\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Fold 5 done. MacroF1=0.6947, WeightedF1=0.7377, HierF1=0.8770\n",
            "\n",
            "LightGBM with TF-IDF Final Results:\n",
            "Macro F1: 0.6911 ± 0.0047\n",
            "Weighted F1: 0.7323 ± 0.0054\n",
            "Hierarchical F1: 0.8742 ± 0.0027\n",
            "Total Training Time: 558.70 seconds\n",
            "\n",
            "============================================================\n",
            "Training XGBoost with TF-IDF\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6893, WeightedF1=0.7334, HierF1=0.8742\n",
            "Fold 2 done. MacroF1=0.6836, WeightedF1=0.7256, HierF1=0.8718\n",
            "Fold 3 done. MacroF1=0.6831, WeightedF1=0.7179, HierF1=0.8677\n",
            "Fold 4 done. MacroF1=0.6801, WeightedF1=0.7302, HierF1=0.8714\n",
            "Fold 5 done. MacroF1=0.6882, WeightedF1=0.7331, HierF1=0.8737\n",
            "\n",
            "XGBoost with TF-IDF Final Results:\n",
            "Macro F1: 0.6849 ± 0.0034\n",
            "Weighted F1: 0.7280 ± 0.0058\n",
            "Hierarchical F1: 0.8717 ± 0.0023\n",
            "Total Training Time: 863.30 seconds\n",
            "\n",
            "============================================================\n",
            "Training LogisticRegression with TF-IDF\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6280, WeightedF1=0.7012, HierF1=0.8503\n",
            "Fold 2 done. MacroF1=0.6171, WeightedF1=0.6961, HierF1=0.8502\n",
            "Fold 3 done. MacroF1=0.6210, WeightedF1=0.6870, HierF1=0.8464\n",
            "Fold 4 done. MacroF1=0.6225, WeightedF1=0.6940, HierF1=0.8463\n",
            "Fold 5 done. MacroF1=0.6273, WeightedF1=0.7016, HierF1=0.8510\n",
            "\n",
            "LogisticRegression with TF-IDF Final Results:\n",
            "Macro F1: 0.6232 ± 0.0041\n",
            "Weighted F1: 0.6960 ± 0.0054\n",
            "Hierarchical F1: 0.8489 ± 0.0020\n",
            "Total Training Time: 26.10 seconds\n",
            "\n",
            "============================================================\n",
            "Training SVM-linear with TF-IDF\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6271, WeightedF1=0.6979, HierF1=0.8520\n",
            "Fold 2 done. MacroF1=0.6242, WeightedF1=0.6981, HierF1=0.8532\n",
            "Fold 3 done. MacroF1=0.6222, WeightedF1=0.6898, HierF1=0.8480\n",
            "Fold 4 done. MacroF1=0.6265, WeightedF1=0.6962, HierF1=0.8503\n",
            "Fold 5 done. MacroF1=0.6275, WeightedF1=0.7003, HierF1=0.8523\n",
            "\n",
            "SVM-linear with TF-IDF Final Results:\n",
            "Macro F1: 0.6255 ± 0.0020\n",
            "Weighted F1: 0.6965 ± 0.0036\n",
            "Hierarchical F1: 0.8511 ± 0.0018\n",
            "Total Training Time: 4307.78 seconds\n",
            "\n",
            "============================================================\n",
            "Training SVM-rbf with TF-IDF\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6986, WeightedF1=0.7418, HierF1=0.8808\n",
            "Fold 2 done. MacroF1=0.6953, WeightedF1=0.7378, HierF1=0.8798\n",
            "Fold 3 done. MacroF1=0.6959, WeightedF1=0.7322, HierF1=0.8752\n",
            "Fold 4 done. MacroF1=0.6904, WeightedF1=0.7343, HierF1=0.8766\n",
            "Fold 5 done. MacroF1=0.7025, WeightedF1=0.7447, HierF1=0.8814\n",
            "\n",
            "SVM-rbf with TF-IDF Final Results:\n",
            "Macro F1: 0.6965 ± 0.0040\n",
            "Weighted F1: 0.7382 ± 0.0046\n",
            "Hierarchical F1: 0.8788 ± 0.0024\n",
            "Total Training Time: 8512.87 seconds\n",
            "\n",
            "============================================================\n",
            "Training RandomForest with TF-IDF\n",
            "============================================================\n",
            "Fold 1 done. MacroF1=0.6771, WeightedF1=0.7189, HierF1=0.8700\n",
            "Fold 2 done. MacroF1=0.6708, WeightedF1=0.7191, HierF1=0.8695\n",
            "Fold 3 done. MacroF1=0.6736, WeightedF1=0.7098, HierF1=0.8657\n",
            "Fold 4 done. MacroF1=0.6624, WeightedF1=0.7082, HierF1=0.8640\n",
            "Fold 5 done. MacroF1=0.6802, WeightedF1=0.7231, HierF1=0.8719\n",
            "\n",
            "RandomForest with TF-IDF Final Results:\n",
            "Macro F1: 0.6728 ± 0.0061\n",
            "Weighted F1: 0.7158 ± 0.0058\n",
            "Hierarchical F1: 0.8682 ± 0.0029\n",
            "Total Training Time: 1778.65 seconds\n"
          ]
        }
      ],
      "source": [
        "# FLAT TFIDF\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load( open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "ml_models = {\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# MACHINE LEARNING MODELS LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name} with TF-IDF\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        model = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42)),\n",
        "            ('clf', clone(model_class))\n",
        "        ])\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-TFIDF-{model_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{model_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{model_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{model_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOaVEBG-YUJ0",
        "outputId": "0bbe7f8b-3f68-47e1-f671-7263e3a93343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training HardVoting-SVM(rbf)-LightGBM-XGBoost with TF-IDF\n",
            "============================================================\n",
            "--- Fold 1 ---\n",
            "  Training SVM-rbf...\n",
            "  Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.721023 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232451\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Training XGBoost...\n",
            "Fold 1 done. MacroF1=0.7141, WeightedF1=0.7492, HierF1=0.8834\n",
            "--- Fold 2 ---\n",
            "  Training SVM-rbf...\n",
            "  Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.932397 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 233295\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Training XGBoost...\n",
            "Fold 2 done. MacroF1=0.7050, WeightedF1=0.7403, HierF1=0.8797\n",
            "--- Fold 3 ---\n",
            "  Training SVM-rbf...\n",
            "  Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.677420 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 232957\n",
            "[LightGBM] [Info] Number of data points in the train set: 73220, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Training XGBoost...\n",
            "Fold 3 done. MacroF1=0.7049, WeightedF1=0.7326, HierF1=0.8755\n",
            "--- Fold 4 ---\n",
            "  Training SVM-rbf...\n",
            "  Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.689546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232669\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Training XGBoost...\n",
            "Fold 4 done. MacroF1=0.6988, WeightedF1=0.7416, HierF1=0.8777\n",
            "--- Fold 5 ---\n",
            "  Training SVM-rbf...\n",
            "  Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.722388 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232511\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Training XGBoost...\n",
            "Fold 5 done. MacroF1=0.7153, WeightedF1=0.7521, HierF1=0.8842\n",
            "\n",
            "HardVoting-SVM(rbf)-LightGBM-XGBoost with TF-IDF Final Results:\n",
            "Macro F1: 0.7076 ± 0.0062\n",
            "Weighted F1: 0.7431 ± 0.0069\n",
            "Hierarchical F1: 0.8801 ± 0.0033\n",
            "Total Training Time: 12902.08 seconds\n"
          ]
        }
      ],
      "source": [
        "# HARD VOTING ENSEMBLES WITH TF-IDF\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define voting ensembles\n",
        "voting_ensembles = {\n",
        "    'HardVoting-SVM(rbf)-LightGBM-XGBoost': ['SVM-rbf', 'LightGBM', 'XGBoost'],\n",
        "    'HardVoting-SVM(rbf)-LightGBM-RandomForest': ['SVM-rbf', 'LightGBM', 'RandomForest'],\n",
        "}\n",
        "\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# HARD VOTING ENSEMBLES LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for ensemble_name, model_names in voting_ensembles.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name} with TF-IDF\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"--- Fold {fold_idx} ---\")\n",
        "\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Store predictions from each model and label encoders for tree-based models\n",
        "        fold_predictions = []\n",
        "        label_encoders = {}\n",
        "\n",
        "        # Train each model in the ensemble\n",
        "        for model_name in model_names:\n",
        "            print(f\"  Training {model_name}...\")\n",
        "\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                # For tree-based models, we need to re-encode labels to be consecutive\n",
        "                label_encoder = LabelEncoder()\n",
        "                y_tr_hybrid_encoded = label_encoder.fit_transform(y_tr_hybrid)\n",
        "                label_encoders[model_name] = label_encoder\n",
        "\n",
        "                model = clone(ml_models[model_name])\n",
        "                model.fit(X_tr_hybrid, y_tr_hybrid_encoded)\n",
        "\n",
        "                # Predict and decode back to original labels\n",
        "                y_pred_encoded = model.predict(X_val)\n",
        "                y_pred_ml = label_encoder.inverse_transform(y_pred_encoded)\n",
        "                fold_predictions.append(y_pred_ml)\n",
        "\n",
        "            else:\n",
        "                # For non-tree-based models\n",
        "                model = clone(ml_models[model_name])\n",
        "                model.fit(X_tr_hybrid, y_tr_hybrid)\n",
        "                y_pred_ml = model.predict(X_val)\n",
        "                fold_predictions.append(y_pred_ml)\n",
        "\n",
        "        # Hard Voting: majority vote\n",
        "        fold_predictions = np.array(fold_predictions)\n",
        "        y_pred_vote = []\n",
        "\n",
        "        for i in range(len(y_val)):\n",
        "            votes = fold_predictions[:, i]\n",
        "            unique, counts = np.unique(votes, return_counts=True)\n",
        "            max_count = np.max(counts)\n",
        "\n",
        "            # Handle ties: if multiple classes have same max votes\n",
        "            if np.sum(counts == max_count) > 1:\n",
        "                # Tie-breaking strategy: use the prediction from the first model in the ensemble\n",
        "                y_pred_vote.append(votes[0])\n",
        "            else:\n",
        "                y_pred_vote.append(unique[np.argmax(counts)])\n",
        "\n",
        "        y_pred = np.array(y_pred_vote)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsupQClmme3f"
      },
      "outputs": [],
      "source": [
        "# WEIGHTED SOFT VOTING ENSEMBLES - TF-IDF\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Define machine learning models\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Define weighted soft voting ensembles\n",
        "weighted_ensembles = {\n",
        "    'WeightedSoft-SVM(rbf)-LightGBM-XGBoost': {\n",
        "        'models': ['SVM-rbf', 'LightGBM', 'XGBoost'],\n",
        "        'weights': [0.7382, 0.7323, 0.728]\n",
        "    },\n",
        "    'WeightedSoft-SVM(rbf)-LightGBM-RandomForest': {\n",
        "        'models': ['SVM-rbf', 'LightGBM', 'RandomForest'],\n",
        "        'weights': [0.7382, 0.7323, 0.7158]\n",
        "    },\n",
        "}\n",
        "# =============================================================================\n",
        "# WEIGHTED SOFT VOTING ENSEMBLES LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in weighted_ensembles.items():\n",
        "    model_names = ensemble_config['models']\n",
        "    performance_weights = ensemble_config['weights']\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    normalized_weights = np.array(performance_weights) / np.sum(performance_weights)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name} with TF-IDF\")\n",
        "    print(f\"Models: {model_names}\")\n",
        "    print(f\"Model Weights: {dict(zip(model_names, normalized_weights))}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get data splits\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        # Store probability predictions from each model\n",
        "        fold_probabilities = []\n",
        "        label_encoders = {}\n",
        "\n",
        "        # Train each model in the ensemble and get probabilities\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            print(f\"  Training {model_name}...\")\n",
        "\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                label_encoder = LabelEncoder()\n",
        "                y_tr_encoded = label_encoder.fit_transform(y_tr_hybrid)\n",
        "                label_encoders[model_name] = label_encoder\n",
        "\n",
        "                model = clone(ml_models[model_name])\n",
        "\n",
        "                model.fit(X_tr_hybrid, y_tr_encoded)\n",
        "\n",
        "                y_proba = model.predict_proba(X_val)\n",
        "\n",
        "                fold_probabilities.append(y_proba)\n",
        "\n",
        "            else:\n",
        "                model = clone(ml_models[model_name])\n",
        "\n",
        "                model.fit(X_tr_hybrid, y_tr_hybrid)\n",
        "\n",
        "                y_proba = model.predict_proba(X_val)\n",
        "\n",
        "                fold_probabilities.append(y_proba)\n",
        "\n",
        "        # Weighted Soft Voting\n",
        "        weighted_probabilities = np.zeros_like(fold_probabilities[0])\n",
        "\n",
        "        for i, proba in enumerate(fold_probabilities):\n",
        "            # Apply normalized performance weight to each model probabilities\n",
        "            weighted_probabilities += normalized_weights[i] * proba\n",
        "\n",
        "        # Get final predictions\n",
        "        y_pred_encoded = np.argmax(weighted_probabilities, axis=1)\n",
        "\n",
        "        y_pred = y_pred_encoded\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Used Weights: {dict(zip(model_names, normalized_weights))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNgSb6SBE6GN",
        "outputId": "cb306d6b-fb82-4537-98b8-313be6568011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training Stacking-SVM-LGBM-XGB-LR with TF-IDF\n",
            "Base Models: ['SVM-rbf', 'LightGBM', 'XGBoost']\n",
            "Meta Model: LogisticRegression\n",
            "============================================================\n",
            "Stage 1: Generating Out-of-Fold predictions...\n",
            "  Generating OOF predictions - Fold 1\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.676824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232451\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Generating OOF predictions - Fold 2\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.594147 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 233295\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Generating OOF predictions - Fold 3\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.625149 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 232957\n",
            "[LightGBM] [Info] Number of data points in the train set: 73220, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Generating OOF predictions - Fold 4\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.113938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 232669\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "  Generating OOF predictions - Fold 5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.705290 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 232511\n",
            "[LightGBM] [Info] Number of data points in the train set: 73213, number of used features: 1000\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "[LightGBM] [Info] Start training from score -1.945910\n",
            "Stage 1 completed: OOF predictions generated for all base models\n",
            "Stage 2: Training and evaluating meta-classifier...\n",
            "Meta-features shape: (42144, 21)\n",
            "Number of meta-features: 21\n",
            "Fold 1 done. MacroF1=0.6837, WeightedF1=0.7376, HierF1=0.8732\n",
            "Fold 2 done. MacroF1=0.6765, WeightedF1=0.7340, HierF1=0.8727\n",
            "Fold 3 done. MacroF1=0.6770, WeightedF1=0.7306, HierF1=0.8707\n",
            "Fold 4 done. MacroF1=0.6698, WeightedF1=0.7310, HierF1=0.8684\n",
            "Fold 5 done. MacroF1=0.6839, WeightedF1=0.7408, HierF1=0.8742\n",
            "\n",
            "Stacking-SVM-LGBM-XGB-LR with TF-IDF Final Results:\n",
            "Macro F1: 0.6782 ± 0.0053\n",
            "Weighted F1: 0.7348 ± 0.0039\n",
            "Hierarchical F1: 0.8718 ± 0.0021\n",
            "Total Training Time: 39372.18 seconds\n",
            "Base Models: ['SVM-rbf', 'LightGBM', 'XGBoost']\n",
            "Meta Model: LogisticRegression\n",
            "Meta-features shape: (42144, 21)\n"
          ]
        }
      ],
      "source": [
        "# STACKING ENSEMBLES - TF-IDF\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', probability=True, random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "stacking_ensembles = {\n",
        "    'Stacking-SVM-LGBM-XGB-LR': {\n",
        "        'base_models': ['SVM-rbf', 'LightGBM', 'XGBoost'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    },\n",
        "    'Stacking-SVM-LGBM-RF-LR': {\n",
        "        'base_models': ['SVM-rbf', 'LightGBM', 'RandomForest'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    },\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# STACKING ENSEMBLES LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for ensemble_name, ensemble_config in stacking_ensembles.items():\n",
        "    base_model_names = ensemble_config['base_models']\n",
        "    meta_model = ensemble_config['meta_model']\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name} with TF-IDF\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    print(\"Stage 1: Generating Out-of-Fold predictions...\")\n",
        "    # Initialize OOF arrays for base model predictions\n",
        "    oof_predictions = {}\n",
        "    for model_name in base_model_names:\n",
        "        oof_predictions[model_name] = np.zeros((X_train_tfidf.shape[0], num_classes))\n",
        "\n",
        "    # Generate OOF predictions\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"  Generating OOF predictions - Fold {fold_idx}\")\n",
        "\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        for model_name in base_model_names:\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                # Handle tree-based models with label encoding\n",
        "                label_encoder = LabelEncoder()\n",
        "                y_tr_encoded = label_encoder.fit_transform(y_tr_hybrid)\n",
        "\n",
        "                model = clone(ml_models[model_name])\n",
        "                model.fit(X_tr_hybrid, y_tr_encoded)\n",
        "\n",
        "                y_proba = model.predict_proba(X_val)\n",
        "                oof_predictions[model_name][val_idx] = y_proba\n",
        "\n",
        "            else:\n",
        "                model = clone(ml_models[model_name])\n",
        "                model.fit(X_tr_hybrid, y_tr_hybrid)\n",
        "\n",
        "                y_proba = model.predict_proba(X_val)\n",
        "                oof_predictions[model_name][val_idx] = y_proba\n",
        "\n",
        "    print(\"Stage 1 completed: OOF predictions generated for all base models\")\n",
        "    print(\"Stage 2: Training and evaluating meta-classifier...\")\n",
        "\n",
        "    # Create meta-features dataset\n",
        "    meta_features_list = []\n",
        "    for model_name in base_model_names:\n",
        "        meta_features_list.append(oof_predictions[model_name])\n",
        "\n",
        "    X_meta = np.hstack(meta_features_list)\n",
        "\n",
        "    print(f\"Meta-features shape: {X_meta.shape}\")\n",
        "    print(f\"Number of meta-features: {X_meta.shape[1]}\")\n",
        "\n",
        "    # Metrics containers\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    # Cross-validation on meta-features\n",
        "    for train_idx, val_idx in skf.split(X_meta, y_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get meta-features splits\n",
        "        X_tr_meta = X_meta[train_idx]\n",
        "        X_val_meta = X_meta[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        # RESAMPLING FOR META-CLASSIFIER\n",
        "        resample_pipe_meta = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_meta_hybrid, y_tr_hybrid = resample_pipe_meta.fit_resample(X_tr_meta, y_tr)\n",
        "\n",
        "        # Train meta-classifier\n",
        "        meta_model_clone = clone(meta_model)\n",
        "        meta_model_clone.fit(X_tr_meta_hybrid, y_tr_hybrid)\n",
        "\n",
        "        y_pred = meta_model_clone.predict(X_val_meta)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this ensemble\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-TFIDF-{ensemble_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "    print(f\"Meta-features shape: {X_meta.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7fvjvCCiQym",
        "outputId": "a0e07871-4b27-4b75-c72c-1dc76a25123b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THIS INSTANCE WILL TEST: Root algorithms = ['XGBoost', 'RandomForest']\n",
            "\n",
            "================================================================================\n",
            "HYBRID LCPN COMBINATION SEARCH - WITH LABEL ENCODERS\n",
            "================================================================================\n",
            "\n",
            "---- Fold 1 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-XGBoost-XGBoost-XGBoost: Weighted F1 = 0.7275\n",
            "  Hybrid-XGBoost-XGBoost-LightGBM: Weighted F1 = 0.7278\n",
            "  Hybrid-XGBoost-XGBoost-SVM-rbf: Weighted F1 = 0.7275\n",
            "  Hybrid-XGBoost-XGBoost-RandomForest: Weighted F1 = 0.7276\n",
            "  Hybrid-XGBoost-LightGBM-XGBoost: Weighted F1 = 0.7276\n",
            "  Hybrid-XGBoost-LightGBM-LightGBM: Weighted F1 = 0.7280\n",
            "  Hybrid-XGBoost-LightGBM-SVM-rbf: Weighted F1 = 0.7277\n",
            "  Hybrid-XGBoost-LightGBM-RandomForest: Weighted F1 = 0.7278\n",
            "  Hybrid-XGBoost-SVM-rbf-XGBoost: Weighted F1 = 0.7284\n",
            "  Hybrid-XGBoost-SVM-rbf-LightGBM: Weighted F1 = 0.7287\n",
            "  Hybrid-XGBoost-SVM-rbf-SVM-rbf: Weighted F1 = 0.7285\n",
            "  Hybrid-XGBoost-SVM-rbf-RandomForest: Weighted F1 = 0.7285\n",
            "  Hybrid-XGBoost-RandomForest-XGBoost: Weighted F1 = 0.7176\n",
            "  Hybrid-XGBoost-RandomForest-LightGBM: Weighted F1 = 0.7180\n",
            "  Hybrid-XGBoost-RandomForest-SVM-rbf: Weighted F1 = 0.7177\n",
            "  Hybrid-XGBoost-RandomForest-RandomForest: Weighted F1 = 0.7178\n",
            "  Hybrid-RandomForest-XGBoost-XGBoost: Weighted F1 = 0.7242\n",
            "  Hybrid-RandomForest-XGBoost-LightGBM: Weighted F1 = 0.7242\n",
            "  Hybrid-RandomForest-XGBoost-SVM-rbf: Weighted F1 = 0.7239\n",
            "  Hybrid-RandomForest-XGBoost-RandomForest: Weighted F1 = 0.7242\n",
            "  Hybrid-RandomForest-LightGBM-XGBoost: Weighted F1 = 0.7257\n",
            "  Hybrid-RandomForest-LightGBM-LightGBM: Weighted F1 = 0.7257\n",
            "  Hybrid-RandomForest-LightGBM-SVM-rbf: Weighted F1 = 0.7254\n",
            "  Hybrid-RandomForest-LightGBM-RandomForest: Weighted F1 = 0.7257\n",
            "  Hybrid-RandomForest-SVM-rbf-XGBoost: Weighted F1 = 0.7251\n",
            "  Hybrid-RandomForest-SVM-rbf-LightGBM: Weighted F1 = 0.7251\n",
            "  Hybrid-RandomForest-SVM-rbf-SVM-rbf: Weighted F1 = 0.7248\n",
            "  Hybrid-RandomForest-SVM-rbf-RandomForest: Weighted F1 = 0.7251\n",
            "  Hybrid-RandomForest-RandomForest-XGBoost: Weighted F1 = 0.7134\n",
            "  Hybrid-RandomForest-RandomForest-LightGBM: Weighted F1 = 0.7134\n",
            "  Hybrid-RandomForest-RandomForest-SVM-rbf: Weighted F1 = 0.7131\n",
            "  Hybrid-RandomForest-RandomForest-RandomForest: Weighted F1 = 0.7134\n",
            "Fold 1 completed: 32 combinations tested\n",
            "\n",
            "---- Fold 2 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-XGBoost-XGBoost-XGBoost: Weighted F1 = 0.7173\n",
            "  Hybrid-XGBoost-XGBoost-LightGBM: Weighted F1 = 0.7175\n",
            "  Hybrid-XGBoost-XGBoost-SVM-rbf: Weighted F1 = 0.7167\n",
            "  Hybrid-XGBoost-XGBoost-RandomForest: Weighted F1 = 0.7169\n",
            "  Hybrid-XGBoost-LightGBM-XGBoost: Weighted F1 = 0.7219\n",
            "  Hybrid-XGBoost-LightGBM-LightGBM: Weighted F1 = 0.7221\n",
            "  Hybrid-XGBoost-LightGBM-SVM-rbf: Weighted F1 = 0.7213\n",
            "  Hybrid-XGBoost-LightGBM-RandomForest: Weighted F1 = 0.7215\n",
            "  Hybrid-XGBoost-SVM-rbf-XGBoost: Weighted F1 = 0.7250\n",
            "  Hybrid-XGBoost-SVM-rbf-LightGBM: Weighted F1 = 0.7253\n",
            "  Hybrid-XGBoost-SVM-rbf-SVM-rbf: Weighted F1 = 0.7244\n",
            "  Hybrid-XGBoost-SVM-rbf-RandomForest: Weighted F1 = 0.7246\n",
            "  Hybrid-XGBoost-RandomForest-XGBoost: Weighted F1 = 0.7191\n",
            "  Hybrid-XGBoost-RandomForest-LightGBM: Weighted F1 = 0.7194\n",
            "  Hybrid-XGBoost-RandomForest-SVM-rbf: Weighted F1 = 0.7186\n",
            "  Hybrid-XGBoost-RandomForest-RandomForest: Weighted F1 = 0.7188\n",
            "  Hybrid-RandomForest-XGBoost-XGBoost: Weighted F1 = 0.7140\n",
            "  Hybrid-RandomForest-XGBoost-LightGBM: Weighted F1 = 0.7140\n",
            "  Hybrid-RandomForest-XGBoost-SVM-rbf: Weighted F1 = 0.7134\n",
            "  Hybrid-RandomForest-XGBoost-RandomForest: Weighted F1 = 0.7138\n",
            "  Hybrid-RandomForest-LightGBM-XGBoost: Weighted F1 = 0.7190\n",
            "  Hybrid-RandomForest-LightGBM-LightGBM: Weighted F1 = 0.7190\n",
            "  Hybrid-RandomForest-LightGBM-SVM-rbf: Weighted F1 = 0.7185\n",
            "  Hybrid-RandomForest-LightGBM-RandomForest: Weighted F1 = 0.7189\n",
            "  Hybrid-RandomForest-SVM-rbf-XGBoost: Weighted F1 = 0.7221\n",
            "  Hybrid-RandomForest-SVM-rbf-LightGBM: Weighted F1 = 0.7221\n",
            "  Hybrid-RandomForest-SVM-rbf-SVM-rbf: Weighted F1 = 0.7215\n",
            "  Hybrid-RandomForest-SVM-rbf-RandomForest: Weighted F1 = 0.7219\n",
            "  Hybrid-RandomForest-RandomForest-XGBoost: Weighted F1 = 0.7160\n",
            "  Hybrid-RandomForest-RandomForest-LightGBM: Weighted F1 = 0.7160\n",
            "  Hybrid-RandomForest-RandomForest-SVM-rbf: Weighted F1 = 0.7154\n",
            "  Hybrid-RandomForest-RandomForest-RandomForest: Weighted F1 = 0.7158\n",
            "Fold 2 completed: 32 combinations tested\n",
            "\n",
            "---- Fold 3 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-XGBoost-XGBoost-XGBoost: Weighted F1 = 0.7127\n",
            "  Hybrid-XGBoost-XGBoost-LightGBM: Weighted F1 = 0.7127\n",
            "  Hybrid-XGBoost-XGBoost-SVM-rbf: Weighted F1 = 0.7128\n",
            "  Hybrid-XGBoost-XGBoost-RandomForest: Weighted F1 = 0.7133\n",
            "  Hybrid-XGBoost-LightGBM-XGBoost: Weighted F1 = 0.7137\n",
            "  Hybrid-XGBoost-LightGBM-LightGBM: Weighted F1 = 0.7137\n",
            "  Hybrid-XGBoost-LightGBM-SVM-rbf: Weighted F1 = 0.7138\n",
            "  Hybrid-XGBoost-LightGBM-RandomForest: Weighted F1 = 0.7143\n",
            "  Hybrid-XGBoost-SVM-rbf-XGBoost: Weighted F1 = 0.7222\n",
            "  Hybrid-XGBoost-SVM-rbf-LightGBM: Weighted F1 = 0.7222\n",
            "  Hybrid-XGBoost-SVM-rbf-SVM-rbf: Weighted F1 = 0.7223\n",
            "  Hybrid-XGBoost-SVM-rbf-RandomForest: Weighted F1 = 0.7228\n",
            "  Hybrid-XGBoost-RandomForest-XGBoost: Weighted F1 = 0.7073\n",
            "  Hybrid-XGBoost-RandomForest-LightGBM: Weighted F1 = 0.7073\n",
            "  Hybrid-XGBoost-RandomForest-SVM-rbf: Weighted F1 = 0.7074\n",
            "  Hybrid-XGBoost-RandomForest-RandomForest: Weighted F1 = 0.7078\n",
            "  Hybrid-RandomForest-XGBoost-XGBoost: Weighted F1 = 0.7089\n",
            "  Hybrid-RandomForest-XGBoost-LightGBM: Weighted F1 = 0.7089\n",
            "  Hybrid-RandomForest-XGBoost-SVM-rbf: Weighted F1 = 0.7086\n",
            "  Hybrid-RandomForest-XGBoost-RandomForest: Weighted F1 = 0.7088\n",
            "  Hybrid-RandomForest-LightGBM-XGBoost: Weighted F1 = 0.7105\n",
            "  Hybrid-RandomForest-LightGBM-LightGBM: Weighted F1 = 0.7105\n",
            "  Hybrid-RandomForest-LightGBM-SVM-rbf: Weighted F1 = 0.7102\n",
            "  Hybrid-RandomForest-LightGBM-RandomForest: Weighted F1 = 0.7104\n",
            "  Hybrid-RandomForest-SVM-rbf-XGBoost: Weighted F1 = 0.7167\n",
            "  Hybrid-RandomForest-SVM-rbf-LightGBM: Weighted F1 = 0.7167\n",
            "  Hybrid-RandomForest-SVM-rbf-SVM-rbf: Weighted F1 = 0.7164\n",
            "  Hybrid-RandomForest-SVM-rbf-RandomForest: Weighted F1 = 0.7165\n",
            "  Hybrid-RandomForest-RandomForest-XGBoost: Weighted F1 = 0.7033\n",
            "  Hybrid-RandomForest-RandomForest-LightGBM: Weighted F1 = 0.7033\n",
            "  Hybrid-RandomForest-RandomForest-SVM-rbf: Weighted F1 = 0.7030\n",
            "  Hybrid-RandomForest-RandomForest-RandomForest: Weighted F1 = 0.7032\n",
            "Fold 3 completed: 32 combinations tested\n",
            "\n",
            "---- Fold 4 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-XGBoost-XGBoost-XGBoost: Weighted F1 = 0.7191\n",
            "  Hybrid-XGBoost-XGBoost-LightGBM: Weighted F1 = 0.7189\n",
            "  Hybrid-XGBoost-XGBoost-SVM-rbf: Weighted F1 = 0.7190\n",
            "  Hybrid-XGBoost-XGBoost-RandomForest: Weighted F1 = 0.7187\n",
            "  Hybrid-XGBoost-LightGBM-XGBoost: Weighted F1 = 0.7221\n",
            "  Hybrid-XGBoost-LightGBM-LightGBM: Weighted F1 = 0.7218\n",
            "  Hybrid-XGBoost-LightGBM-SVM-rbf: Weighted F1 = 0.7219\n",
            "  Hybrid-XGBoost-LightGBM-RandomForest: Weighted F1 = 0.7216\n",
            "  Hybrid-XGBoost-SVM-rbf-XGBoost: Weighted F1 = 0.7227\n",
            "  Hybrid-XGBoost-SVM-rbf-LightGBM: Weighted F1 = 0.7225\n",
            "  Hybrid-XGBoost-SVM-rbf-SVM-rbf: Weighted F1 = 0.7225\n",
            "  Hybrid-XGBoost-SVM-rbf-RandomForest: Weighted F1 = 0.7222\n",
            "  Hybrid-XGBoost-RandomForest-XGBoost: Weighted F1 = 0.7143\n",
            "  Hybrid-XGBoost-RandomForest-LightGBM: Weighted F1 = 0.7141\n",
            "  Hybrid-XGBoost-RandomForest-SVM-rbf: Weighted F1 = 0.7141\n",
            "  Hybrid-XGBoost-RandomForest-RandomForest: Weighted F1 = 0.7139\n",
            "  Hybrid-RandomForest-XGBoost-XGBoost: Weighted F1 = 0.7135\n",
            "  Hybrid-RandomForest-XGBoost-LightGBM: Weighted F1 = 0.7135\n",
            "  Hybrid-RandomForest-XGBoost-SVM-rbf: Weighted F1 = 0.7133\n",
            "  Hybrid-RandomForest-XGBoost-RandomForest: Weighted F1 = 0.7130\n",
            "  Hybrid-RandomForest-LightGBM-XGBoost: Weighted F1 = 0.7164\n",
            "  Hybrid-RandomForest-LightGBM-LightGBM: Weighted F1 = 0.7164\n",
            "  Hybrid-RandomForest-LightGBM-SVM-rbf: Weighted F1 = 0.7163\n",
            "  Hybrid-RandomForest-LightGBM-RandomForest: Weighted F1 = 0.7160\n",
            "  Hybrid-RandomForest-SVM-rbf-XGBoost: Weighted F1 = 0.7163\n",
            "  Hybrid-RandomForest-SVM-rbf-LightGBM: Weighted F1 = 0.7163\n",
            "  Hybrid-RandomForest-SVM-rbf-SVM-rbf: Weighted F1 = 0.7162\n",
            "  Hybrid-RandomForest-SVM-rbf-RandomForest: Weighted F1 = 0.7158\n",
            "  Hybrid-RandomForest-RandomForest-XGBoost: Weighted F1 = 0.7072\n",
            "  Hybrid-RandomForest-RandomForest-LightGBM: Weighted F1 = 0.7072\n",
            "  Hybrid-RandomForest-RandomForest-SVM-rbf: Weighted F1 = 0.7070\n",
            "  Hybrid-RandomForest-RandomForest-RandomForest: Weighted F1 = 0.7067\n",
            "Fold 4 completed: 32 combinations tested\n",
            "\n",
            "---- Fold 5 ----\n",
            "Loading models and label encoders for this fold...\n",
            "  Loaded internal label encoder for XGBoost\n",
            "  Loaded external label encoder for XGBoost\n",
            "  Loaded internal label encoder for LightGBM\n",
            "  Loaded external label encoder for LightGBM\n",
            "Models and encoders loaded. Testing hybrid combinations...\n",
            "  Hybrid-XGBoost-XGBoost-XGBoost: Weighted F1 = 0.7224\n",
            "  Hybrid-XGBoost-XGBoost-LightGBM: Weighted F1 = 0.7224\n",
            "  Hybrid-XGBoost-XGBoost-SVM-rbf: Weighted F1 = 0.7218\n",
            "  Hybrid-XGBoost-XGBoost-RandomForest: Weighted F1 = 0.7219\n",
            "  Hybrid-XGBoost-LightGBM-XGBoost: Weighted F1 = 0.7271\n",
            "  Hybrid-XGBoost-LightGBM-LightGBM: Weighted F1 = 0.7272\n",
            "  Hybrid-XGBoost-LightGBM-SVM-rbf: Weighted F1 = 0.7266\n",
            "  Hybrid-XGBoost-LightGBM-RandomForest: Weighted F1 = 0.7267\n",
            "  Hybrid-XGBoost-SVM-rbf-XGBoost: Weighted F1 = 0.7359\n",
            "  Hybrid-XGBoost-SVM-rbf-LightGBM: Weighted F1 = 0.7359\n",
            "  Hybrid-XGBoost-SVM-rbf-SVM-rbf: Weighted F1 = 0.7353\n",
            "  Hybrid-XGBoost-SVM-rbf-RandomForest: Weighted F1 = 0.7354\n",
            "  Hybrid-XGBoost-RandomForest-XGBoost: Weighted F1 = 0.7249\n",
            "  Hybrid-XGBoost-RandomForest-LightGBM: Weighted F1 = 0.7249\n",
            "  Hybrid-XGBoost-RandomForest-SVM-rbf: Weighted F1 = 0.7243\n",
            "  Hybrid-XGBoost-RandomForest-RandomForest: Weighted F1 = 0.7244\n",
            "  Hybrid-RandomForest-XGBoost-XGBoost: Weighted F1 = 0.7187\n",
            "  Hybrid-RandomForest-XGBoost-LightGBM: Weighted F1 = 0.7189\n",
            "  Hybrid-RandomForest-XGBoost-SVM-rbf: Weighted F1 = 0.7186\n",
            "  Hybrid-RandomForest-XGBoost-RandomForest: Weighted F1 = 0.7187\n",
            "  Hybrid-RandomForest-LightGBM-XGBoost: Weighted F1 = 0.7234\n",
            "  Hybrid-RandomForest-LightGBM-LightGBM: Weighted F1 = 0.7235\n",
            "  Hybrid-RandomForest-LightGBM-SVM-rbf: Weighted F1 = 0.7232\n",
            "  Hybrid-RandomForest-LightGBM-RandomForest: Weighted F1 = 0.7234\n",
            "  Hybrid-RandomForest-SVM-rbf-XGBoost: Weighted F1 = 0.7300\n",
            "  Hybrid-RandomForest-SVM-rbf-LightGBM: Weighted F1 = 0.7301\n",
            "  Hybrid-RandomForest-SVM-rbf-SVM-rbf: Weighted F1 = 0.7298\n",
            "  Hybrid-RandomForest-SVM-rbf-RandomForest: Weighted F1 = 0.7300\n",
            "  Hybrid-RandomForest-RandomForest-XGBoost: Weighted F1 = 0.7190\n",
            "  Hybrid-RandomForest-RandomForest-LightGBM: Weighted F1 = 0.7192\n",
            "  Hybrid-RandomForest-RandomForest-SVM-rbf: Weighted F1 = 0.7189\n",
            "  Hybrid-RandomForest-RandomForest-RandomForest: Weighted F1 = 0.7190\n",
            "Fold 5 completed: 32 combinations tested\n",
            "\n",
            "Instance completed! Results saved for: ['XGBoost', 'RandomForest']\n"
          ]
        }
      ],
      "source": [
        "# Find Hybrid LCPN Combo SEPERATED - WITH LABEL ENCODERS\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "algorithms = ['XGBoost', 'LightGBM', 'SVM-rbf', 'RandomForest']\n",
        "\n",
        "# COLAB INSTANCE A\n",
        "allowed_roots = ['XGBoost','RandomForest']\n",
        "\n",
        "# COLAB INSTANCE B\n",
        "#allowed_roots = ['LightGBM']\n",
        "\n",
        "# COLAB INSTANCE C\n",
        "#allowed_roots = ['SVM-rbf']\n",
        "\n",
        "print(f\"THIS INSTANCE WILL TEST: Root algorithms = {allowed_roots}\")\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN COMBINATION SEARCH\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"HYBRID LCPN COMBINATION SEARCH\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# For each fold, test all combinations\n",
        "fold_no = 0\n",
        "for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "    fold_no += 1\n",
        "    print(f\"\\n---- Fold {fold_no} ----\")\n",
        "\n",
        "    # Get data splits\n",
        "    X_tr = X_train_tfidf[train_idx]\n",
        "    X_val = X_train_tfidf[val_idx]\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "    y_super_tr = y_super_train[train_idx]\n",
        "    y_super_val = y_super_train[val_idx]\n",
        "\n",
        "    print(\"Loading models and label encoders for this fold...\")\n",
        "    models = {}\n",
        "    label_encoders = {}\n",
        "\n",
        "    for algo in algorithms:\n",
        "        # Load root model\n",
        "        try:\n",
        "            models[f'{algo}_root'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{algo}_root_fold{fold_no}.joblib')\n",
        "        except:\n",
        "            print(f\"  Warning: Could not load root model for {algo}\")\n",
        "            continue\n",
        "\n",
        "        # Load internal model and label encoder\n",
        "        try:\n",
        "            models[f'{algo}_internal'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{algo}_internal_fold{fold_no}.joblib')\n",
        "            # Try to load label encoder for tree-based models\n",
        "            if algo in ['XGBoost', 'LightGBM']:\n",
        "                try:\n",
        "                    label_encoders[f'{algo}_internal'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{algo}_internal_label_encoder_fold{fold_no}.joblib')\n",
        "                    print(f\"  Loaded internal label encoder for {algo}\")\n",
        "                except:\n",
        "                    print(f\"  Warning: Could not load internal label encoder for {algo}\")\n",
        "                    label_encoders[f'{algo}_internal'] = None\n",
        "        except:\n",
        "            models[f'{algo}_internal'] = None\n",
        "\n",
        "        # Load external model and label encoder\n",
        "        try:\n",
        "            models[f'{algo}_external'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{algo}_external_fold{fold_no}.joblib')\n",
        "            # Try to load label encoder for tree-based models\n",
        "            if algo in ['XGBoost', 'LightGBM']:\n",
        "                try:\n",
        "                    label_encoders[f'{algo}_external'] = joblib.load(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{algo}_external_label_encoder_fold{fold_no}.joblib')\n",
        "                    print(f\"  Loaded external label encoder for {algo}\")\n",
        "                except:\n",
        "                    print(f\"  Warning: Could not load external label encoder for {algo}\")\n",
        "                    label_encoders[f'{algo}_external'] = None\n",
        "        except:\n",
        "            models[f'{algo}_external'] = None\n",
        "\n",
        "    print(\"Models and encoders loaded. Testing hybrid combinations...\")\n",
        "\n",
        "    # Test combinations\n",
        "    combination_count = 0\n",
        "\n",
        "    for root_algo in allowed_roots:\n",
        "        for internal_algo in algorithms:\n",
        "            for external_algo in algorithms:\n",
        "                combination_count += 1\n",
        "\n",
        "                if (f'{root_algo}_root' not in models or models[f'{root_algo}_root'] is None):\n",
        "                    continue\n",
        "\n",
        "                # Get the models for this combination\n",
        "                root_model = models[f'{root_algo}_root']\n",
        "                internal_model = models[f'{internal_algo}_internal'] if f'{internal_algo}_internal' in models else None\n",
        "                external_model = models[f'{external_algo}_external'] if f'{external_algo}_external' in models else None\n",
        "\n",
        "                # Get label encoders for tree-based models\n",
        "                internal_encoder = label_encoders.get(f'{internal_algo}_internal', None)\n",
        "                external_encoder = label_encoders.get(f'{external_algo}_external', None)\n",
        "\n",
        "                # =====================================================================\n",
        "                # Hierarchical Prediction - WITH LABEL ENCODER HANDLING\n",
        "                # =====================================================================\n",
        "                # Get root predictions\n",
        "                y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "                # Build final leaf predictions\n",
        "                y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "                # Find normal leaf index\n",
        "                normal_leaf_idx = None\n",
        "                for idx, name in leaf_index_to_name.items():\n",
        "                    if name.lower() == 'normal' or name == 'Normal':\n",
        "                        normal_leaf_idx = idx\n",
        "                        break\n",
        "                if normal_leaf_idx is None:\n",
        "                    normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "                    normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "                # Apply hierarchical prediction logic\n",
        "                for i in range(len(y_super_val_pred)):\n",
        "                    pred_sup = y_super_val_pred[i]\n",
        "\n",
        "                    if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                        single_sample = X_val[i].reshape(1, -1)\n",
        "                        if internal_algo in ['XGBoost', 'LightGBM'] and internal_encoder is not None:\n",
        "                            # For tree-based models, decode the prediction back to original label\n",
        "                            pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                            y_val_final_pred[i] = internal_encoder.inverse_transform([pred_encoded])[0]\n",
        "                        else:\n",
        "                            y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "                    elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                        single_sample = X_val[i].reshape(1, -1)\n",
        "                        if external_algo in ['XGBoost', 'LightGBM'] and external_encoder is not None:\n",
        "                            # For tree-based models, decode the prediction back to original label\n",
        "                            pred_encoded = external_model.predict(single_sample)[0]\n",
        "                            y_val_final_pred[i] = external_encoder.inverse_transform([pred_encoded])[0]\n",
        "                        else:\n",
        "                            y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "                    else:\n",
        "                        if normal_leaf_idx is not None:\n",
        "                            y_val_final_pred[i] = normal_leaf_idx\n",
        "                        else:\n",
        "                            vals, counts = np.unique(y_tr, return_counts=True)\n",
        "                            y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "                # =====================================================================\n",
        "                # Calculate Metrics\n",
        "                # =====================================================================\n",
        "                p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "                p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "                hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "                # Store results\n",
        "                hybrid_name = f'Hybrid-{root_algo}-{internal_algo}-{external_algo}'\n",
        "\n",
        "                all_results.append({\n",
        "                    'Model': hybrid_name,\n",
        "                    'Root': root_algo,\n",
        "                    'Internal': internal_algo,\n",
        "                    'External': external_algo,\n",
        "                    'Fold': f'Fold_{fold_no}',\n",
        "                    'Macro_Precision': p_macro,\n",
        "                    'Macro_Recall': r_macro,\n",
        "                    'Macro_F1': f1_macro,\n",
        "                    'Weighted_Precision': p_weighted,\n",
        "                    'Weighted_Recall': r_weighted,\n",
        "                    'Weighted_F1': f1_weighted,\n",
        "                    'Hierarchical_Precision': hP,\n",
        "                    'Hierarchical_Recall': hR,\n",
        "                    'Hierarchical_F1': hF1,\n",
        "                    'Type': 'Fold',\n",
        "                    'Instance': 'A'\n",
        "                })\n",
        "\n",
        "                print(f\"  {hybrid_name}: Weighted F1 = {f1_weighted:.4f}\")\n",
        "\n",
        "    print(f\"Fold {fold_no} completed: {combination_count} combinations tested\")\n",
        "\n",
        "final_results_df = pd.DataFrame(all_results)\n",
        "final_results_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_hybrid_lcpn_instance_A.csv', index=False)\n",
        "print(f\"\\nInstance completed! Results saved for: {allowed_roots}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvM6UARzf1kP",
        "outputId": "f7a23a92-8bb6-44cb-8756-ed421fb494a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined 320 results from all instances!\n"
          ]
        }
      ],
      "source": [
        "# Load results from all instances\n",
        "results_a = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_hybrid_lcpn_instance_A.csv')\n",
        "results_b = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_hybrid_lcpn_instance_B.csv')\n",
        "results_c = pd.read_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_hybrid_lcpn_instance_C.csv')\n",
        "\n",
        "# Combine all results\n",
        "all_results_combined = pd.concat([results_a, results_b, results_c], ignore_index=True)\n",
        "\n",
        "# Filter only Fold results\n",
        "fold_results = all_results_combined[all_results_combined['Type'] == 'Fold'].copy()\n",
        "\n",
        "print(f\"Total fold evaluations: {len(fold_results)}\")\n",
        "print(f\"Unique hybrid combinations: {fold_results['Model'].nunique()}\")\n",
        "\n",
        "# Calculate mean and std for each hybrid combination across all folds\n",
        "hybrid_summary = fold_results.groupby(['Model', 'Root', 'Internal', 'External']).agg({\n",
        "    'Macro_Precision': ['mean', 'std'],\n",
        "    'Macro_Recall': ['mean', 'std'],\n",
        "    'Macro_F1': ['mean', 'std'],\n",
        "    'Weighted_Precision': ['mean', 'std'],\n",
        "    'Weighted_Recall': ['mean', 'std'],\n",
        "    'Weighted_F1': ['mean', 'std'],\n",
        "    'Hierarchical_Precision': ['mean', 'std'],\n",
        "    'Hierarchical_Recall': ['mean', 'std'],\n",
        "    'Hierarchical_F1': ['mean', 'std']\n",
        "}).round(4)\n",
        "\n",
        "# Flatten column names\n",
        "hybrid_summary.columns = ['_'.join(col).strip() for col in hybrid_summary.columns.values]\n",
        "hybrid_summary = hybrid_summary.reset_index()\n",
        "\n",
        "print(f\"\\nCombined summary for {len(hybrid_summary)} hybrid combinations\")\n",
        "\n",
        "final_combined_results = []\n",
        "\n",
        "# Add Fold results\n",
        "final_combined_results.extend(fold_results.to_dict('records'))\n",
        "\n",
        "for _, row in hybrid_summary.iterrows():\n",
        "    final_combined_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'Root': row['Root'],\n",
        "        'Internal': row['Internal'],\n",
        "        'External': row['External'],\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': row['Macro_Precision_mean'],\n",
        "        'Macro_Recall': row['Macro_Recall_mean'],\n",
        "        'Macro_F1': row['Macro_F1_mean'],\n",
        "        'Weighted_Precision': row['Weighted_Precision_mean'],\n",
        "        'Weighted_Recall': row['Weighted_Recall_mean'],\n",
        "        'Weighted_F1': row['Weighted_F1_mean'],\n",
        "        'Hierarchical_Precision': row['Hierarchical_Precision_mean'],\n",
        "        'Hierarchical_Recall': row['Hierarchical_Recall_mean'],\n",
        "        'Hierarchical_F1': row['Hierarchical_F1_mean'],\n",
        "        'Type': 'Mean',\n",
        "        'Time': '',\n",
        "        'Instance': 'Combined'\n",
        "    })\n",
        "\n",
        "    final_combined_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'Root': row['Root'],\n",
        "        'Internal': row['Internal'],\n",
        "        'External': row['External'],\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': row['Macro_Precision_std'],\n",
        "        'Macro_Recall': row['Macro_Recall_std'],\n",
        "        'Macro_F1': row['Macro_F1_std'],\n",
        "        'Weighted_Precision': row['Weighted_Precision_std'],\n",
        "        'Weighted_Recall': row['Weighted_Recall_std'],\n",
        "        'Weighted_F1': row['Weighted_F1_std'],\n",
        "        'Hierarchical_Precision': row['Hierarchical_Precision_std'],\n",
        "        'Hierarchical_Recall': row['Hierarchical_Recall_std'],\n",
        "        'Hierarchical_F1': row['Hierarchical_F1_std'],\n",
        "        'Type': 'Std',\n",
        "        'Time': '',\n",
        "        'Instance': 'Combined'\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "final_combined_df = pd.DataFrame(final_combined_results)\n",
        "final_combined_df.to_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_hybrid_lcpn_combined_final.csv', index=False)\n",
        "\n",
        "print(f\"Final combined results saved with {len(final_combined_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSqohW4c4duD",
        "outputId": "8a80bbbb-86bb-4630-b53b-83da84618b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LCPN-Hierarchical-RandomForest with TF-IDF\n",
            "============================================================\n",
            "  Processing Fold 1\n",
            "Fold 1 done. MacroF1=0.6672, WeightedF1=0.7134, HierF1=0.8654\n",
            "  Processing Fold 2\n",
            "Fold 2 done. MacroF1=0.6627, WeightedF1=0.7158, HierF1=0.8672\n",
            "  Processing Fold 3\n",
            "Fold 3 done. MacroF1=0.6595, WeightedF1=0.7032, HierF1=0.8611\n",
            "  Processing Fold 4\n",
            "Fold 4 done. MacroF1=0.6556, WeightedF1=0.7067, HierF1=0.8619\n",
            "  Processing Fold 5\n",
            "Fold 5 done. MacroF1=0.6696, WeightedF1=0.7190, HierF1=0.8681\n",
            "\n",
            "LCPN-RandomForest with TF-IDF Final Results:\n",
            "Macro F1: 0.6629 ± 0.0051\n",
            "Weighted F1: 0.7116 ± 0.0059\n",
            "Hierarchical F1: 0.8647 ± 0.0028\n",
            "Total Training Time: 2159.21 seconds\n"
          ]
        }
      ],
      "source": [
        "# Uniform LCPN ML - TF-IDF\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "ml_models = {\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'LightGBM': LGBMClassifier(random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'SVM-linear': SVC(kernel='linear', random_state=42),\n",
        "    'SVM-rbf': SVC(kernel='rbf', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# LCPN HIERARCHICAL MODELS LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for model_name, model_class in ml_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training LCPN-Hierarchical-{model_name} with TF-IDF\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Start timing for this model\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"  Processing Fold {fold_idx}\")\n",
        "\n",
        "        # Split TF-IDF data - maintaining sparse matrix format\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier\n",
        "        # =====================================================================\n",
        "        root_model = clone(model_class)\n",
        "        root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "        joblib.dump(root_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{model_name}_root_fold{fold_idx}.joblib')\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            # For tree-based models re-encode labels to be consecutive\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                internal_label_encoder = LabelEncoder()\n",
        "                y_tr_internal_hybrid_encoded = internal_label_encoder.fit_transform(y_tr_internal_hybrid)\n",
        "                joblib.dump(internal_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{model_name}_internal_label_encoder_fold{fold_idx}.joblib')\n",
        "            else:\n",
        "                internal_label_encoder = None\n",
        "                y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "            internal_model = clone(model_class)\n",
        "            internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "            joblib.dump(internal_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{model_name}_internal_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            if model_name in ['XGBoost', 'LightGBM']:\n",
        "                external_label_encoder = LabelEncoder()\n",
        "                y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "                joblib.dump(external_label_encoder, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{model_name}_external_label_encoder_fold{fold_idx}.joblib')\n",
        "            else:\n",
        "                external_label_encoder = None\n",
        "                y_tr_external_hybrid_encoded = y_tr_external_hybrid\n",
        "\n",
        "            external_model = clone(model_class)\n",
        "            external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "            joblib.dump(external_model, f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/tfidf_{model_name}_external_fold{fold_idx}.joblib')\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        # Get root predictions\n",
        "        y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and internal_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = internal_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = internal_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                if model_name in ['XGBoost', 'LightGBM'] and external_label_encoder is not None:\n",
        "                    # For tree-based models, decode the prediction back to original label\n",
        "                    pred_encoded = external_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-TFIDF-{model_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-TFIDF-{model_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-TFIDF-{model_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\nLCPN-{model_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q07spL1gmXa-",
        "outputId": "d9a6f375-87ac-4257-b81d-702b836ec2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training Hybrid-SVM-rbf-SVM-rbf-LightGBM with TF-IDF\n",
            "============================================================\n",
            "  Processing Fold 1\n",
            "[LightGBM] [Info] Number of positive: 10459, number of negative: 10459\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 76615\n",
            "[LightGBM] [Info] Number of data points in the train set: 20918, number of used features: 999\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 1 done. MacroF1=0.6951, WeightedF1=0.7394, HierF1=0.8792\n",
            "  Processing Fold 2\n",
            "[LightGBM] [Info] Number of positive: 10459, number of negative: 10459\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 77794\n",
            "[LightGBM] [Info] Number of data points in the train set: 20918, number of used features: 998\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 2 done. MacroF1=0.6902, WeightedF1=0.7345, HierF1=0.8776\n",
            "  Processing Fold 3\n",
            "[LightGBM] [Info] Number of positive: 10460, number of negative: 10460\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238286 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 76412\n",
            "[LightGBM] [Info] Number of data points in the train set: 20920, number of used features: 998\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 3 done. MacroF1=0.6937, WeightedF1=0.7327, HierF1=0.8759\n",
            "  Processing Fold 4\n",
            "[LightGBM] [Info] Number of positive: 10459, number of negative: 10459\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.348224 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 78521\n",
            "[LightGBM] [Info] Number of data points in the train set: 20918, number of used features: 999\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 4 done. MacroF1=0.6819, WeightedF1=0.7304, HierF1=0.8744\n",
            "  Processing Fold 5\n",
            "[LightGBM] [Info] Number of positive: 10459, number of negative: 10459\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 76216\n",
            "[LightGBM] [Info] Number of data points in the train set: 20918, number of used features: 998\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 5 done. MacroF1=0.7019, WeightedF1=0.7423, HierF1=0.8796\n",
            "\n",
            "Hybrid-SVM-rbf-SVM-rbf-LightGBM with TF-IDF Final Results:\n",
            "Macro F1: 0.6926 ± 0.0065\n",
            "Weighted F1: 0.7359 ± 0.0044\n",
            "Hierarchical F1: 0.8774 ± 0.0020\n",
            "Total Training Time: 14605.10 seconds\n"
          ]
        }
      ],
      "source": [
        "# Hybrid LCPN ML - TF-IDF (Best Combination: SVM-rbf + SVM-rbf + LightGBM)\n",
        "X_train_tfidf = scipy.sparse.load_npz('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_tfidf.npz')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "y_super_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_super_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "super_labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/super_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Create leaf to super mapping\n",
        "leaf_index_to_name = {i: name for i, name in enumerate(labels)}\n",
        "leaf_to_super_name = {}\n",
        "for idx, name in leaf_index_to_name.items():\n",
        "    anc = ancestor_sets[name]\n",
        "    assigned = None\n",
        "    for cand in ['Internal','External','Normal']:\n",
        "        if cand in anc:\n",
        "            assigned = cand\n",
        "            break\n",
        "    if assigned is None:\n",
        "        assigned = 'Root'\n",
        "    leaf_to_super_name[idx] = assigned\n",
        "\n",
        "# Reverse map: which leaf indices belong to Internal/External/Normal\n",
        "super_to_leaf_indices = {'Internal': [], 'External': [], 'Normal': []}\n",
        "for idx, sname in leaf_to_super_name.items():\n",
        "    if sname in super_to_leaf_indices:\n",
        "        super_to_leaf_indices[sname].append(idx)\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the best hybrid combination\n",
        "hybrid_combinations = {\n",
        "    'Hybrid-SVM-rbf-SVM-rbf-LightGBM': {\n",
        "        'root': SVC(kernel='rbf', random_state=42),\n",
        "        'internal': SVC(kernel='rbf', random_state=42),\n",
        "        'external': LGBMClassifier(random_state=42)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# =============================================================================\n",
        "# HYBRID LCPN HIERARCHICAL MODELS LOOP - TF-IDF\n",
        "# =============================================================================\n",
        "for hybrid_name, model_classes in hybrid_combinations.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {hybrid_name} with TF-IDF\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # Metrics containers for this model\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_idx, val_idx in skf.split(X_train_tfidf, y_train):\n",
        "        fold_idx += 1\n",
        "        print(f\"  Processing Fold {fold_idx}\")\n",
        "\n",
        "        X_tr = X_train_tfidf[train_idx]\n",
        "        X_val = X_train_tfidf[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        y_super_tr = y_super_train[train_idx]\n",
        "        y_super_val = y_super_train[val_idx]\n",
        "\n",
        "        # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        y_super_tr_hybrid = []\n",
        "        for leaf_label in y_tr_hybrid:\n",
        "            super_label = leaf_to_super_name[leaf_label]\n",
        "            super_idx = list(super_labels).index(super_label)\n",
        "            y_super_tr_hybrid.append(super_idx)\n",
        "        y_super_tr_hybrid = np.array(y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 1) Train ROOT level classifier (SVM-rbf)\n",
        "        # =====================================================================\n",
        "        root_model = clone(model_classes['root'])\n",
        "        root_model.fit(X_tr_hybrid, y_super_tr_hybrid)\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2) Train INTERNAL level classifier (SVM-rbf)\n",
        "        # =====================================================================\n",
        "        internal_mask_tr = (y_super_tr_hybrid == list(super_labels).index('Internal'))\n",
        "        if internal_mask_tr.sum() > 0:\n",
        "            X_tr_internal = X_tr_hybrid[internal_mask_tr]\n",
        "            y_tr_internal = y_tr_hybrid[internal_mask_tr]\n",
        "\n",
        "            resample_internal = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_internal_hybrid, y_tr_internal_hybrid = resample_internal.fit_resample(X_tr_internal, y_tr_internal)\n",
        "\n",
        "            internal_label_encoder = None\n",
        "            y_tr_internal_hybrid_encoded = y_tr_internal_hybrid\n",
        "\n",
        "            internal_model = clone(model_classes['internal'])\n",
        "            internal_model.fit(X_tr_internal_hybrid, y_tr_internal_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No internal samples for training\")\n",
        "            internal_model = None\n",
        "            internal_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 3) Train EXTERNAL level classifier (LightGBM)\n",
        "        # =====================================================================\n",
        "        external_mask_tr = (y_super_tr_hybrid == list(super_labels).index('External'))\n",
        "        if external_mask_tr.sum() > 0:\n",
        "            X_tr_external = X_tr_hybrid[external_mask_tr]\n",
        "            y_tr_external = y_tr_hybrid[external_mask_tr]\n",
        "\n",
        "            resample_external = ImbPipeline([\n",
        "                ('ros', RandomOverSampler(random_state=42)),\n",
        "                ('rus', RandomUnderSampler(random_state=42))\n",
        "            ])\n",
        "            X_tr_external_hybrid, y_tr_external_hybrid = resample_external.fit_resample(X_tr_external, y_tr_external)\n",
        "\n",
        "            external_label_encoder = LabelEncoder()\n",
        "            y_tr_external_hybrid_encoded = external_label_encoder.fit_transform(y_tr_external_hybrid)\n",
        "\n",
        "            external_model = clone(model_classes['external'])\n",
        "            external_model.fit(X_tr_external_hybrid, y_tr_external_hybrid_encoded)\n",
        "        else:\n",
        "            print(f\"  No external samples for training\")\n",
        "            external_model = None\n",
        "            external_label_encoder = None\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4) Hierarchical Prediction\n",
        "        # =====================================================================\n",
        "        # Get root predictions\n",
        "        y_super_val_pred = root_model.predict(X_val)\n",
        "\n",
        "        # Build final leaf predictions\n",
        "        y_val_final_pred = np.zeros_like(y_super_val_pred)\n",
        "\n",
        "        # Find normal leaf index\n",
        "        normal_leaf_idx = None\n",
        "        for idx, name in leaf_index_to_name.items():\n",
        "            if name.lower() == 'normal' or name == 'Normal':\n",
        "                normal_leaf_idx = idx\n",
        "                break\n",
        "        if normal_leaf_idx is None:\n",
        "            normal_leaf_list = super_to_leaf_indices.get('Normal', [])\n",
        "            normal_leaf_idx = normal_leaf_list[0] if len(normal_leaf_list) > 0 else None\n",
        "\n",
        "        # Apply hierarchical prediction logic\n",
        "        for i in range(len(y_super_val_pred)):\n",
        "            pred_sup = y_super_val_pred[i]\n",
        "\n",
        "            if pred_sup == list(super_labels).index('Internal') and internal_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                # SVM doesn't need label encoding\n",
        "                y_val_final_pred[i] = internal_model.predict(single_sample)[0]\n",
        "\n",
        "            elif pred_sup == list(super_labels).index('External') and external_model is not None:\n",
        "                single_sample = X_val[i].reshape(1, -1)\n",
        "                # LightGBM needs label encoding handling\n",
        "                if external_label_encoder is not None:\n",
        "                    pred_encoded = external_model.predict(single_sample)[0]\n",
        "                    y_val_final_pred[i] = external_label_encoder.inverse_transform([pred_encoded])[0]\n",
        "                else:\n",
        "                    y_val_final_pred[i] = external_model.predict(single_sample)[0]\n",
        "\n",
        "            else:\n",
        "                if normal_leaf_idx is not None:\n",
        "                    y_val_final_pred[i] = normal_leaf_idx\n",
        "                else:\n",
        "                    vals, counts = np.unique(y_tr_hybrid, return_counts=True)\n",
        "                    y_val_final_pred[i] = vals[np.argmax(counts)]\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5) Calculate Metrics\n",
        "        # =====================================================================\n",
        "        p_macro = precision_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_val_final_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_val_final_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_val_final_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_val_final_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'LCPN-TFIDF-{hybrid_name}',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    # Calculate total time for this model\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    # Calculate and save final averages for this model\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-TFIDF-{hybrid_name}',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'LCPN-TFIDF-{hybrid_name}',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{hybrid_name} with TF-IDF Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA8pWUfWU6sV"
      },
      "source": [
        "# Printing Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk_RkJVFUMF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7361eea-dbbc-491d-f9ff-cf6c56866188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/drive/MyDrive/Skripsi Dataset/FinalFile/final_model_fold_results.xlsx\n",
            "Total rows: 455\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "metrics = ['Macro_Precision', 'Macro_Recall', 'Macro_F1',\n",
        "              'Weighted_Precision', 'Weighted_Recall', 'Weighted_F1',\n",
        "              'Hierarchical_Precision', 'Hierarchical_Recall', 'Hierarchical_F1']\n",
        "\n",
        "column_order = ['Model', 'Fold', 'Type',\n",
        "                'Macro_Precision', 'Macro_Recall', 'Macro_F1',\n",
        "                'Weighted_Precision', 'Weighted_Recall', 'Weighted_F1',\n",
        "                'Hierarchical_Precision', 'Hierarchical_Recall', 'Hierarchical_F1','Time']\n",
        "\n",
        "# Apply formatting based on Type\n",
        "for index, row in results_df.iterrows():\n",
        "    if row['Type'] == 'Mean':\n",
        "        for col in metrics:\n",
        "            if pd.notna(row[col]):\n",
        "                results_df.at[index, col] = f\"{row[col]:.4f}\"\n",
        "    elif row['Type'] == 'Std':\n",
        "        for col in metrics:\n",
        "            if pd.notna(row[col]):\n",
        "                results_df.at[index, col] = f\"{row[col]:.3f}\"\n",
        "    else:\n",
        "        for col in metrics:\n",
        "            if pd.notna(row[col]):\n",
        "                results_df.at[index, col] = f\"{row[col]:.4f}\"\n",
        "\n",
        "results_df = results_df[column_order]\n",
        "excel_path = '/content/drive/MyDrive/Skripsi Dataset/FinalFile/final_model_fold_results.xlsx'\n",
        "if os.path.exists(excel_path):\n",
        "    old = pd.read_excel(excel_path)\n",
        "    results_df = pd.concat([old, results_df], ignore_index=True)\n",
        "results_df.to_excel(excel_path, index=False)\n",
        "print(f\"Results saved to: {excel_path}\")\n",
        "print(f\"Total rows: {len(results_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBomE1pjdwcw"
      },
      "source": [
        "# Printing Hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlMPZriKdyh2",
        "outputId": "bf5bd455-22a9-4ce5-aca0-abf9d6a1fec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved!\n",
            "Results saved to: /content/drive/MyDrive/Skripsi Dataset/FinalFile/lcpn_all_algorithms_level_performance.xlsx\n",
            "Total rows: 104\n",
            "\n",
            "Sample of results:\n",
            "        Algorithm     Level  Macro_Precision_Mean  Macro_Precision_Std  \\\n",
            "0     XGBoost-W2V      Root              0.847341             0.005979   \n",
            "1     XGBoost-W2V  Internal              0.401753             0.003835   \n",
            "2     XGBoost-W2V  External              0.270954             0.004046   \n",
            "3     XGBoost-W2V   Overall              0.676760             0.006574   \n",
            "4    LightGBM-W2V      Root              0.809560             0.003965   \n",
            "5    LightGBM-W2V  Internal              0.385615             0.005760   \n",
            "6    LightGBM-W2V  External              0.263203             0.007346   \n",
            "7    LightGBM-W2V   Overall              0.624069             0.008911   \n",
            "8  SVM-linear-W2V      Root              0.705723             0.002719   \n",
            "9  SVM-linear-W2V  Internal              0.357135             0.005063   \n",
            "\n",
            "   Macro_Recall_Mean  Macro_Recall_Std  Macro_F1_Mean  Macro_F1_Std  \\\n",
            "0           0.809149          0.006503       0.826375      0.006087   \n",
            "1           0.366345          0.004161       0.382276      0.004157   \n",
            "2           0.159742          0.003707       0.200386      0.003134   \n",
            "3           0.651254          0.006689       0.661962      0.006659   \n",
            "4           0.812199          0.006301       0.810034      0.004293   \n",
            "5           0.374816          0.005958       0.378630      0.005880   \n",
            "6           0.173679          0.003382       0.209108      0.003937   \n",
            "7           0.669607          0.008769       0.642422      0.008594   \n",
            "8           0.728429          0.004719       0.690705      0.004482   \n",
            "9           0.355716          0.005941       0.348963      0.005115   \n",
            "\n",
            "   Weighted_Precision_Mean  Weighted_Precision_Std  Weighted_Recall_Mean  \\\n",
            "0                 0.891818                0.004323              0.893698   \n",
            "1                 0.693847                0.005972              0.648810   \n",
            "2                 0.962603                0.009575              0.594374   \n",
            "3                 0.719129                0.006921              0.715357   \n",
            "4                 0.881063                0.002772              0.878963   \n",
            "5                 0.688806                0.009247              0.628800   \n",
            "6                 0.941142                0.018419              0.630366   \n",
            "7                 0.708931                0.006449              0.696849   \n",
            "8                 0.827814                0.002511              0.788084   \n",
            "9                 0.675511                0.008354              0.576964   \n",
            "\n",
            "   Weighted_Recall_Std  Weighted_F1_Mean  Weighted_F1_Std  \\\n",
            "0             0.004338          0.892104         0.004337   \n",
            "1             0.007489          0.669788         0.006914   \n",
            "2             0.017388          0.733221         0.013061   \n",
            "3             0.006929          0.716547         0.006999   \n",
            "4             0.002775          0.879432         0.002856   \n",
            "5             0.008693          0.653924         0.008937   \n",
            "6             0.011756          0.754598         0.011565   \n",
            "7             0.006689          0.700018         0.006581   \n",
            "8             0.003935          0.794864         0.004065   \n",
            "9             0.006793          0.610044         0.007530   \n",
            "\n",
            "   Hierarchical_Precision_Mean  Hierarchical_Precision_Std  \\\n",
            "0                     0.946849                    0.002169   \n",
            "1                     0.873140                    0.002805   \n",
            "2                     0.744278                    0.011717   \n",
            "3                     0.865275                    0.003670   \n",
            "4                     0.939481                    0.001388   \n",
            "5                     0.860134                    0.003092   \n",
            "6                     0.772643                    0.007680   \n",
            "7                     0.852721                    0.002906   \n",
            "8                     0.894042                    0.001968   \n",
            "9                     0.827399                    0.002306   \n",
            "\n",
            "   Hierarchical_Recall_Mean  Hierarchical_Recall_Std  Hierarchical_F1_Mean  \\\n",
            "0                  0.946849                 0.002169              0.946849   \n",
            "1                  0.861438                 0.003538              0.867249   \n",
            "2                  0.736717                 0.011805              0.740478   \n",
            "3                  0.868937                 0.003414              0.867102   \n",
            "4                  0.939481                 0.001388              0.939481   \n",
            "5                  0.849445                 0.003463              0.854756   \n",
            "6                  0.766547                 0.007621              0.769583   \n",
            "7                  0.859851                 0.002668              0.856271   \n",
            "8                  0.894042                 0.001968              0.894042   \n",
            "9                  0.820531                 0.002212              0.823951   \n",
            "\n",
            "   Hierarchical_F1_Std  \n",
            "0             0.002169  \n",
            "1             0.003172  \n",
            "2             0.011746  \n",
            "3             0.003523  \n",
            "4             0.001388  \n",
            "5             0.003278  \n",
            "6             0.007648  \n",
            "7             0.002753  \n",
            "8             0.001968  \n",
            "9             0.002253  \n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Save to Excel\n",
        "excel_path = '/content/drive/MyDrive/Skripsi Dataset/FinalFile/lcpn_all_algorithms_level_performance.xlsx'\n",
        "if os.path.exists(excel_path):\n",
        "    old = pd.read_excel(excel_path)\n",
        "    results_df = pd.concat([old, results_df], ignore_index=True)\n",
        "\n",
        "results_df.to_excel(excel_path, index=False)\n",
        "print(f\"Results saved!\")\n",
        "\n",
        "results_df.to_excel(excel_path, index=False)\n",
        "print(f\"Results saved to: {excel_path}\")\n",
        "print(f\"Total rows: {len(results_df)}\")\n",
        "\n",
        "print(f\"\\nSample of results:\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Best Model"
      ],
      "metadata": {
        "id": "a1cKfGlIu6GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STACKING ENSEMBLES WORD2VEC with Test Set Prediction, Analysis and ROC Curves\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "import time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Load training data\n",
        "X_train_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v.npy')\n",
        "X_train_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_train_w2v_sequences.npy')\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/w2v_embedding_matrix.npy')\n",
        "y_train = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_train.pkl')\n",
        "labels = pickle.load(open(\"/content/drive/MyDrive/Skripsi Dataset/FinalFile/status_label_encoder.pkl\",'rb'))['classes']\n",
        "\n",
        "# Load test data\n",
        "X_test_w2v = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_w2v.npy')\n",
        "X_test_sequences = np.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_w2v_sequences.npy')\n",
        "y_test = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/y_test.pkl')\n",
        "\n",
        "# Load text data for analysis\n",
        "X_test_text = joblib.load('/content/drive/MyDrive/Skripsi Dataset/FinalFile/X_test_statement.pkl')\n",
        "\n",
        "# Get GRU dimensions\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "max_sequence_length = X_train_sequences.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "test_predictions = {}\n",
        "\n",
        "stacking_ensembles = {\n",
        "    'Stacking-XGB-GRU-RF-LR': {\n",
        "        'base_models': ['XGBoost', 'GRU', 'RandomForest'],\n",
        "        'meta_model': LogisticRegression(random_state=42)\n",
        "    }\n",
        "}\n",
        "\n",
        "# STACKING ENSEMBLES LOOP\n",
        "for ensemble_name, ensemble_config in stacking_ensembles.items():\n",
        "    base_model_names = ensemble_config['base_models']\n",
        "    meta_model = ensemble_config['meta_model']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {ensemble_name}\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    print(\"Stage 1: Generating Out-of-Fold predictions...\")\n",
        "\n",
        "    # Initialize OOF arrays for base model predictions\n",
        "    oof_predictions = {}\n",
        "    for model_name in base_model_names:\n",
        "        oof_predictions[model_name] = np.zeros((len(X_train_w2v), num_classes))\n",
        "\n",
        "    # Generate OOF predictions for each base model\n",
        "    for train_idx, val_idx in skf.split(X_train_w2v, y_train):\n",
        "        X_tr_ft = X_train_w2v[train_idx]\n",
        "        X_val_ft = X_train_w2v[val_idx]\n",
        "        X_tr_seq = X_train_sequences[train_idx]\n",
        "        X_val_seq = X_train_sequences[val_idx]\n",
        "        y_tr = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "         # SINGLE RESAMPLE PIPE\n",
        "        resample_pipe = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "\n",
        "        X_tr_ft_hybrid, y_tr_hybrid = resample_pipe.fit_resample(X_tr_ft, y_tr)\n",
        "        X_tr_seq_hybrid, y_tr_seq_hybrid = resample_pipe.fit_resample(X_tr_seq, y_tr)\n",
        "\n",
        "        # Train each base model and get OOF predictions\n",
        "        for model_name in base_model_names:\n",
        "            if model_name == 'GRU':\n",
        "                # GRU Model\n",
        "                y_tr_cat = tf.keras.utils.to_categorical(y_tr_seq_hybrid, num_classes)\n",
        "                y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "                gru_model = Sequential([\n",
        "                    Embedding(\n",
        "                        input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_sequence_length,\n",
        "                        trainable=False\n",
        "                    ),\n",
        "                    GRU(64),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "                gru_model.fit(\n",
        "                    X_tr_seq_hybrid, y_tr_cat,\n",
        "                    validation_data=(X_val_seq, y_val_cat),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                y_proba_gru = gru_model.predict(X_val_seq, verbose=0)\n",
        "                oof_predictions['GRU'][val_idx] = y_proba_gru\n",
        "\n",
        "            else:\n",
        "                # ML Models\n",
        "                if model_name == 'XGBoost':\n",
        "                    model = XGBClassifier(random_state=42)\n",
        "                elif model_name == 'RandomForest':\n",
        "                    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "                model.fit(X_tr_ft_hybrid, y_tr_hybrid)\n",
        "\n",
        "                y_proba_ml = model.predict_proba(X_val_ft)\n",
        "                oof_predictions[model_name][val_idx] = y_proba_ml\n",
        "\n",
        "    print(\"Stage 1 completed: OOF predictions generated for all base models\")\n",
        "    print(\"Stage 2: Training and evaluating meta-classifier...\")\n",
        "\n",
        "    # Create meta-features dataset from OOF predictions\n",
        "    X_meta_train = np.hstack([oof_predictions[model_name] for model_name in base_model_names])\n",
        "    y_meta_train = y_train\n",
        "\n",
        "    # Metrics containers for stacking ensemble\n",
        "    p_macros=[]; r_macros=[]; f1_macros=[]\n",
        "    p_weights=[]; r_weights=[]; f1_weights=[]\n",
        "    hF1s=[]; hPs=[]; hRs=[]\n",
        "    all_y_true = []; all_y_pred = []\n",
        "\n",
        "    fold_idx = 0\n",
        "    # Cross-validation on meta-features\n",
        "    for train_idx, val_idx in skf.split(X_meta_train, y_meta_train):\n",
        "        fold_idx += 1\n",
        "\n",
        "        # Get meta-features splits\n",
        "        X_tr_meta = X_meta_train[train_idx]\n",
        "        X_val_meta = X_meta_train[val_idx]\n",
        "        y_tr = y_meta_train[train_idx]\n",
        "        y_val = y_meta_train[val_idx]\n",
        "\n",
        "        resample_pipe_meta = ImbPipeline([\n",
        "            ('ros', RandomOverSampler(random_state=42)),\n",
        "            ('rus', RandomUnderSampler(random_state=42))\n",
        "        ])\n",
        "        X_tr_meta_hybrid, y_tr_hybrid = resample_pipe_meta.fit_resample(X_tr_meta, y_tr)\n",
        "\n",
        "        # Train meta-classifier\n",
        "        meta_model_clone = clone(meta_model)\n",
        "        meta_model_clone.fit(X_tr_meta_hybrid, y_tr_hybrid)\n",
        "\n",
        "        y_pred = meta_model_clone.predict(X_val_meta)\n",
        "\n",
        "        # Calculate metrics\n",
        "        p_macro = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        r_macro = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        p_weighted = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        r_weighted = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "        hF1, hP, hR, _, _, _ = hierarchical_metrics_journal(y_val, y_pred, labels, ancestor_sets)\n",
        "\n",
        "        # Store fold results\n",
        "        p_macros.append(p_macro)\n",
        "        r_macros.append(r_macro)\n",
        "        f1_macros.append(f1_macro)\n",
        "        p_weights.append(p_weighted)\n",
        "        r_weights.append(r_weighted)\n",
        "        f1_weights.append(f1_weighted)\n",
        "        hF1s.append(hF1)\n",
        "        hPs.append(hP)\n",
        "        hRs.append(hR)\n",
        "\n",
        "        all_y_true.extend(list(y_val))\n",
        "        all_y_pred.extend(list(y_pred))\n",
        "\n",
        "        # Save individual fold result\n",
        "        all_results.append({\n",
        "            'Model': f'Flat-w2v-{ensemble_name}-skipgram',\n",
        "            'Fold': f'Fold_{fold_idx}',\n",
        "            'Macro_Precision': p_macro,\n",
        "            'Macro_Recall': r_macro,\n",
        "            'Macro_F1': f1_macro,\n",
        "            'Weighted_Precision': p_weighted,\n",
        "            'Weighted_Recall': r_weighted,\n",
        "            'Weighted_F1': f1_weighted,\n",
        "            'Hierarchical_Precision': hP,\n",
        "            'Hierarchical_Recall': hR,\n",
        "            'Hierarchical_F1': hF1,\n",
        "            'Type': 'Fold',\n",
        "            'Time': ''\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold_idx} done. MacroF1={f1_macro:.4f}, WeightedF1={f1_weighted:.4f}, HierF1={hF1:.4f}\")\n",
        "\n",
        "    model_total_time = time.time() - model_start_time\n",
        "\n",
        "    print(f\"\\nTraining final stacking model for test set prediction...\")\n",
        "    print(\"Training base models on full training data...\")\n",
        "    base_models_full = {}\n",
        "\n",
        "    resample_pipe_full = ImbPipeline([\n",
        "        ('ros', RandomOverSampler(random_state=42)),\n",
        "        ('rus', RandomUnderSampler(random_state=42))\n",
        "    ])\n",
        "\n",
        "    X_train_ft_full, y_train_full = resample_pipe_full.fit_resample(X_train_w2v, y_train)\n",
        "    X_train_seq_full, _ = resample_pipe_full.fit_resample(X_train_sequences, y_train)\n",
        "\n",
        "    for model_name in base_model_names:\n",
        "        if model_name == 'GRU':\n",
        "            # GRU Model\n",
        "            y_train_cat = tf.keras.utils.to_categorical(y_train_full, num_classes)\n",
        "\n",
        "            gru_model_full = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length,\n",
        "                    trainable=False\n",
        "                ),\n",
        "                GRU(64),\n",
        "                Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            gru_model_full.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            gru_model_full.fit(\n",
        "                X_train_seq_full, y_train_cat,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)],\n",
        "                verbose=0,\n",
        "            )\n",
        "            base_models_full['GRU'] = gru_model_full\n",
        "\n",
        "        else:\n",
        "            # ML Models\n",
        "            if model_name == 'XGBoost':\n",
        "                model_full = XGBClassifier(random_state=42)\n",
        "            elif model_name == 'RandomForest':\n",
        "                model_full = RandomForestClassifier(random_state=42)\n",
        "\n",
        "            model_full.fit(X_train_ft_full, y_train_full)\n",
        "            base_models_full[model_name] = model_full\n",
        "\n",
        "    # Generate test predictions from base models\n",
        "    print(\"Generating test predictions from base models...\")\n",
        "    meta_features_test = []\n",
        "    for model_name in base_model_names:\n",
        "        if model_name == 'GRU':\n",
        "            y_proba_test = base_models_full['GRU'].predict(X_test_sequences, verbose=0)\n",
        "        else:\n",
        "            y_proba_test = base_models_full[model_name].predict_proba(X_test_w2v)\n",
        "        meta_features_test.append(y_proba_test)\n",
        "\n",
        "    X_meta_test = np.hstack(meta_features_test)\n",
        "\n",
        "    # Train final meta-classifier on ALL OOF meta-features\n",
        "    X_meta_full_hybrid, y_meta_full_hybrid = resample_pipe_full.fit_resample(X_meta_train, y_meta_train)\n",
        "    final_meta_model = clone(meta_model)\n",
        "    final_meta_model.fit(X_meta_full_hybrid, y_meta_full_hybrid)\n",
        "\n",
        "    # Final prediction on test set\n",
        "    y_test_pred = final_meta_model.predict(X_meta_test)\n",
        "    y_test_proba = final_meta_model.predict_proba(X_meta_test)\n",
        "\n",
        "    # Store test predictions for analysis\n",
        "    test_predictions[ensemble_name] = {\n",
        "        'true': y_test,\n",
        "        'pred': y_test_pred,\n",
        "        'proba': y_test_proba,\n",
        "        'meta_features': X_meta_test\n",
        "    }\n",
        "\n",
        "    # Calculate and save final average\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-w2v-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Mean',\n",
        "        'Macro_Precision': np.mean(p_macros),\n",
        "        'Macro_Recall': np.mean(r_macros),\n",
        "        'Macro_F1': np.mean(f1_macros),\n",
        "        'Weighted_Precision': np.mean(p_weights),\n",
        "        'Weighted_Recall': np.mean(r_weights),\n",
        "        'Weighted_F1': np.mean(f1_weights),\n",
        "        'Hierarchical_Precision': np.mean(hPs),\n",
        "        'Hierarchical_Recall': np.mean(hRs),\n",
        "        'Hierarchical_F1': np.mean(hF1s),\n",
        "        'Type': 'Mean',\n",
        "        'Time': model_total_time\n",
        "    })\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': f'Flat-w2v-{ensemble_name}-skipgram',\n",
        "        'Fold': 'Std',\n",
        "        'Macro_Precision': np.std(p_macros),\n",
        "        'Macro_Recall': np.std(r_macros),\n",
        "        'Macro_F1': np.std(f1_macros),\n",
        "        'Weighted_Precision': np.std(p_weights),\n",
        "        'Weighted_Recall': np.std(r_weights),\n",
        "        'Weighted_F1': np.std(f1_weights),\n",
        "        'Hierarchical_Precision': np.std(hPs),\n",
        "        'Hierarchical_Recall': np.std(hRs),\n",
        "        'Hierarchical_F1': np.std(hF1s),\n",
        "        'Type': 'Std',\n",
        "        'Time': ''\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{ensemble_name} Final Results:\")\n",
        "    print(f\"Macro F1: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
        "    print(f\"Weighted F1: {np.mean(f1_weights):.4f} ± {np.std(f1_weights):.4f}\")\n",
        "    print(f\"Hierarchical F1: {np.mean(hF1s):.4f} ± {np.std(hF1s):.4f}\")\n",
        "    print(f\"Total Training Time: {model_total_time:.2f} seconds\")\n",
        "    print(f\"Base Models: {base_model_names}\")\n",
        "    print(f\"Meta Model: {type(meta_model).__name__}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSIS 1: CONFUSION PAIR ANALYSIS\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS 1: CONFUSION PAIR ANALYSIS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for ensemble_name, predictions in test_predictions.items():\n",
        "    y_true = predictions['true']\n",
        "    y_pred = predictions['pred']\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Confusion Matrix - {ensemble_name}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find top 10 most confused pairs\n",
        "    confused_pairs = []\n",
        "    n_classes = len(labels)\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        for j in range(n_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                true_class = labels[i]\n",
        "                pred_class = labels[j]\n",
        "                count = cm[i, j]\n",
        "                confused_pairs.append((true_class, pred_class, count))\n",
        "\n",
        "    # Sort by count in descending order and get top 10\n",
        "    confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    top_confused = confused_pairs[:10]\n",
        "\n",
        "    print(f\"\\nTop 10 Most Confused Pairs - {ensemble_name}:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'True Class':<20} {'Predicted Class':<20} {'Count':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "    for true_class, pred_class, count in top_confused:\n",
        "        print(f\"{true_class:<20} {pred_class:<20} {count:<10}\")\n",
        "\n",
        "    # Save confused pairs to CSV\n",
        "    confused_df = pd.DataFrame(top_confused, columns=['True_Class', 'Predicted_Class', 'Count'])\n",
        "    confused_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{ensemble_name}_confused_pairs.csv', index=False)\n",
        "\n",
        "# ANALYSIS 2: F1 vs SUPPORT CORRELATION\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS 2: F1 vs SUPPORT CORRELATION\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for ensemble_name, predictions in test_predictions.items():\n",
        "    y_true = predictions['true']\n",
        "    y_pred = predictions['pred']\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=range(len(labels)), zero_division=0\n",
        "    )\n",
        "\n",
        "    # Create DataFrame for per-class metrics\n",
        "    class_metrics = pd.DataFrame({\n",
        "        'Class': labels,\n",
        "        'Precision': precision_per_class,\n",
        "        'Recall': recall_per_class,\n",
        "        'F1_Score': f1_per_class,\n",
        "        'Support': support_per_class\n",
        "    })\n",
        "\n",
        "    print(f\"\\nPer-Class Metrics - {ensemble_name}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Class':<20} {'Precision':<10} {'Recall':<10} {'F1_Score':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "    for _, row in class_metrics.iterrows():\n",
        "        print(f\"{row['Class']:<20} {row['Precision']:.4f}    {row['Recall']:.4f}    {row['F1_Score']:.4f}    {row['Support']:<10}\")\n",
        "\n",
        "    # Create scatter plot: F1 Score vs Support\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(class_metrics['Support'], class_metrics['F1_Score'], alpha=0.7, s=100)\n",
        "\n",
        "    # Add class labels to points\n",
        "    for i, row in class_metrics.iterrows():\n",
        "        plt.annotate(row['Class'],\n",
        "                    (row['Support'], row['F1_Score']),\n",
        "                    xytext=(5, 5),\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=8)\n",
        "\n",
        "    plt.xlabel('Number of Samples (Support)')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title(f'F1 Score vs Support - {ensemble_name}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(class_metrics['Support'], class_metrics['F1_Score'])[0, 1]\n",
        "    print(f\"\\nCorrelation between Support and F1 Score: {correlation:.4f}\")\n",
        "\n",
        "    # Save per-class metrics to CSV\n",
        "    class_metrics.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{ensemble_name}_per_class_metrics.csv', index=False)\n",
        "\n",
        "# ANALYSIS 3: ROC CURVE ANALYSIS (PER-CLASS)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS 3: ROC CURVE ANALYSIS (PER-CLASS)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for ensemble_name, predictions in test_predictions.items():\n",
        "    y_true = predictions['true']\n",
        "    y_proba = predictions['proba']\n",
        "\n",
        "    # Binarize the output for ROC curve calculation\n",
        "    y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
        "\n",
        "    # Calculate ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Calculate micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_proba.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot micro-average ROC curve\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.4f})',\n",
        "             color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "    # Plot ROC curves for each class\n",
        "    colors = plt.cm.rainbow(np.linspace(0, 1, num_classes))\n",
        "    for i, color in zip(range(num_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                 label=f'ROC curve of class {labels[i]} (area = {roc_auc[i]:0.4f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'Multi-class ROC Curves - {ensemble_name}')\n",
        "    plt.legend(loc=\"lower right\", fontsize=8)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print AUC values for each class\n",
        "    print(f\"\\nAUC Scores - {ensemble_name}:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(num_classes):\n",
        "        print(f\"{labels[i]:<20}: {roc_auc[i]:.4f}\")\n",
        "    print(f\"{'Micro-average':<20}: {roc_auc['micro']:.4f}\")\n",
        "\n",
        "    # Save ROC data to CSV\n",
        "    roc_data = []\n",
        "    for i in range(num_classes):\n",
        "        for j in range(len(fpr[i])):\n",
        "            roc_data.append({\n",
        "                'Class': labels[i],\n",
        "                'False_Positive_Rate': fpr[i][j],\n",
        "                'True_Positive_Rate': tpr[i][j],\n",
        "                'AUC': roc_auc[i]\n",
        "            })\n",
        "\n",
        "    roc_df = pd.DataFrame(roc_data)\n",
        "    roc_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{ensemble_name}_roc_data.csv', index=False)\n",
        "\n",
        "# ANALYSIS 4: GENERATE SPREADSHEET WITH PREDICTIONS\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS 4: PREDICTION SPREADSHEET\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for ensemble_name, predictions in test_predictions.items():\n",
        "    y_true = predictions['true']\n",
        "    y_pred = predictions['pred']\n",
        "    y_proba = predictions['proba']\n",
        "\n",
        "    # Create spreadsheet with text, true labels, and predicted labels\n",
        "    results_df = pd.DataFrame({\n",
        "        'Text_Input': X_test_text,\n",
        "        'True_Label': [labels[i] for i in y_true],\n",
        "        'Predicted_Label': [labels[i] for i in y_pred]\n",
        "    })\n",
        "\n",
        "    # Add confidence scores for each prediction\n",
        "    confidence_scores = np.max(y_proba, axis=1)\n",
        "    results_df['Confidence_Score'] = confidence_scores\n",
        "\n",
        "    # Add predicted probability for each class\n",
        "    for i, class_name in enumerate(labels):\n",
        "        results_df[f'Prob_{class_name}'] = y_proba[:, i]\n",
        "\n",
        "    # whether prediction was correct\n",
        "    results_df['Correct_Prediction'] = (y_true == y_pred)\n",
        "\n",
        "    print(f\"\\nPrediction Spreadsheet Sample - {ensemble_name}:\")\n",
        "    print(\"-\" * 100)\n",
        "    print(results_df.head(10).to_string(index=False))\n",
        "\n",
        "    # Save to CSV\n",
        "    results_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{ensemble_name}_predictions.csv', index=False)\n",
        "    print(f\"\\nFull prediction spreadsheet saved: {ensemble_name}_predictions.csv\")\n",
        "\n",
        "    # Summary statistics\n",
        "    accuracy = (y_true == y_pred).mean()\n",
        "    print(f\"\\nTest Set Performance - {ensemble_name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Total Samples: {len(y_true)}\")\n",
        "    print(f\"Correct Predictions: {(y_true == y_pred).sum()}\")\n",
        "    print(f\"Incorrect Predictions: {(y_true != y_pred).sum()}\")\n",
        "\n",
        "# ANALYSIS 5: DETAILED CLASSIFICATION REPORT\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANALYSIS 5: DETAILED CLASSIFICATION REPORT\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for ensemble_name, predictions in test_predictions.items():\n",
        "    y_true = predictions['true']\n",
        "    y_pred = predictions['pred']\n",
        "\n",
        "    print(f\"\\nDetailed Classification Report - {ensemble_name}:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Get detailed classification report as dictionary\n",
        "    report = classification_report(y_true, y_pred,\n",
        "                                 target_names=labels,\n",
        "                                 output_dict=True,\n",
        "                                 zero_division=0)\n",
        "\n",
        "    # Convert to DataFrame for better display\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(report_df.round(4))\n",
        "\n",
        "    # Save detailed report\n",
        "    report_df.to_csv(f'/content/drive/MyDrive/Skripsi Dataset/FinalFile/{ensemble_name}_detailed_report.csv')\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ALL ANALYSES COMPLETED!\")\n",
        "print(f\"Saved files:\")\n",
        "print(\"- Confused pairs analysis\")\n",
        "print(\"- Per-class metrics with F1 vs Support correlation\")\n",
        "print(\"- ROC curves with per-class AUC scores\")\n",
        "print(\"- Prediction spreadsheet with text, true labels, and predicted labels\")\n",
        "print(\"- Detailed classification reports\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Create final results summary\n",
        "final_results_df = pd.DataFrame(all_results)\n",
        "final_results_df.to_csv('/content/drive/MyDrive/Skripsi Dataset/FinalFile/stacking_ensemble_results.csv', index=False)\n",
        "print(f\"\\nFinal results summary saved: stacking_ensemble_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l3pb8sSku8cw",
        "outputId": "95442dab-f5e9-44be-8135-6c8b8fa51d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training Stacking-XGB-GRU-RF-LR\n",
            "Base Models: ['XGBoost', 'GRU', 'RandomForest']\n",
            "Meta Model: LogisticRegression\n",
            "============================================================\n",
            "Stage 1: Generating Out-of-Fold predictions...\n",
            "Stage 1 completed: OOF predictions generated for all base models\n",
            "Stage 2: Training and evaluating meta-classifier...\n",
            "Fold 1 done. MacroF1=0.7251, WeightedF1=0.7629, HierF1=0.8886\n",
            "Fold 2 done. MacroF1=0.7094, WeightedF1=0.7534, HierF1=0.8836\n",
            "Fold 3 done. MacroF1=0.6990, WeightedF1=0.7373, HierF1=0.8771\n",
            "Fold 4 done. MacroF1=0.7017, WeightedF1=0.7494, HierF1=0.8815\n",
            "Fold 5 done. MacroF1=0.7196, WeightedF1=0.7565, HierF1=0.8863\n",
            "\n",
            "Training final stacking model for test set prediction...\n",
            "Training base models on full training data...\n",
            "Generating test predictions from base models...\n",
            "\n",
            "Stacking-XGB-GRU-RF-LR Final Results:\n",
            "Macro F1: 0.7109 ± 0.0101\n",
            "Weighted F1: 0.7519 ± 0.0085\n",
            "Hierarchical F1: 0.8834 ± 0.0040\n",
            "Total Training Time: 1987.63 seconds\n",
            "Base Models: ['XGBoost', 'GRU', 'RandomForest']\n",
            "Meta Model: LogisticRegression\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS 1: CONFUSION PAIR ANALYSIS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Most Confused Pairs - Stacking-XGB-GRU-RF-LR:\n",
            "------------------------------------------------------------\n",
            "True Class           Predicted Class      Count     \n",
            "------------------------------------------------------------\n",
            "Depression           Suicidal             894       \n",
            "Suicidal             Depression           391       \n",
            "Normal               Suicidal             159       \n",
            "Stress               Depression           95        \n",
            "Anxiety              Depression           84        \n",
            "Normal               Stress               81        \n",
            "Depression           Stress               77        \n",
            "Depression           Normal               74        \n",
            "Normal               Depression           70        \n",
            "Suicidal             Normal               69        \n",
            "\n",
            "================================================================================\n",
            "ANALYSIS 2: F1 vs SUPPORT CORRELATION\n",
            "================================================================================\n",
            "\n",
            "Per-Class Metrics - Stacking-XGB-GRU-RF-LR:\n",
            "--------------------------------------------------------------------------------\n",
            "Class                Precision  Recall     F1_Score   Support   \n",
            "--------------------------------------------------------------------------------\n",
            "Anxiety              0.8243    0.7695    0.7960    768       \n",
            "Bipolar              0.7910    0.7910    0.7910    555       \n",
            "Depression           0.7148    0.6108    0.6587    3081      \n",
            "Normal               0.9207    0.8838    0.9018    3269      \n",
            "Personality disorder 0.6368    0.6930    0.6637    215       \n",
            "Stress               0.5802    0.5656    0.5728    518       \n",
            "Suicidal             0.6002    0.7761    0.6769    2130      \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation between Support and F1 Score: 0.3374\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS 3: ROC CURVE ANALYSIS (PER-CLASS)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AUC Scores - Stacking-XGB-GRU-RF-LR:\n",
            "--------------------------------------------------\n",
            "Anxiety             : 0.9694\n",
            "Bipolar             : 0.9688\n",
            "Depression          : 0.8842\n",
            "Normal              : 0.9799\n",
            "Personality disorder: 0.9442\n",
            "Stress              : 0.9322\n",
            "Suicidal            : 0.9137\n",
            "Micro-average       : 0.9543\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS 4: PREDICTION SPREADSHEET\n",
            "================================================================================\n",
            "\n",
            "Prediction Spreadsheet Sample - Stacking-XGB-GRU-RF-LR:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Text_Input True_Label Predicted_Label  Confidence_Score  Prob_Anxiety  Prob_Bipolar  Prob_Depression  Prob_Normal  Prob_Personality disorder  Prob_Stress  Prob_Suicidal  Correct_Prediction\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                             left mildly suicidal message dyslexia website someone contacted the police welfare check what not understand how they got address never left any info the website maybe first and last name not recall that though they arrived had brief chat and assured them wa fine apart from feeling suicidal entire life but have never tried more just fantasied about the peace not living how did the police find this the you flat mate thought they were coming arrest logically but how did they track down mean it pretty impressive and unnerving the same time the police turned door   Suicidal        Suicidal          0.834175      0.002426      0.006566         0.117674     0.023290                   0.003561     0.012307       0.834175                True\n",
            "just need vent the past few week have been the worst ever felt teacher and haven been able work for over two week now because the job just too stressful and can deal been seeking treatment for depression for year now with bipolar diagnosis happening last september treatment have worked for all them have made feel even worse than before with short period respite between exhausting going medical leave which mean won able afford own place anymore and have move back with parent who thankfully have been very supportive but still just suck much and feel like life going backward after working hard get just this far one best friend who used extremely supportive just doesn seem care that much anymore about and been dealing with that for month now heartbreaking have other friend can rely but she wa very special still learning how move from but just can sick waking every morning feeling like shit want badly for something make feel happy but nothing work wish giving wa option    Bipolar         Bipolar          0.840747      0.013380      0.840747         0.065984     0.002878                   0.052491     0.019072       0.005449                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  hey how about stop asking yourself why here what the purpose stop asking all those question because there answer there meaning purpose life and think that the best outcome there nothing sad yeah maybe hard swallow pill but that not reason end yourself just experience whether good bad doe not matter but you choose not experience life nothing wrong that nothing matter   Suicidal        Suicidal          0.496594      0.005858      0.014437         0.428272     0.004108                   0.031754     0.018975       0.496594                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  people love say suicide selfish but it the other way around the selfish one are your family member who want you alive and make them happy despite the fact your suffering and it the only way out it not selfish   Suicidal        Suicidal          0.884615      0.001090      0.003495         0.094463     0.007406                   0.003198     0.005734       0.884615                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       hihi come back this account     Normal          Normal          0.957367      0.008712      0.002964         0.005348     0.957367                   0.007447     0.006493       0.011669                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           guy seasonal depression over time for just regular depression now happy Depression      Depression          0.835479      0.013987      0.023423         0.835479     0.000958                   0.033144     0.021064       0.071944                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           for whom with health anxiety over diagnosed health issue therapy helped Depression      Depression          0.750735      0.031257      0.032636         0.750735     0.002351                   0.023974     0.049766       0.109280                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 you guy looking for sunscreen but confused which one buy what kind sunscreen you guy who are oily acne prone use can have review please thank you     Normal          Normal          0.951759      0.008900      0.003568         0.006374     0.951759                   0.009297     0.007136       0.012966                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       good luck with presentation     Normal          Normal          0.924191      0.012915      0.005598         0.009044     0.924191                   0.014004     0.012704       0.021544                True\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               why dizzy after watching movie that had brain tumor patient been feeling lightheaded and off and honestly this the first time health anxiety ha been back after year not being active been lightheaded for day and not getting worse but sure aint getting better also just very stressed with school dont know whats going cant check the hospital this ha happened last year and took week get rid whats wrong with just wan live    Anxiety         Anxiety          0.948146      0.948146      0.008942         0.011915     0.001330                   0.002416     0.026805       0.000447                True\n",
            "\n",
            "Full prediction spreadsheet saved: Stacking-XGB-GRU-RF-LR_predictions.csv\n",
            "\n",
            "Test Set Performance - Stacking-XGB-GRU-RF-LR:\n",
            "Accuracy: 0.7494\n",
            "Total Samples: 10536\n",
            "Correct Predictions: 7896\n",
            "Incorrect Predictions: 2640\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS 5: DETAILED CLASSIFICATION REPORT\n",
            "================================================================================\n",
            "\n",
            "Detailed Classification Report - Stacking-XGB-GRU-RF-LR:\n",
            "------------------------------------------------------------\n",
            "                      precision  recall  f1-score     support\n",
            "Anxiety                  0.8243  0.7695    0.7960    768.0000\n",
            "Bipolar                  0.7910  0.7910    0.7910    555.0000\n",
            "Depression               0.7148  0.6108    0.6587   3081.0000\n",
            "Normal                   0.9207  0.8838    0.9018   3269.0000\n",
            "Personality disorder     0.6368  0.6930    0.6637    215.0000\n",
            "Stress                   0.5802  0.5656    0.5728    518.0000\n",
            "Suicidal                 0.6002  0.7761    0.6769   2130.0000\n",
            "accuracy                 0.7494  0.7494    0.7494      0.7494\n",
            "macro avg                0.7240  0.7271    0.7230  10536.0000\n",
            "weighted avg             0.7593  0.7494    0.7507  10536.0000\n",
            "\n",
            "================================================================================\n",
            "ALL ANALYSES COMPLETED!\n",
            "Saved files:\n",
            "- Confused pairs analysis\n",
            "- Per-class metrics with F1 vs Support correlation\n",
            "- ROC curves with per-class AUC scores\n",
            "- Prediction spreadsheet with text, true labels, and predicted labels\n",
            "- Detailed classification reports\n",
            "================================================================================\n",
            "\n",
            "Final results summary saved: stacking_ensemble_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Test"
      ],
      "metadata": {
        "id": "Sx_eS6ZRNL1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Skripsi Dataset/FinalFile/final_model_fold_results_2.xlsx\"\n",
        "\n",
        "# Load the data\n",
        "try:\n",
        "    df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    print(\"Please update the file_path variable with the correct path to your file.\")"
      ],
      "metadata": {
        "id": "yKPDWkzrNg7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing and Filtering\n",
        "# Filter only Fold rows (remove Mean and Std rows)\n",
        "fold_data = df[df['Type'] == 'Fold'].copy()\n",
        "\n",
        "# Extract unique models\n",
        "models = fold_data['Model'].unique()\n",
        "print(f\"Found {len(models)} unique models:\")\n",
        "for i, model in enumerate(models, 1):\n",
        "    print(f\"{i}. {model}\")\n",
        "\n",
        "# Create a pivot table for analysis\n",
        "metric = 'Weighted_F1'\n",
        "\n",
        "pivot_data = fold_data.pivot_table(\n",
        "    index='Fold',\n",
        "    columns='Model',\n",
        "    values=metric\n",
        ")\n",
        "\n",
        "print(f\"\\nPivot table shape: {pivot_data.shape}\")\n",
        "print(f\"\\nUsing metric: {metric}\")\n",
        "display(pivot_data.head())"
      ],
      "metadata": {
        "id": "JMdwXETCYBsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate descriptive statistics for each model\n",
        "print(\"Descriptive Statistics for Each Model:\")\n",
        "stats_summary = pivot_data.describe()\n",
        "display(stats_summary)\n",
        "\n",
        "# Visualize the performance distribution\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=pivot_data)\n",
        "plt.title(f'Performance Distribution ({metric}) by Model')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(metric)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Line plot to see trends across folds\n",
        "plt.figure(figsize=(14, 8))\n",
        "for model in pivot_data.columns:\n",
        "    plt.plot(pivot_data.index, pivot_data[model], marker='o', label=model, linewidth=2)\n",
        "plt.title(f'Performance Across Folds ({metric})')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel(metric)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0eDnIi-YIgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Friedman test\n",
        "friedman_data = pivot_data.values.T\n",
        "\n",
        "print(\"Friedman Test Results:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "stat, p_value = friedmanchisquare(*friedman_data)\n",
        "\n",
        "print(f\"Friedman Test Statistic: {stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"\\nResult: Significant differences exist among models (p < {alpha})\")\n",
        "    print(\"Proceeding with post-hoc analysis...\")\n",
        "else:\n",
        "    print(f\"\\nResult: No significant differences among models (p ≥ {alpha})\")\n",
        "    print(\"Post-hoc analysis may not be necessary.\")\n",
        "\n",
        "# Degrees of freedom\n",
        "k = len(models)\n",
        "df = k - 1\n",
        "print(f\"\\nDegrees of freedom: {df}\")"
      ],
      "metadata": {
        "id": "CAst68aYYMfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-hoc Nemenyi Test\n",
        "if p_value < alpha:\n",
        "    print(\"Performing Nemenyi Post-hoc Test...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    nemenyi_results = sp.posthoc_nemenyi_friedman(friedman_data.T)\n",
        "\n",
        "    # Set model names as index and columns\n",
        "    nemenyi_results.index = pivot_data.columns\n",
        "    nemenyi_results.columns = pivot_data.columns\n",
        "\n",
        "    print(\"Nemenyi Test P-value Matrix:\")\n",
        "    display(nemenyi_results)\n",
        "\n",
        "    # Create a heatmap of p-values\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(nemenyi_results, dtype=bool))\n",
        "    sns.heatmap(nemenyi_results,\n",
        "                annot=True,\n",
        "                fmt=\".4f\",\n",
        "                cmap='coolwarm',\n",
        "                center=0.05,\n",
        "                cbar_kws={'label': 'P-value'},\n",
        "                mask=mask,\n",
        "                square=True)\n",
        "    plt.title('Nemenyi Post-hoc Test P-values\\n(Lower triangle)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Nemenyi test due to non-significant Friedman test result.\")"
      ],
      "metadata": {
        "id": "KqlVuI0MYPUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Critical Difference Diagram\n",
        "if p_value < alpha:\n",
        "    print(\"Generating Critical Difference Diagram...\")\n",
        "\n",
        "    # Calculate average ranks for each model\n",
        "    ranks = pivot_data.rank(axis=1, ascending=False)\n",
        "    average_ranks = ranks.mean(axis=0).sort_values()\n",
        "\n",
        "    print(\"\\nAverage Ranks (1 = best):\")\n",
        "    for model, rank in average_ranks.items():\n",
        "        print(f\"{model}: {rank:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Calculate critical difference\n",
        "    k = len(models)\n",
        "    N = len(pivot_data)\n",
        "    q_alpha = 2.569\n",
        "\n",
        "    CD = q_alpha * np.sqrt(k * (k + 1) / (6 * N))\n",
        "\n",
        "    print(f\"\\nCritical Difference (CD): {CD:.3f}\")\n",
        "    print(\"Models connected by a line are not significantly different\")\n",
        "\n",
        "    # Plot ranks\n",
        "    plt.scatter(average_ranks.values, [1] * len(average_ranks), s=100, color='blue', zorder=3)\n",
        "\n",
        "    # Add model names\n",
        "    for i, (model, rank) in enumerate(average_ranks.items()):\n",
        "        plt.text(rank, 1.1, model, ha='center', va='bottom', fontsize=9, rotation=45)\n",
        "\n",
        "    plt.axhline(y=1, color='gray', linestyle='-', alpha=0.3)\n",
        "\n",
        "    # Identify groups of non-significantly different models\n",
        "    groups = []\n",
        "    current_group = [average_ranks.index[0]]\n",
        "\n",
        "    for i in range(1, len(average_ranks)):\n",
        "        if average_ranks.iloc[i] - average_ranks.iloc[0] <= CD:\n",
        "            current_group.append(average_ranks.index[i])\n",
        "        else:\n",
        "            groups.append(current_group)\n",
        "            current_group = [average_ranks.index[i]]\n",
        "    groups.append(current_group)\n",
        "\n",
        "    # Print\n",
        "    print(\"\\nGroups of non-significantly different models:\")\n",
        "    for i, group in enumerate(groups):\n",
        "        print(f\"Group {i+1}: {', '.join(group)}\")\n",
        "\n",
        "    plt.xlim(0.5, len(models) + 0.5)\n",
        "    plt.ylim(0.5, 1.5)\n",
        "    plt.xlabel('Average Rank')\n",
        "    plt.title(f'Critical Difference Diagram (CD = {CD:.3f})')\n",
        "    plt.yticks([])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "byojxtB3YTas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save result to drive\n",
        "results_dir = \"/content/drive/MyDrive/Skripsi Dataset/Statistical_Analysis_Results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(\"Saving results to Google Drive...\")\n",
        "\n",
        "stats_summary.to_csv(f\"{results_dir}/descriptive_statistics.csv\")\n",
        "\n",
        "# Save Friedman test results\n",
        "friedman_results = pd.DataFrame({\n",
        "    'Friedman_Statistic': [stat],\n",
        "    'P_value': [p_value],\n",
        "    'Significant': [p_value < 0.05]\n",
        "})\n",
        "friedman_results.to_csv(f\"{results_dir}/friedman_test_results.csv\", index=False)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    # Save Nemenyi results\n",
        "    nemenyi_results.to_csv(f\"{results_dir}/nemenyi_test_results.csv\")\n",
        "\n",
        "    # Save ranks\n",
        "    average_ranks.to_csv(f\"{results_dir}/average_ranks.csv\", header=['Average_Rank'])\n",
        "\n",
        "# Create comprehensive report\n",
        "report = f\"\"\"\n",
        "STATISTICAL ANALYSIS REPORT\n",
        "Generated on: {pd.Timestamp.now()}\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Number of models: {len(models)}\n",
        "- Number of folds: {len(pivot_data)}\n",
        "- Primary metric: {metric}\n",
        "\n",
        "FRIEDMAN TEST RESULTS:\n",
        "- Test Statistic: {stat:.4f}\n",
        "- P-value: {p_value:.4f}\n",
        "- Significant: {'Yes' if p_value < 0.05 else 'No'}\n",
        "\n",
        "TOP PERFORMING MODELS:\n",
        "\"\"\"\n",
        "\n",
        "if p_value < 0.05:\n",
        "    ranks = pivot_data.rank(axis=1, ascending=False)\n",
        "    avg_ranks = ranks.mean(axis=0).sort_values()\n",
        "    for i, (model, rank) in enumerate(avg_ranks.head(3).items()):\n",
        "        report += f\"{i+1}. {model} (Avg Rank: {rank:.2f})\\n\"\n",
        "else:\n",
        "    report += \"No significant differences found among models.\\n\"\n",
        "\n",
        "# Save report\n",
        "with open(f\"{results_dir}/analysis_report.txt\", \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"All results saved to: {results_dir}\")\n",
        "print(\"Files created:\")\n",
        "!ls -la \"{results_dir}\""
      ],
      "metadata": {
        "id": "SVDfFvKAYVmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gFiThj0tIi3i",
        "V4ufSf2ecynh",
        "Hj0B3o0w0mSb",
        "OoimxhJnHlQk",
        "Th3MZM4go9jB",
        "pA8pWUfWU6sV",
        "hBomE1pjdwcw",
        "a1cKfGlIu6GW",
        "Sx_eS6ZRNL1i"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}